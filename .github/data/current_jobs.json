[
  {
    "job_id": "DmKkjIHFGqwpZawzAAAAAA==",
    "job_title": "Associate Data Engineer",
    "employer_name": "THE DOW CHEMICAL COMPANY",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHE6Qv_UOqARLxrnwfpr-ucKbs6GEppbjL9zYs&s=0",
    "employer_website": null,
    "job_publisher": "THE DOW CHEMICAL COMPANY",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "THE DOW CHEMICAL COMPANY",
        "apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Dow, we believe in putting people first and we’re passionate about delivering integrity, respect and safety to our customers, our employees and the planet.\n\nOur people are at the heart of our solutions. They reflect the communities we live in and the world where we do business. Their diversity is our strength. We’re a community of relentless problem solvers that offers the daily opportunity to contribute with your perspective, transform industries and shape the future. Our purpose is simple - to deliver a sustainable future for the world through science and collaboration.If you’re looking for a challenge and meaningful role, you’re in the right place.\n\nAbout you and the role\n\nDow has an exciting and challenging opportunity for an Associate Data Engineer located in Midland, MI or Houston, TX. which will work on the Enterprise Data & Analytics - Data Analytics Platform team.\n\nAs an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow. TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects. They will work closely with amulti-disciplinary team to:\n• Createdata pipelines forprojectsin the Azure environment.\n• Write notebooks in Databricks\n• Develop Infrastructure as Code (IaC) code\n• Build logic apps\n• Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation.\n\nAssociate Data Engineer Responsibilities / Duties:\n• Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)\n• Develop and deploy data pipelines using Azure data services\n• Deployment of Azure services using Infrastructure as Code and Azure DevOps.\n• This entry level position is for an Independent Contributor and is not expected to be a people leader\n\nKnowledge, skills and abilities include:\n• Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives\n• Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’\n• Concepts of Data architecting - concepts\n• General understanding of digital industry trends\n\nOther Critical Skills:\n• Ability to thrive in challenging situations and solve complex problems\n• Ability to manage own work effort across multiple projects with little supervision\n• Analytical and problem-solving skills\n• Customer centricity\n• Good communication\n\nYour Skills\n• Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs). This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms.\n• Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems.\n• Business Processes: Understanding how business functions operate and how technology enables or improves them. This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs.\n• Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments.\n• Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards.\n\nRequired qualifications:\n• Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree.\n• A minimum requirement for this U.S. based position is the ability to work legally in the United States. No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process.\n\nYour preferred qualifications include:\n• A degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines\n• Experiencewith Azuredata services andPython (other programming language)\n• Experience developing intheAzureenvironment\n• Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling\n• Multi-application and cross-platform design experience\n\nNote: Relocation assistance is not available with this position.\n\nBenefits – What Dow offers you\n\nWe invest in you.\n\nDow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career. You bring your background, talent, and perspective to work every day. Dow rewards that commitment by investing in your total wellbeing.\n\nHere are just a few highlights of what you would be offered as a Dow employee:\n• Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives.\n• Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it.\n• Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals.\n• Employee stock purchase programs (availability varies depending on location).\n• Student Debt Retirement Savings Match Program (U.S. only).\n• Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match.\n• Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs. Travel insurance is also available in certain countries/locations.\n• Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building.\n• Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs.\n• Competitive yearly vacation allowance.\n• Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents).\n• Paid time off to care for family members who are sick or injured.\n• Paid time off to support volunteering and Employee Resource Group’s (ERG) participation.\n• Wellbeing Portal for all Dow employees, our one-stop shop to promote wellbeing, empowering employees to take ownership of their entire wellbeing journey.\n• On-site fitness facilities to help stay healthy and active (availability varies depending on location).\n• Employee discounts for online shopping, cinema tickets, gym memberships and more.\n• Additionally, some of our locations might offer:\n• Transportation allowance (availability varies depending on location)\n• Meal subsidiaries/vouchers (availability varies depending on location)\n• Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)\n\nJoin our team, we can make a difference together.\n\nAbout Dow\nDow (NYSE: DOW) is one of the world’s leading materials science companies, serving customers in high-growth markets such as packaging, infrastructure, mobility and consumer applications.Our global breadth, asset integration and scale, focused innovation, leading business positions and commitment to sustainability enable us to achieve profitable growth and help deliver a sustainable future. We operate manufacturing sites in 30countries and employ approximately36,000 people. Dow delivered sales of approximately$43 billionin 2024. References to Dow or the Company mean Dow Inc. and its subsidiaries. Learn more about us and our ambition to be the most innovative, customer-centric, inclusive and sustainable materials science company in the world by visitingwww.dow.comopens in a new tab.\n\nAs part of our dedication to inclusion, Dow is committed to equal opportunities in employment. We encourage every employee to bring their whole self to work each day to not only deliver more value, but also have a more fulfilling career. Further information regarding Dow's equal opportunities is available on www.dow.comopens in a new tab.\nDow is an Equal Employment Opportunity employer and is committed to providing opportunities without regard for race, color, religion, sex, including pregnancy, sexual orientation, or gender identity, national origin, age, disability and genetic information, including family medical history. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may call us at 1-833-My Dow HR (833-693-6947) and select option 8.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Richmond, TX",
    "job_city": "Richmond",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 29.5821811,
    "job_longitude": -95.76078319999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDmKkjIHFGqwpZawzAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This entry level position is for an Independent Contributor and is not expected to be a people leader",
        "Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives",
        "Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’",
        "Concepts of Data architecting - concepts",
        "General understanding of digital industry trends",
        "Ability to thrive in challenging situations and solve complex problems",
        "Ability to manage own work effort across multiple projects with little supervision",
        "Analytical and problem-solving skills",
        "Customer centricity",
        "Good communication",
        "Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs)",
        "Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree",
        "A minimum requirement for this U.S. based position is the ability to work legally in the United States",
        "No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process",
        "Python (other programming language)",
        "Experience developing intheAzureenvironment",
        "Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling",
        "Multi-application and cross-platform design experience"
      ],
      "Benefits": [
        "We invest in you",
        "Dow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career",
        "You bring your background, talent, and perspective to work every day",
        "Dow rewards that commitment by investing in your total wellbeing",
        "Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives",
        "Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it",
        "Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals",
        "Employee stock purchase programs (availability varies depending on location)",
        "Student Debt Retirement Savings Match Program (U.S. only)",
        "Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match",
        "Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs",
        "Travel insurance is also available in certain countries/locations",
        "Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building",
        "Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs",
        "Competitive yearly vacation allowance",
        "Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents)",
        "Paid time off to care for family members who are sick or injured",
        "Paid time off to support volunteering and Employee Resource Group’s (ERG) participation",
        "On-site fitness facilities to help stay healthy and active (availability varies depending on location)",
        "Employee discounts for online shopping, cinema tickets, gym memberships and more",
        "Additionally, some of our locations might offer:",
        "Transportation allowance (availability varies depending on location)",
        "Meal subsidiaries/vouchers (availability varies depending on location)",
        "Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)"
      ],
      "Responsibilities": [
        "which will work on the Enterprise Data & Analytics - Data Analytics Platform team",
        "As an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow",
        "TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects",
        "They will work closely with amulti-disciplinary team to:",
        "Createdata pipelines forprojectsin the Azure environment",
        "Write notebooks in Databricks",
        "Develop Infrastructure as Code (IaC) code",
        "Build logic apps",
        "Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation",
        "Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)",
        "Develop and deploy data pipelines using Azure data services",
        "Deployment of Azure services using Infrastructure as Code and Azure DevOps",
        "This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms",
        "Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems",
        "Business Processes: Understanding how business functions operate and how technology enables or improves them",
        "This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs",
        "Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments",
        "Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-dow-com-hiring-associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22",
    "_source": "new_jobs"
  },
  {
    "job_id": "4jWnCYR8XCPyHreLAAAAAA==",
    "job_title": "Lead Data Engineer; Java, Python, Spark, AWS",
    "employer_name": "Capital One",
    "employer_logo": null,
    "employer_website": "https://www.capitalone.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/baltimore/maryland/software_development/4710755767/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Overview\n\nThe Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment. In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools. If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you.\n\nKey Responsibilities\n• Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools.\n• Work with developers experienced in machine learning, distributed microservices, and full stack systems.\n• Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake.\n• Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers.\n• Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans.\n• Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions.\n\nRequired Qualifications\n• Bachelor’s Degree\n• Minimum 4 years of experience in application development (internship experience does not apply)\n• At least 2 years of experience in big data technologies\n• At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)\n\nPreferred Qualifications\n• 7+ years of application development experience including Python, SQL, Scala, or Java\n• 4+ years of experience with a public cloud (AWS, Microsoft Azure, or Google Cloud)\n• 4+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n• 4+ years of experience with real-time data and streaming applications\n• 4+ years of experience with NoSQL implementations (e.g., Mongo or Cassandra)\n• 4+ years of data warehousing experience (Redshift or Snowflake)\n• 4+ years of Unix/Linux experience including basic commands and shell scripting\n• 2+ years of experience with Agile engineering practices\n\nBenefits & Perks\n• Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer\n• Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer\n• Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer\n• Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours.",
    "job_is_remote": false,
    "job_posted_at": "20 hours ago",
    "job_posted_at_timestamp": 1771023600,
    "job_posted_at_datetime_utc": "2026-02-13T23:00:00.000Z",
    "job_location": "Baltimore, MD",
    "job_city": "Baltimore",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.2905023,
    "job_longitude": -76.6104072,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4jWnCYR8XCPyHreLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor’s Degree",
        "Minimum 4 years of experience in application development (internship experience does not apply)",
        "At least 2 years of experience in big data technologies",
        "At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)"
      ],
      "Benefits": [
        "Benefits & Perks",
        "Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer",
        "Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer",
        "Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer",
        "Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours"
      ],
      "Responsibilities": [
        "The Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment",
        "In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools",
        "If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you",
        "Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools",
        "Work with developers experienced in machine learning, distributed microservices, and full stack systems",
        "Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake",
        "Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers",
        "Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans",
        "Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-7obxlyk4atqenktrzypk5a",
    "_source": "new_jobs"
  },
  {
    "job_id": "y9Ez-YEkLqpd9staAAAAAA==",
    "job_title": "DataEngineer",
    "employer_name": "Jobs via Dice",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR9CC1NlnLA7sshF1s1dqKvk8U495jsMwImnyPP&s=0",
    "employer_website": null,
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/dataengineer-at-jobs-via-dice-4372009753?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/dataengineer-at-jobs-via-dice-4372009753?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/8450f66f-d9c9-4a3f-9a3e-7e7ddf43420c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/185ad29d20a173269bde7c12ca5be9f0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Dice is the leading career destination for tech experts at every stage of their careers. Our client, Key Business Solutions, Inc., is seeking the following. Apply via Dice today!\n\nJob Description: Data Engineer\n\nPlano, TX - 4 Days Hybrid\n\n12+ Months\n\nDescription:\n\nCollecting data from internal and external sources, ensuring accuracy, and removing duplicates or errors.\n\nApplying statistical and analytical techniques to identify trends, correlations, and anomalies.\n\nCreating dashboards, charts, and reports using tools like Excel, SQL, Python, and Tableau to communicate findings to stakeholders.\n\nTranslating complex datasets into actionable recommendations for business strategy, marketing, or operations.\n\nWorking with cross-functional teams, including project managers, business analysts, and marketing, to understand data needs and develop solutions.\n\nEnsuring compliance with data management policies, maintaining data integrity, and supporting data architecture and modeling efforts.\n\nSkills And Qualifications\n\nProficiency in SQL, Excel, Python, R, and data visualization tools like Tableau or Power BI.\n\nAbility to interpret complex data, identify patterns, and provide actionable insights.\n\nTranslating technical findings into clear, business-friendly language for stakeholders.\n\nUnderstanding of business processes, KPIs, and regulatory requirements, especially in marketing.\n\nWillingness to adopt new tools, programming languages, and Big Data techniques.\n\nKnowledge of marketing data sets like Google Analytics, Social Media Platforms, Salesforce, Marketing Cloud and Customer Data Platforms.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Plano, TX",
    "job_city": "Plano",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 33.0216577,
    "job_longitude": -96.6979973,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dy9Ez-YEkLqpd9staAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "12+ Months",
        "Proficiency in SQL, Excel, Python, R, and data visualization tools like Tableau or Power BI",
        "Ability to interpret complex data, identify patterns, and provide actionable insights",
        "Translating technical findings into clear, business-friendly language for stakeholders",
        "Understanding of business processes, KPIs, and regulatory requirements, especially in marketing",
        "Willingness to adopt new tools, programming languages, and Big Data techniques",
        "Knowledge of marketing data sets like Google Analytics, Social Media Platforms, Salesforce, Marketing Cloud and Customer Data Platforms"
      ],
      "Responsibilities": [
        "Collecting data from internal and external sources, ensuring accuracy, and removing duplicates or errors",
        "Applying statistical and analytical techniques to identify trends, correlations, and anomalies",
        "Creating dashboards, charts, and reports using tools like Excel, SQL, Python, and Tableau to communicate findings to stakeholders",
        "Translating complex datasets into actionable recommendations for business strategy, marketing, or operations",
        "Working with cross-functional teams, including project managers, business analysts, and marketing, to understand data needs and develop solutions",
        "Ensuring compliance with data management policies, maintaining data integrity, and supporting data architecture and modeling efforts"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-dataengineer-at-jobs-via-dice-4372009753",
    "_source": "new_jobs"
  },
  {
    "job_id": "aLEr5N2HS9ba0TU3AAAAAA==",
    "job_title": "Data Engineer - Remote at Staffing the Universe United States",
    "employer_name": "Staffing the Universe",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Ibfportal.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Ibfportal.com",
        "apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Remote job at Staffing the Universe. United States. Data Engineer\n\nData Engineer Hartford, CT or Remote Contract No third-party C2C\n\nJob Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team. On this team, you will be helping architect and deliver a wide variety of code artifacts. You will be working to build a scalable and secure ETL solutions for ope...",
    "job_is_remote": false,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "New Haven, CT",
    "job_city": "New Haven",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.308274,
    "job_longitude": -72.9278835,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaLEr5N2HS9ba0TU3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Data Engineer Hartford, CT or Remote Contract No third-party C2C"
      ],
      "Responsibilities": [
        "Job Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team",
        "On this team, you will be helping architect and deliver a wide variety of code artifacts",
        "You will be working to build a scalable and secure ETL solutions for ope.."
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "ibfportal-com-office-job-data-engineer-remote-at-staffing-the-universe-united-states-ym9lbu9gtu5hnlh5ede0nys1utrrk2vvzmc9pq",
    "_source": "new_jobs"
  },
  {
    "job_id": "8ojyM0IL2Dgweb6mAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Guardianlife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEHyPUSx8in9lbGwtt3n-urbaK-a93llGwRZYg&s=0",
    "employer_website": "https://www.guardianlife.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/81170e5434e33fef083d666436b3feaa?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6984c41c0f6f7e7a2cdf36a2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/guardian-life/holmdel-nj/lead-data-engineer/448a3771cda355807800f3a245f659f2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5618263952?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/lead-data-engineer-holmdel-new-jersey-us-guardian-life-r000108510?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/lead-data-engineer--holmdel-township--e33d7f694afe11a60b979debb2c4bd89a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/holmdel-township/new-jersey/software_development/4858586430/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Guardian is seeking a highly skilled and motivated Lead Data Engineer to join the FPRS&CSWM Data Engineering team. In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases. Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient. The ideal candidate will have a passion for data engineering, thrive in a collaborative environment and are excited about leveraging cutting-edge technologies to drive business success.\n\nYour contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers. You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs. We value curiosity, creativity, and continuous learning. If you're passionate about solving meaningful problems and creating value through data-driven innovation, we look forward to welcoming you to our team.\n\nYou will\n• Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables.\n• Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth.\n• Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products\n• Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems.\n• Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.\n• Construct meaningful data assets sourced from structured, semi structured, and unstructured data.\n• Develop real-time data solutions by creating new API endpoints or streaming frameworks.\n• Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle.\n• Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams.\n• Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication.\n• Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions.\n• Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives.\n• Stay up-to-date with the latest trends in modern data engineering, machine learning & AI.\n\nYou have\n• Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field.\n• 5+ years of experience working with Python, SQL, PySpark, and bash scripts. Proficient in software development lifecycle and software engineering practices.\n• 4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases.\n• 3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark.\n• 2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality.\n• Solid understanding of data modeling and warehousing techniques. Experience working in a data warehouse is a plus.\n• Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities.\n• Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries).\n• Proficient in understanding and incorporating software engineering principles in design & development process.\n• Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent).\n• Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business.\n\nLocation\n• Three days a week at a Guardian office in Bethlehem, PA, New York, NY. Pittsfield, MA or Holmdel, NJ.\n\nSalary Range:\n\n$99,150.00 - $162,885.00\n\nThe salary range reflected above is a good faith estimate of base pay for the primary location of the position. The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate. In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation.\n\nOur Promise\n\nAt Guardian, you'll have the support and flexibility to achieve your professional and personal goals. Through skill-building, leadership development and philanthropic opportunities, we provide opportunities to build communities and grow your career, surrounded by diverse colleagues with high ethical standards.\n\nInspire Well-Being\n\nAs part of Guardian's Purpose - to inspire well-being - we are committed to offering contemporary, supportive, flexible, and inclusive benefits and resources to our colleagues. Explore our company benefits at www.guardianlife.com/careers/corporate/benefits.Benefits apply to full-time eligible employees. Interns are not eligible for most Company benefits.\n\nEqual Employment Opportunity\n\nGuardian is an equal opportunity employer. All qualified applicants will be considered for employment without regard to age, race, color, creed, religion, sex, affectional or sexual orientation, national origin, ancestry, marital status, disability, military or veteran status, or any other classification protected by applicable law.\n\nAccommodations\n\nGuardian is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities.Guardian also provides reasonable accommodations to qualified job applicants (and employees) to accommodate the individual's known limitations related to pregnancy, childbirth, or related medical conditions, unless doing so would create an undue hardship. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact MyHR@glic.com. Please note: this resource is for accommodation requests only. For all other inquires related to your application and careers at Guardian, refer to the Guardian Careers site.\n\nVisa Sponsorship\n\nGuardian is not currently or in the foreseeable future sponsoring employment visas. In order to be a successful applicant. you must be legally authorized to work in the United States, without the need for employer sponsorship.\n\nCurrent Guardian Colleagues: Please apply through the internal Jobs Hub in Workday.",
    "job_is_remote": false,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Holmdel, NJ",
    "job_city": "Holmdel",
    "job_state": "New Jersey",
    "job_country": "US",
    "job_latitude": 40.3848944,
    "job_longitude": -74.18900599999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D8ojyM0IL2Dgweb6mAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 99150,
    "job_max_salary": 162885,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "We value curiosity, creativity, and continuous learning",
        "Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field",
        "5+ years of experience working with Python, SQL, PySpark, and bash scripts",
        "Proficient in software development lifecycle and software engineering practices",
        "4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases",
        "3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark",
        "2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality",
        "Solid understanding of data modeling and warehousing techniques",
        "Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities",
        "Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries)",
        "Proficient in understanding and incorporating software engineering principles in design & development process",
        "Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent)",
        "Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business",
        "you must be legally authorized to work in the United States, without the need for employer sponsorship"
      ],
      "Benefits": [
        "$99,150.00 - $162,885.00",
        "The salary range reflected above is a good faith estimate of base pay for the primary location of the position",
        "The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate",
        "In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation",
        "Interns are not eligible for most Company benefits"
      ],
      "Responsibilities": [
        "In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases",
        "Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient",
        "Your contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers",
        "You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs",
        "Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables",
        "Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth",
        "Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products",
        "Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems",
        "Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues",
        "Construct meaningful data assets sourced from structured, semi structured, and unstructured data",
        "Develop real-time data solutions by creating new API endpoints or streaming frameworks",
        "Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle",
        "Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams",
        "Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication",
        "Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions",
        "Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives",
        "Stay up-to-date with the latest trends in modern data engineering, machine learning & AI"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-guardian-life-job-lead-data-engineer-in-holmdel-nj",
    "_source": "new_jobs"
  },
  {
    "job_id": "fKq613YVZoGaO3ZhAAAAAA==",
    "job_title": "Temporary  Student Data Engineer",
    "employer_name": "University of Notre Dame",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB_bk0V1eXoFSLkXbpRAL9w5OPDz55ACjhxHof&s=0",
    "employer_website": null,
    "job_publisher": "BMES Career Center",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BMES Career Center",
        "apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/5e385fd85cc450ceae1fd6846215958e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/698af9934db8972cec006c83?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/indiana/business/4868026961/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-notre-dame-temporary-student-data-engineer-university-part_time?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temporary Student Data Engineer\n\nNotre Dame, IN, United States\nContract\nProvost\nTemporary\n\nCompany Description\n\nJob Description\nWe are seeking a Student Data Engineer (SDE) to join a team of students creating a Roblox game as a data collection tool, gauging students' interest in STEM and health care careers. The Lucy Family Institute for Data & Society (LFIDS) leverages data science, AI & ML toward social good. LFIDS engages with the Notre Dame community & beyond through funded research projects & collaborations, educational workshops, and special events. SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana. In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed.\n\nKey Responsibilities:\n\nWithin assigned projects, this role requires completion of data processing and programming tasks related to:\n• Data collection, management, harvesting, processing, transformation, and visualization;\n• Prototype data processing solutions;\n• Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms.\n\nThe successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners. Other responsibilities may include data analysis and giving presentations to diverse audiences.\n\nAdditional Requirements\n\nIn addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested. For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience.\n\nAdditional Opportunities\n\nStudent data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects. Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately.\n\nThe Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests.\n\nCore Qualities & Expectations\n• Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving\n• Confidentiality: Maintaining confidentiality is required.\n• Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings).\n• Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly. Perfection is not expected-it's okay to take time to learn. What matters is trying your best in each unique circumstance. We're committed to supporting your growth and confidence in the role.\n• Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities. We aim to provide a structure that supports your success.\n• Attention to detail or the ability to follow a set of instructions that we'll co-create and adjust based on your preferred learning and working style.\n• Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer.\n\nQualifications\nWe are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:\n• Data science methods and tools,\n• Software design\n• User experience principles\n\nExperience in an LFIDS area of expertise, like:\n• R and/or Python\n• Events and communications support\n• Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites).\n\nAdditional Information\nCompensation: $17.00/hour\n\nApplications for this position will close on February 13, 2026.\n\nThe University of Notre Dame seeks to attract, develop, and retain the highest quality faculty, staff and administration. The University is an Equal Opportunity Employer, and does not discriminate on the basis of race, color, national or ethnic origin, sex, disability, veteran status, genetic information, or age in employment. Moreover, Notre Dame prohibits discrimination against veterans or disabled qualified individuals, and complies with 41 CFR 60-741.5(a) and 41 CFR 60-300.5(a). We strongly encourage applications from candidates attracted to a university with a Catholic identity.\n\nTo apply, visit https://jobs.smartrecruiters.com/UniversityOfNotreDame/3743990011597785-temporary-student-data-engineer\n\nCopyright 2025 Jobelephant.com Inc. All rights reserved.\n\nPosted by the FREE value-added recruitment advertising agency jeid-5691f5d28abbde4e8b71884761a96763",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Notre Dame, IN",
    "job_city": "Notre Dame",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.7001908,
    "job_longitude": -86.2379328,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DfKq613YVZoGaO3ZhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Student data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects",
        "Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving",
        "Confidentiality: Maintaining confidentiality is required",
        "Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings)",
        "Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly",
        "Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities",
        "We aim to provide a structure that supports your success",
        "Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer",
        "We are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:",
        "Data science methods and tools,",
        "Software design",
        "User experience principles",
        "Experience in an LFIDS area of expertise, like:",
        "R and/or Python",
        "Events and communications support",
        "Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites)"
      ],
      "Benefits": [
        "Compensation: $17.00/hour"
      ],
      "Responsibilities": [
        "SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana",
        "In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed",
        "Within assigned projects, this role requires completion of data processing and programming tasks related to:",
        "Data collection, management, harvesting, processing, transformation, and visualization;",
        "Prototype data processing solutions;",
        "Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms",
        "The successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners",
        "Other responsibilities may include data analysis and giving presentations to diverse audiences",
        "In addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested",
        "For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience",
        "Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately",
        "The Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobboard-bmes-org-jobs-22037211-temporary-student-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "vmP5DjcyLh4_B94fAAAAAA==",
    "job_title": "Senior Data Engineer (Remote)",
    "employer_name": "Parsons Corporation",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWoeHM7rt-U37K1_ab3N5lIK6QuMuDbepnStZr&s=0",
    "employer_website": "https://www.parsons.com",
    "job_publisher": "Parsons Careers - Parsons Corporation",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Parsons Careers - Parsons Corporation",
        "apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "In a world of possibilities, pursue one with endless opportunities. Imagine Next!\n\nAt Parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. Guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. Unleash your talent and redefine what’s possible.\n\nJob Description:\n\nParsons is looking for an amazingly talented Senior Data Engineer to join our team! In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization.\n\nWhat You'll Be Doing:\n• Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture.\n• Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake.\n• Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing.\n• Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing.\n• Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation.\n• Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms.\n• Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data.\n• Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement.\n\nWhat Required Skills You'll Bring:\n• Strong hands-on experience with T-SQL and Python.\n• Experience with comprehensive data conversion projects is preferred (ERP systems including Oracle Cloud ERP and/or SAP S4/HANA)\n• Experience with Relational Database systems\n• Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)\n• Familiar with multi-dimensional and tabular models\n• 5+ years of experience in data engineering, data architecture, or data platform development.\n• Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar).\n• Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines.\n• Deep understanding of lakehouse architecture and medallion design patterns.\n• Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines.\n• Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats.\n• Strong problem-solving skills and ability to work independently in a fast-paced environment.\n• US person\n\nWhat Desired Skills You'll Bring:\n• Experience with data governance, security, and compliance (e.g., SOX, HIPAA).\n• Snowflake, Azure Data Engineer, dbt, and/or Databricks certifications\n• Exposure to real-time data processing and streaming technologies (e.g., Kafka, Spark Streaming).\n• Familiarity with data observability tools and automated testing frameworks for pipelines.\n• Bachelor's or Master’s degree in Computer Science, Information Systems, or a related field\n\nSecurity Clearance Requirement:\nNone\n\nThis position is part of our Corporate team.\n\nFor over 80 years, Parsons Corporation, has shaped the future of the defense, intelligence, and critical infrastructure markets. Our employees work in a close-knit team environment to find new, innovative ways to deliver smart solutions that are used and valued by customers around the world. By combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, transportation, smart cities, and more, we're providing tomorrow's solutions today.\n\nSalary Range: $100,900.00 - $176,600.00\n\nWe value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!\n\nThis position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled.\n\nParsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status.\n\nWe truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! Imagine next and join the Parsons quest—APPLY TODAY!\n\nParsons is aware of fraudulent recruitment practices. To learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Virginia",
    "job_city": null,
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 37.4315734,
    "job_longitude": -78.6568942,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvmP5DjcyLh4_B94fAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 101000,
    "job_max_salary": 177000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Strong hands-on experience with T-SQL and Python",
        "Experience with Relational Database systems",
        "Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)",
        "Familiar with multi-dimensional and tabular models",
        "5+ years of experience in data engineering, data architecture, or data platform development",
        "Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar)",
        "Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines",
        "Deep understanding of lakehouse architecture and medallion design patterns",
        "Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines",
        "Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats",
        "Strong problem-solving skills and ability to work independently in a fast-paced environment",
        "US person"
      ],
      "Benefits": [
        "Salary Range: $100,900.00 - $176,600.00",
        "We value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!"
      ],
      "Responsibilities": [
        "In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization",
        "Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture",
        "Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake",
        "Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing",
        "Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing",
        "Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation",
        "Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms",
        "Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data",
        "Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-parsons-com-jobs-senior-data-engineer-remote-virtual-r-174702-jobs-information-technology",
    "_source": "new_jobs"
  },
  {
    "job_id": "1Eb8GME5F6h5wzAiAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Rural King",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSI99A-RNW3mNS-GYEPIGd0U4Qdfm6662lvJ7uw&s=0",
    "employer_website": "https://www.ruralking.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-rural-king-4371246007?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Rural-King-Supply/Job/Data-Engineer/-in-Mattoon,IL?jid=4c03fb6a091c6a1e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/data-engineer_7ea1a64fa7b5bb1f8b19872d37b34c2da2d7e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/data-engineer-mattoon-il--e8d0d5a5-70ea-4eed-87dd-58b13c57a77d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Job Listings - ICIMS",
        "apply_link": "https://careers-ruralking.icims.com/jobs/35283/data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Nexxt",
        "apply_link": "https://www.nexxt.com/jobs/data-engineer-mattoon-il-3161318692-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/_m6FAmu2Nyxj74VafN91ouQs45puOR74OyTLalSIYaTrYL6icAgj4w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About us\n\nRural King is America's Farm and Home Store, providing essentials to the communities we serve. With a wide array of necessities ranging from food and feed to farm and home products, Rural King serves over 130 locations across 13 states and is constantly expanding. Our annual sales exceed $2.5 Billion, and our heart beats in Mattoon, IL, home to our corporate office, distribution center, and flagship store.\n\nOne thing our customers appreciate is our unique shopping experience, complete with complimentary popcorn and coffee. It's just one way we show our appreciation for their support.\n\nAt Rural King, we value our associates and strive to create a positive, rewarding workplace. We offer growth opportunities, competitive benefits, and a people-first environment where dedicated individuals come together to serve rural communities passionately. Join us, and you'll find not just a job but a chance to grow professionally, contribute meaningfully, and make a difference in the lives of those we serve.\n\nHow we reward you\n\n401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%\n\nHealthcare plans to support your needs\n\nVirtual doctor visits\n\nAccess to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program\n\n15% Associate Discount\n\nDave Ramsey's SmartDollar Program\n\nAssociate Assistance Program\n\nRK Cares Associate Hardship Program\n\n24/7 Chaplaincy Services\n\nCompany paid YMCA Family Membership\n\nWhat You'll do\n\nAs a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization.\n• Design, build, maintain, and manage the systems that move data efficiently within the organization.\n• Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake.\n• Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs\n• Monitor the performance of data pipelines that are efficient and secure\n• Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism.\n• Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments.\n• Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively.\n• Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement.\n• Perform other duties as assigned.\n\nSupervisory Responsibilities\n\nNone\n\nEssential Qualities for Success\n• At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education.\n• Information Technology Engineering experience preferred.\n• Experience as a Data Engineer or similar software engineering role.\n• Proficient with git.\n• Good knowledge of SQL and PHP or similar languages.\n• Good knowledge of big data technologies.\n• Good knowledge of data modeling.\n• Familiar with Looker, Power BI or other BI tools.\n• Working knowledge of databases, MySQL/MariaDB, and SQL.\n• Strong understanding of retail business practices.\n• Excellent negotiation and conflict resolution skills.\n• Demonstrated ability to adapt in a fast-paced environment.\n• Strong analytical and problem-solving skills.\n• Excellent organizational skills and attention to detail.\n• Demonstrated behaviors must reflect integrity, professionalism, and confidentiality.\n\nPhysical Requirements\n• Ability to maintain a seated or standing position for extended durations.\n• Capability to lift 15 pounds periodically.\n• Able to navigate and access all facilities.\n• Skill to effectively communicate verbally with others, both in-person and via electronic devices.\n• Close vision for computer-related tasks.\n\nReasonable accommodations may be made to enable individuals with disabilities to perform essential job functions.\n\nThe pay range for this position is $55,000 - $65,000 annualized and is bonus eligible. Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs. To learn more about our benefits, review here https://acrobat.adobe.com/id/urn:aaid:sc:VA6C2:14539c15-191a-4b77-9c13-f6ccfce10094.\n\nResponsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization. - Design, build, maintain, and manage the systems that move data efficiently within the organization. - Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake. - Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism. - Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments. - Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively. - Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement. - Perform other duties as assigned. Supervisory Responsibilities None",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Mattoon, IL",
    "job_city": "Mattoon",
    "job_state": "Illinois",
    "job_country": "US",
    "job_latitude": 39.4830897,
    "job_longitude": -88.37282549999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D1Eb8GME5F6h5wzAiAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55000,
    "job_max_salary": 65000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education",
        "Experience as a Data Engineer or similar software engineering role",
        "Proficient with git",
        "Good knowledge of SQL and PHP or similar languages",
        "Good knowledge of big data technologies",
        "Good knowledge of data modeling",
        "Familiar with Looker, Power BI or other BI tools",
        "Working knowledge of databases, MySQL/MariaDB, and SQL",
        "Strong understanding of retail business practices",
        "Excellent negotiation and conflict resolution skills",
        "Demonstrated ability to adapt in a fast-paced environment",
        "Strong analytical and problem-solving skills",
        "Excellent organizational skills and attention to detail",
        "Demonstrated behaviors must reflect integrity, professionalism, and confidentiality",
        "Ability to maintain a seated or standing position for extended durations",
        "Capability to lift 15 pounds periodically",
        "Able to navigate and access all facilities",
        "Skill to effectively communicate verbally with others, both in-person and via electronic devices",
        "Close vision for computer-related tasks"
      ],
      "Benefits": [
        "401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%",
        "Healthcare plans to support your needs",
        "Access to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program",
        "15% Associate Discount",
        "Dave Ramsey's SmartDollar Program",
        "Associate Assistance Program",
        "RK Cares Associate Hardship Program",
        "24/7 Chaplaincy Services",
        "The pay range for this position is $55,000 - $65,000 annualized and is bonus eligible",
        "Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs"
      ],
      "Responsibilities": [
        "As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs",
        "Monitor the performance of data pipelines that are efficient and secure",
        "Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned",
        "Responsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d",
    "_source": "new_jobs"
  },
  {
    "job_id": "19HGoOhNy4sBttccAAAAAA==",
    "job_title": "Data Engineer (Databricks)",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=7c1e5ae1ac6f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=7c1e5ae1ac6f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Data Consultant (Databricks).\n\nKey Responsibilities\n\nDesign and implement scalable data pipelines using Lakeflow and Databricks Asset Bundles\n\nDevelop high-volume ingestion pipelines and ensure data governance and security\n\nUtilize Terraform for managing Databricks workspace resources and support AWS deployment patterns\n\nRequired Qualifications\n\nHands-on experience with Databricks in production environments\n\nExpertise in PySpark and advanced SQL\n\nExperience with Delta Lake and data transformation frameworks\n\nFamiliarity with AWS infrastructure, including S3 and IAM\n\nTerraform experience specifically with Databricks and AWS resources",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Manchester, NH",
    "job_city": "Manchester",
    "job_state": "New Hampshire",
    "job_country": "US",
    "job_latitude": 42.9956397,
    "job_longitude": -71.4547891,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D19HGoOhNy4sBttccAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Hands-on experience with Databricks in production environments",
        "Expertise in PySpark and advanced SQL",
        "Experience with Delta Lake and data transformation frameworks",
        "Familiarity with AWS infrastructure, including S3 and IAM",
        "Terraform experience specifically with Databricks and AWS resources"
      ],
      "Responsibilities": [
        "Design and implement scalable data pipelines using Lakeflow and Databricks Asset Bundles",
        "Develop high-volume ingestion pipelines and ensure data governance and security",
        "Utilize Terraform for managing Databricks workspace resources and support AWS deployment patterns"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "XhgeNP4cggggaxCtAAAAAA==",
    "job_title": "(USA) Senior Manager, Data Engineering",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0XcK_pPTIiFuiAYrGraOGjkX3xDSPgjHRhL3B&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do...The Mission\nAt Sam’s Club, we are no longer just building dashboards or static pipelines; we are building the \"brain\" of the retail experience. As the Senior Engineering Manager for Agentic Data, you will be at the forefront of the AI revolution. Your mission is to evolve our massive data ecosystem into a Semantic & Contextual Layer—the foundational intelligence that allows AI Agents to reason, plan, and act autonomously for millions of members. You aren't just managing a team; you are architecting the bridge between raw data and autonomous action. You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time.\n________________________________________\nWhat You’ll Do\n• Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows.\n• Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents. You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state.\n• Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles. You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models.\n• Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations.\n• Strategic Leadership: Manage and mentor a high-performing team of engineers. You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals.\n• Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources.\n• Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows.\n________________________________________\nWhat You’ll Bring\n• Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role.\n• Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals. You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem).\n• Real-Time Expertise: Deep experience with Kafka and event-driven architectures. You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming).\n• Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI).\n• Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads.\n• Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality.\n• Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design.\n\nBenefits & Perks:\nBeyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWe believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.\n\nAbout Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.\n\nWe’re virtual\nWorking virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nThe annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years’ experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.\n4 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n1 year’s supervisory experience.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n809 11th Ave, Sunnyvale, CA 94089-4731, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "Cupertino, CA",
    "job_city": "Cupertino",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.322997799999996,
    "job_longitude": -122.03218229999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXhgeNP4cggggaxCtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 143000,
    "job_max_salary": 286000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role",
        "Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals",
        "You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem)",
        "You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming)",
        "Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI)",
        "Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads",
        "Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design",
        "Option 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field",
        "Option 2: 7 years’ experience in software engineering or related field",
        "Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field",
        "4 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "Beyond competitive pay, you can receive incentive awards for your performance",
        "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "The annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time",
        "Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows",
        "Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents",
        "You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state",
        "Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles",
        "You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models",
        "Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations",
        "Strategic Leadership: Manage and mentor a high-performing team of engineers",
        "You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals",
        "Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources",
        "Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows",
        "Real-Time Expertise: Deep experience with Kafka and event-driven architectures",
        "Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality"
      ]
    },
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-walmart-job-usa-senior-manager-data-engineering-in-cupertino-ca",
    "_source": "new_jobs"
  },
  {
    "job_id": "mLWbCE2BsJ1026TLAAAAAA==",
    "job_title": "Slalom Flex (Project Based)- Federal GCP Data Engineer",
    "employer_name": "Slalom",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTz83P7TkZM86wEdE65TUA9A-yqu267uumzStA3&s=0",
    "employer_website": "https://www.slalom.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "GCP Data Engineer (U.S. Citizenship Required)\n\nAbout Us\n\nSlalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six+ countries and 43+ markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 10,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.\n\nAbout The Role\n\nWe are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA. This role is a hands‑on engineering position working within a secure Google Cloud Platform environment to design, build, and optimize scalable data pipelines and analytics capabilities that enable high‑quality insights and operational excellence for our federal client.\n\nThis position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance.\n\nWhat You Will Do\n\nData Engineering & Cloud Development\n• Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage.\n• Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs.\n• Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation.\n\nClient Leadership & Delivery\n• Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements.\n• Translate technical designs and delivery timelines for both technical and non‑technical audiences.\n• Support modernization of legacy data environments into scalable GCP‑based architectures.\n• Ensure all solutions align with federal data governance, security, and performance standards.\n\nSolution Optimization & Innovation\n• Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling.\n• Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines.\n• Develop and maintain data lineage, metadata documentation, and enterprise data models.\n• Identify linkages across disparate datasets to build unified, interoperable data architectures.\n• Perform cleanup of existing datasets and transformation logic where needed.\n\nCollaboration & Team Leadership\n• Work closely with data architects, BI developers, cloud engineers, and data scientists.\n• Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning.\n• Track work in Jira and maintain documentation in Confluence.\n• Support testing, deployment, and quality assurance of data products.\n• Mentor junior data engineering team members and contribute to best-practice frameworks.\n\nMust-Have Qualifications\n• U.S. citizenship\n�� Ability to obtain and maintain a federal Public Trust clearance\n• 3+ years of experience in cloud-based data engineering\n• Strong hands-on expertise with Google BigQuery\n• Proficiency in Python for pipeline development, automation, and cloud integration\n• Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar\n• Strong SQL skills, including data modeling and data quality testing\n• Experience with Git-based version control and CI/CD concepts\n• Familiarity with data governance, metadata management, and compliance considerations\n• Strong communication and stakeholder engagement skills\n\nNice-to-Have Skills\n• Experience supporting federal or regulated environments\n• Familiarity with Looker and downstream BI enablement\n• Understanding of ML workloads or data structures optimized for modeling\n• Experience with Agile/Scrum or SAFe\n• Knowledge of data quality frameworks and testing strategies\n• Exposure to GCP data governance tools such as Dataplex\n• Experience with serverless architectures (Cloud Functions, Cloud Run)\n• Familiarity with JavaScript for Dataform SQLX extensibility\n• Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)\n• Experience with dbt, Dataform, Databricks, or other analytics engineering tooling\n• Relevant GCP certifications\n\nCompensation And Benefits\n\nSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses.\n\nSlalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $55/hr to $75/hr. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.\n\nEEO and Accommodations\n\nSlalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration\n\nfor employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements.\n\nSlalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the\n\nselection process. Please advise the talent acquisition team if you require accommodations during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1769990400,
    "job_posted_at_datetime_utc": "2026-02-02T00:00:00.000Z",
    "job_location": "Oak Grove, NC",
    "job_city": "Oak Grove",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.9815344,
    "job_longitude": -78.8205619,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmLWbCE2BsJ1026TLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55,
    "job_max_salary": 75,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "We are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA",
        "This position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance",
        "U.S. citizenship",
        "Ability to obtain and maintain a federal Public Trust clearance",
        "3+ years of experience in cloud-based data engineering",
        "Strong hands-on expertise with Google BigQuery",
        "Proficiency in Python for pipeline development, automation, and cloud integration",
        "Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar",
        "Strong SQL skills, including data modeling and data quality testing",
        "Experience with Git-based version control and CI/CD concepts",
        "Familiarity with data governance, metadata management, and compliance considerations",
        "Strong communication and stakeholder engagement skills",
        "Experience supporting federal or regulated environments",
        "Familiarity with Looker and downstream BI enablement",
        "Understanding of ML workloads or data structures optimized for modeling",
        "Experience with Agile/Scrum or SAFe",
        "Knowledge of data quality frameworks and testing strategies",
        "Exposure to GCP data governance tools such as Dataplex",
        "Experience with serverless architectures (Cloud Functions, Cloud Run)",
        "Familiarity with JavaScript for Dataform SQLX extensibility",
        "Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)",
        "Experience with dbt, Dataform, Databricks, or other analytics engineering tooling",
        "Relevant GCP certifications",
        "Slalom will also consider qualified applications with criminal histories, consistent with legal requirements"
      ],
      "Benefits": [
        "Slalom prides itself on helping team members thrive in their work and life",
        "As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability",
        "We also offer yearly $350 reimbursement account for any well-being-related expenses",
        "Slalom is committed to fair and equitable compensation practices",
        "For this position, the base salary pay range is $55/hr to $75/hr",
        "Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors",
        "The salary pay range is subject to change and may be modified at any time"
      ],
      "Responsibilities": [
        "Data Engineering & Cloud Development",
        "Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage",
        "Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs",
        "Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation",
        "Client Leadership & Delivery",
        "Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements",
        "Translate technical designs and delivery timelines for both technical and non‑technical audiences",
        "Support modernization of legacy data environments into scalable GCP‑based architectures",
        "Ensure all solutions align with federal data governance, security, and performance standards",
        "Solution Optimization & Innovation",
        "Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling",
        "Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines",
        "Develop and maintain data lineage, metadata documentation, and enterprise data models",
        "Identify linkages across disparate datasets to build unified, interoperable data architectures",
        "Perform cleanup of existing datasets and transformation logic where needed",
        "Collaboration & Team Leadership",
        "Work closely with data architects, BI developers, cloud engineers, and data scientists",
        "Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning",
        "Track work in Jira and maintain documentation in Confluence",
        "Support testing, deployment, and quality assurance of data products",
        "Mentor junior data engineering team members and contribute to best-practice frameworks",
        "Slalom welcomes and encourages applications from individuals with disabilities"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712",
    "_source": "new_jobs"
  },
  {
    "job_id": "vFBZM-83PchAIwxdAAAAAA==",
    "job_title": "Data Engineer, Prime Video Core Analytics and Tooling",
    "employer_name": "Amazon",
    "employer_logo": null,
    "employer_website": "https://www.amazon.com",
    "job_publisher": "Women For Hire- Job Board",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.womenforhire.com/job/usa/tamalpais-valley-ca/data-engineer-prime-video-core-analytics-and-tooling-393303/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Women For Hire- Job Board",
        "apply_link": "https://jobs.womenforhire.com/job/usa/tamalpais-valley-ca/data-engineer-prime-video-core-analytics-and-tooling-393303/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer, Prime Video Core Analytics and Tooling\n\nJob ID: 3139902 | Amazon Digital UK Limited\n\nCome build the future of entertainment with us. Prime Video is a premium streaming service that offers customers a vast collection of TV shows and movies with the ease of finding what they love to watch in one place. We offer thousands of popular movies and TV shows from Originals and Exclusive content to exciting live sports events, as well as add-on channels, rental or purchase options for new release content. Prime Video is a fast-paced, growth business available in over 240 countries and territories worldwide. The team owns a global data platform that powers analytics and data science within Prime Video, built on AWS cloud technology to handle large relational data volumes, with a focus on security, latency and usability. The team abstracts complexity from the analytics community to enable rapid innovation.\n\nKey job responsibilities\n• Solve data warehousing problems at scale and apply cloud-based AWS services to challenges around big data processing, data warehouse design, self-service data access, automated data quality detection, and infrastructure as code.\n• Contribute to automation and optimization for all areas of DW/ETL maintenance and deployment.\n• Collaborate with global business partners and technical teams on non-standard problems, delivering data products that underpin Prime Video strategic decision making from content selection to on-platform customer experience.\n• Develop efficient systems and tools to process data, scalable to seasonal spikes and adaptable to future growth.\n• Ensure your work has a direct impact on day-to-day decision making across Prime Video.\n\nBasic Qualifications\n• Experience in at least one modern scripting or programming language (e.g., Python, Java, Scala, NodeJS).\n• Experience with big data technologies (e.g., Hadoop, Hive, Spark, EMR).\n• Experience building/operating highly available, distributed data extraction, ingestion, and processing of large data sets.\n• Experience with data modeling, warehousing and building ETL pipelines.\n• Knowledge of distributed systems as they pertain to data storage and computing.\n• Knowledge of professional software engineering and best practices for full software development life cycle (coding standards, architectures, code reviews, source control, continuous deployments, testing and operational excellence).\n• Experience as a Data Engineer or in a similar role.\n• Experience with SQL.\n\nPreferred Qualifications\n• Experience with AWS technologies such as Redshift, S3, AWS Glue, EMR, Kinesis, Firehose, Lambda, and IAM roles and permissions.\n• Experience delivering end-to-end projects independently.\n\nAmazon is an equal opportunities employer. We believe that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. Protecting your privacy and the security of your data is a longstanding priority. Please consult our Privacy Notice to learn how we collect, use and transfer personal data of candidates.\n\nAmazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.\n\nOur inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit the accommodations page for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.\n\nPosted: January 10, 2026 (Updated 14 days ago)\n\nPosted: September 26, 2025 (Updated about 2 months ago)",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Tamalpais-Homestead Valley, CA",
    "job_city": "Tamalpais-Homestead Valley",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.879069,
    "job_longitude": -122.52937259999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvFBZM-83PchAIwxdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Experience in at least one modern scripting or programming language (e.g., Python, Java, Scala, NodeJS)",
        "Experience with big data technologies (e.g., Hadoop, Hive, Spark, EMR)",
        "Experience building/operating highly available, distributed data extraction, ingestion, and processing of large data sets",
        "Experience with data modeling, warehousing and building ETL pipelines",
        "Knowledge of distributed systems as they pertain to data storage and computing",
        "Knowledge of professional software engineering and best practices for full software development life cycle (coding standards, architectures, code reviews, source control, continuous deployments, testing and operational excellence)",
        "Experience as a Data Engineer or in a similar role",
        "Experience with SQL"
      ],
      "Responsibilities": [
        "Solve data warehousing problems at scale and apply cloud-based AWS services to challenges around big data processing, data warehouse design, self-service data access, automated data quality detection, and infrastructure as code",
        "Contribute to automation and optimization for all areas of DW/ETL maintenance and deployment",
        "Collaborate with global business partners and technical teams on non-standard problems, delivering data products that underpin Prime Video strategic decision making from content selection to on-platform customer experience",
        "Develop efficient systems and tools to process data, scalable to seasonal spikes and adaptable to future growth",
        "Ensure your work has a direct impact on day-to-day decision making across Prime Video"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-womenforhire-com-job-usa-tamalpais-valley-ca-data-engineer-prime-video-core-analytics-and-tooling-393303",
    "_source": "new_jobs"
  },
  {
    "job_id": "h69CGzgflQdWtmsHAAAAAA==",
    "job_title": "Advanced Cloud Data Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=4d4ccca8d83f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=4d4ccca8d83f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for an Advanced Cloud Data Engineer for a contract position that is 100% remote.\n\nKey Responsibilities\n\nComplete analysis, design, and development of BI solutions\n\nDatabase development primarily in SSIS, Databricks, and SQL\n\nCollaborate with other developers to create and implement optimal solutions\n\nRequired Qualifications\n\nBachelor's degree in Data Analytics, MIS, Computer Science, or related area\n\n3+ years of experience in data engineering within a data warehouse\n\n3+ years of experience designing and developing ETLs with tools like SSIS, Databricks, or Python\n\nExperience working as part of an Agile Scrum team",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Raleigh, NC",
    "job_city": "Raleigh",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.779589699999995,
    "job_longitude": -78.6381787,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dh69CGzgflQdWtmsHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree in Data Analytics, MIS, Computer Science, or related area",
        "3+ years of experience in data engineering within a data warehouse",
        "3+ years of experience designing and developing ETLs with tools like SSIS, Databricks, or Python",
        "Experience working as part of an Agile Scrum team"
      ],
      "Responsibilities": [
        "Complete analysis, design, and development of BI solutions",
        "Database development primarily in SSIS, Databricks, and SQL",
        "Collaborate with other developers to create and implement optimal solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "xu-VGehYTuGfUSZxAAAAAA==",
    "job_title": "Data Engineer - Python",
    "employer_name": "Apptad Inc",
    "employer_logo": null,
    "employer_website": "https://apptad.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Overview\n\nWe are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions. This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment.\n\nKey Responsibilities\n• Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory.\n• Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions.\n• Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing.\n• Implement data migration and transformation workflows leveraging Azure cloud services.\n• Ensure data quality, performance tuning, and optimization of data pipelines.\n• Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects.\n• Apply best practices for data governance, privacy, and security within Azure environments.\n• Lead the design and implementation of both batch and streaming data pipelines.\n• Provide technical leadership and mentorship to team members.\n• Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions.\n• Troubleshoot and resolve issues to ensure high availability and reliability.\n• Participate in architectural discussions and develop coding standards and security guidelines.\n• Drive automation of data workflows and improve operational efficiency.\n• Support production environments with timely incident response and root cause analysis.\n\nRequired Qualifications\n• 5 to 7 years of relevant experience in data engineering or related fields.\n• Expertise in Python programming.\n• Proficiency in Azure Data Factory and its ecosystem.\n\nPreferred Qualifications\n• Familiarity with Azure Data Lake and other Azure cloud services.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "Cockrell Hill, TX",
    "job_city": "Cockrell Hill",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7362421,
    "job_longitude": -96.8869481,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dxu-VGehYTuGfUSZxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions",
        "5 to 7 years of relevant experience in data engineering or related fields",
        "Expertise in Python programming",
        "Proficiency in Azure Data Factory and its ecosystem"
      ],
      "Responsibilities": [
        "This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment",
        "Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory",
        "Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions",
        "Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing",
        "Implement data migration and transformation workflows leveraging Azure cloud services",
        "Ensure data quality, performance tuning, and optimization of data pipelines",
        "Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects",
        "Apply best practices for data governance, privacy, and security within Azure environments",
        "Lead the design and implementation of both batch and streaming data pipelines",
        "Provide technical leadership and mentorship to team members",
        "Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions",
        "Troubleshoot and resolve issues to ensure high availability and reliability",
        "Participate in architectural discussions and develop coding standards and security guidelines",
        "Drive automation of data workflows and improve operational efficiency",
        "Support production environments with timely incident response and root cause analysis"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-2vqttgf6jgfthenvnt8bjz",
    "_source": "new_jobs"
  },
  {
    "job_id": "yJN3Vkow8jzMypyxAAAAAA==",
    "job_title": "Sr Data Engineer 20",
    "employer_name": "LegitScript",
    "employer_logo": null,
    "employer_website": "https://www.legitscript.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-ma-all-cities-sr-data-engineer-20-legitscript-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-ma-all-cities-sr-data-engineer-20-legitscript-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At LegitScript, we are passionate about making the internet and payment ecosystems safer and more transparent.\nWe help companies of all sizes keep their services legal and safe for consumers.\nTo do this, LegitScript combines big data with the world's leading team of experts skilled in highly regulated and complex sectors, including transaction laundering detection, pharmaceuticals, online gambling, and more.\n\nThe result? Unmatched accuracy and deep risk analysis that identifies which commercial entities play by the rules, and which do not.\nOur diverse industry partnerships provide unique insights that keep businesses and governments at the forefront of emerging trends.\nThat's why LegitScript is trusted by the world's largest search engines, internet platforms, payment companies, and regulatory agencies.\n\nOverview:\n\nWe're an innovative technology incubator seeking an experienced and forward-thinking Sr Data Engineer specializing in Generative AI to join our team.\nIn this role, you'll spearhead the development and implementation of cutting-edge AI solutions, with a primary focus on creating a sophisticated risk detection algorithm using large language models, Generative AI techniques, and traditional machine learning methods within our SaaS environment.\n\nWhat You'll Do:\n• Design, build, and maintain scalable data pipelines to ingest data from disparate sources into our data warehouse/lake.\n• Research and develop high-performance machine learning models to solve complex business problems.\n• Wrap models into production-ready APIs and integrate them into our core product.\n• Implement automated workflows for data validation, model training, and continuous deployment (CI/CD for ML).\n• Monitor pipeline latency and model drift, ensuring that the system remains performant and accurate as data evolves.\n\nWhat You'll Bring:\n• 5-8+ years in a Data Engineering or Data Science role, with a proven track record of shipping models to production.\n• Advanced proficiency in Structured Query Language for complex data transformation and analysis.\n• Hands-on experience with cloud-based data platforms such as Databricks or Snowflake.\n• Experience with ETL and ELT tools or frameworks such as Lakeflow Declarative Pipelines, Databricks Autoloader, Informatica, Talend, or dbt.\n• Strong proficiency in Python, Spark/PySpark, and DABs/Terraform for data processing and pipeline development.\n• Strong understanding of data modeling, database design principles, and building curated datasets for analytics and operational use cases.\n• Experience with DevOps practices including IAC, CI/CD, Git-based development, branching strategies, and code reviews.\n• Proven history implementing continuous integration and continuous deployment for data pipelines and managing deployments across environments.\n• Familiarity with orchestration and workflow tools such as Databricks Workflows or Airflow is preferred.\n• Previous experience working with containerization technologies such as Docker\n• Proficiency with ML experiment tracking tools like MLFlow or Weights & Biases\n• Design ML models that do the heavy lifting-prioritizing tasks and automating risk assessment to make our operations smarter.\n• Ensure every prediction is explainable, turning \"black box\" code into actionable \"reason codes\" for our end users.\n• Partner directly with the teams using your tools to refine features and improve model relevance based on their feedback.\n• Own the success of your models by measuring their real-world efficacy, focusing on business ROI.\n\nIn addition to competitive salaries, full-time employees enjoy a great benefits package:\n• Multiple Medical, Dental & Vision plans\n• 401k with company match and immediate vesting\n• Generous paid time off package and 11 paid holidays\n• And much more!\n\nIf you got to this point, we hope you're feeling excited about the job description you just read.\nEven if you don't feel that you meet every single requirement, we still encourage you to apply.\nWe're eager to meet people that believe in LegitScript's mission and can contribute to our team in a variety of ways.\n\nThis job description is not designed to cover or contain a comprehensive listing of all activities, duties or responsibilities that are required of the employee.\nDuties, responsibilities and activities may change or new ones may be assigned at any time with or without notice.\n\nPlease note that visa sponsorship is not available for this position.\nWe cannot support international remote work.\n\nWe do not accept unsolicited applications from third-party recruiters or agencies for this job posting.\nAny candidate submission without a prior agreement will be considered the property of our company, and we will not be responsible for any fees or obligations related to such submissions.\nWe encourage interested candidates to apply directly through our official channels.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Massachusetts",
    "job_city": null,
    "job_state": "Massachusetts",
    "job_country": "US",
    "job_latitude": 42.4072107,
    "job_longitude": -71.3824374,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DyJN3Vkow8jzMypyxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5-8+ years in a Data Engineering or Data Science role, with a proven track record of shipping models to production",
        "Advanced proficiency in Structured Query Language for complex data transformation and analysis",
        "Hands-on experience with cloud-based data platforms such as Databricks or Snowflake",
        "Experience with ETL and ELT tools or frameworks such as Lakeflow Declarative Pipelines, Databricks Autoloader, Informatica, Talend, or dbt",
        "Strong proficiency in Python, Spark/PySpark, and DABs/Terraform for data processing and pipeline development",
        "Strong understanding of data modeling, database design principles, and building curated datasets for analytics and operational use cases",
        "Experience with DevOps practices including IAC, CI/CD, Git-based development, branching strategies, and code reviews",
        "Proven history implementing continuous integration and continuous deployment for data pipelines and managing deployments across environments",
        "Previous experience working with containerization technologies such as Docker",
        "Proficiency with ML experiment tracking tools like MLFlow or Weights & Biases"
      ],
      "Benefits": [
        "In addition to competitive salaries, full-time employees enjoy a great benefits package:",
        "Multiple Medical, Dental & Vision plans",
        "401k with company match and immediate vesting",
        "Generous paid time off package and 11 paid holidays",
        "And much more!"
      ],
      "Responsibilities": [
        "Design, build, and maintain scalable data pipelines to ingest data from disparate sources into our data warehouse/lake",
        "Research and develop high-performance machine learning models to solve complex business problems",
        "Wrap models into production-ready APIs and integrate them into our core product",
        "Implement automated workflows for data validation, model training, and continuous deployment (CI/CD for ML)",
        "Monitor pipeline latency and model drift, ensuring that the system remains performant and accurate as data evolves",
        "Design ML models that do the heavy lifting-prioritizing tasks and automating risk assessment to make our operations smarter",
        "Ensure every prediction is explainable, turning \"black box\" code into actionable \"reason codes\" for our end users",
        "Partner directly with the teams using your tools to refine features and improve model relevance based on their feedback",
        "Own the success of your models by measuring their real-world efficacy, focusing on business ROI"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-jobilize-com-job-us-ma-all-cities-sr-data-engineer-20-legitscript-hiring-now-job-immediately",
    "_source": "new_jobs"
  },
  {
    "job_id": "UBaAiKRJzBskW0c3AAAAAA==",
    "job_title": "Distinguished Data Engineer- Card",
    "employer_name": "Capital One",
    "employer_logo": null,
    "employer_website": "https://www.capitalone.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time and Part-time",
    "job_employment_types": [
      "FULLTIME",
      "PARTTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/data-management?id=2455126404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-management?id=2455126404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/mclean/virginia/info_technology/4669622749/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Join to apply for the Distinguished Data Engineer- Card role at Capital One\n\nDistinguished Data Engineers are individual contributors who strive to be diverse in thought so we visualize the problem space. At Capital One, we believe diversity of thought strengthens our ability to influence, collaborate and provide the most innovative solutions across organizational boundaries. Distinguished Engineers will significantly impact our trajectory and devise clear roadmaps to deliver next generation technology solutions.\nDeep technical experts and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices Visionaries, collaborating on Capital One’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates Role models and mentors, helping to coach and strengthen the technical expertise and know-how of our engineering and product community Evangelists, both internally and externally, helping to elevate the Distinguished Engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities\n\nAbout the team: Capital One’s Card Core Architecture team is responsible for Card modernization’s data strategy, including data abstraction, data standardization, data products, GraphQL and API services to deliver reads and writes to clients, and analytical data strategy to enable our business to drive value across the customer lifecycle with real time intelligence and agility.\n\nIn this role, you will work on delivering Proof-of-concepts, actionable architectures and designs with hands-on deep technical expertise. You will act as a bridge between technical delivery and product management. You will work with other Distinguished Engineers to detail high level designs to help software engineers develop the products as envisioned. You will provide mentorship and strategic guidance to our engineer, product, and leadership groups.\n\nResponsibilities:\nBuild awareness, increase knowledge and drive adoption of modern technologies, sharing consumer and engineering benefits to gain buy-in Strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader Capital One team Promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in a unified manner Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent\n\nBasic Qualifications:\nBachelor’s Degree At least 7 years of experience in data engineering At least 3 years of experience in data architecture At least 2 years of experience building applications in AWS\n\nPreferred Qualifications:\nMasters’ Degree 9+ years of experience in data architecture, engineering and delivery 5+ years of experience developing in Python, Java, Scala or SQL 3+ years of architecting experience with AWS services such as S3, Glue, Aurora Global, DynamoDB, Kinesis, Lambda 3+ years of hands-on experience with data lakes including AVRO, Parquet, Iceberg, Delta, Spark, Kafka, Flink technologies 3+ years of data modeling experience 2+ years of experience with ontology standards for defining a domain 1+ year of experience deploying machine learning models 3+ years of experience implementing big data processing solutions on AWS 3+ years of experience in building highly resilient distributed data systems\n\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Salaries for part-time roles will be prorated based upon hours worked. Locations include Chicago, IL; McLean, VA; New York, NY; Plano, TX; Richmond, VA; San Francisco, CA; Wilmington, DE; and other locations with pay ranges adjusted accordingly. The actual offer will be reflected in the candidate’s offer letter. This role is eligible for performance-based incentive compensation.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits. Eligibility varies by status. This role is expected to accept applications for a minimum of 5 business days.\n\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable laws. Capital One promotes a drug-free workplace. Details on accommodations for applicants with disabilities are available from Capital One Recruiting.\n\nSeniority level: Mid-Senior level\n\nEmployment type: Full-time\n\nJob function: Information Technology\n\nReferrals increase your chances of interviewing at Capital One by 2x\n\nGet notified about new Data Engineer jobs in McLean, VA.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "McLean, VA",
    "job_city": "McLean",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.9338676,
    "job_longitude": -77.1772604,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DUBaAiKRJzBskW0c3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-data-management",
    "_source": "new_jobs"
  },
  {
    "job_id": "B94kz2kLrz_jTu22AAAAAA==",
    "job_title": "Manager, HR Data Engineer",
    "employer_name": "Metropolitan Transportation Authority",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJ2luycnmBz43jpzLUO-YuE-x2KemrXhe2VUsU&s=0",
    "employer_website": null,
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/manager-hr-data-engineer-metropolitan-transportation-authority-JV_KO0,24_KE25,62.htm?jl=1009935451706&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/manager-hr-data-engineer-metropolitan-transportation-authority-JV_KO0,24_KE25,62.htm?jl=1009935451706&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talents By Vaia",
        "apply_link": "https://talents.vaia.com/companies/unavailable/manager-data-engineer-41508266/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Winzons.com",
        "apply_link": "https://usa.winzons.com/job/manager-data-operations-data-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description\n\nJOB TITLE:\n\nManager, HR Data Engineer\n\nDEPT/DIV:\n\nPeople Department\n\nWORK LOCATION:\n\n2 Broadway\n\nFULL/PART-TIME\n\nFULL\n\nSALARY RANGE:\n\n$110,000 - $123,000\n\nDEADLINE:\n\nUntil filled\n\nThis position is eligible for teleworking, which is currently one day per week. New hires are eligible to apply 30 days after their effective hire date.\n\nOpening:\n\nThe Metropolitan Transportation Authority is North America's largest transportation network, serving a population of 15.3 million people across a 5,000-square-mile travel area surrounding New York City, Long Island, southeastern New York State, and Connecticut. The MTA network comprises the nation’s largest bus fleet and more subway and commuter rail cars than all other U.S. transit systems combined. MTA strives to provide a safe and reliable commute, excellent customer service, and rewarding opportunities.\n\nPosition Objective:\n\nThis position will report to the Director of HR Data Science and support the day-to-day data pipeline initiatives to design, build, and maintain ease for data structures to facilitate reporting and monitor key performance indicators. The incumbent will collaborate across Human Capital Management disciplines to identify internal/external data sources to design table structure, define ETL strategy, automate quality assurance checks, and implement scalable ETL solutions.\n\nResponsibilities:\n• Ensure that all assignments are completed with the highest quality and within agreed-upon Service Level agreement guidelines and Key Performance Indicator (KPI) targets.\n• Create and conduct a project/architecture design review.\n• Proficient knowledge of Azure Data Factory, Databricks, & Azure Delta Lake.\n• Experience programming languages (e.g., Python, R).\n• Working experience in data extraction using API.\n• Develop HR data pipelines and reports with advanced SQL programming language and maintain Data Warehousing/Data Lakes/Data Hubs and Analytical reporting Environment.\n• Work with IT teams to collect required data from internal and external systems and troubleshoot HR system issues.\n• Design and build modern data management solutions and create POC when necessary to test new approaches.\n• Create runbooks and actionable alerts as part of the development process.\n• Perform SQL and ETL tuning as necessary.\n• Perform ad hoc analysis as necessary.\n• Identify and implement continuous improvement initiatives as assigned.\n• Other duties as assigned.\n\nQualifications:\n\nKnowledge/Skills/Abilities:\n• Strong understanding of data modeling principles, including Dimensional modeling, data normalization principles, etc.\n• Proficient understanding of SQL Engines and able to develop advanced queries and analytics\n• Familiarity with data exploration/data visualization tools like MS Power BI, PeopleSoft HCM, JobVite,\n• JDXpert, Jetdocs, or Oracle Analytics CloudAbility to think strategically, analyze, and interpret Human Capital Management and Financial data.\n• Strong communication skills – written and verbal presentations.\n• Excellent conceptual and analytical reasoning competencies.\n• Comfortable working in a fast-paced and highly collaborative environment.\n• Process-oriented with excellent documentation skills, including strong Excel & PowerPoint skills, Visio flows, and mock-up creation.\n\nRequired Education and Experience:\n• Bachelor's degree in Computer Science, Information Management, Statistics, or Finance or related field. An equivalent combination of education and experience may be considered in lieu of a degree.\n• A minimum of four (4) years of relevant professional experience leading, implementing, and reporting on business key performance indicators in a data warehousing/lake/hub environment.\n• Minimum of four (4) years of experience using SQL for analytics, working with traditional relational databases and/or distributed systems such as PeopleSoft Enterprise Performance Management (EPM), Hadoop / Hive, BigQuery, Redshift, or Oracle Databases.\n\nPreferred:\n• Master's Degree in Computer Science, Information Management, or Statistics\n• Experience programming languages (e.g., Python, R) preferred.\n• Minimum of four (4) years of management experience\n• Minimum of one (1) year of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\n• Understanding of Finance Data and Human Resource Data practices and procedures.\n• Proficient Knowledge of Data Repositories/warehouses/Lakes/Hubs.\n\nOther Information\n\nMay need to work outside of normal work hours (i.e., evenings and weekends)\n\nTravel may be required to other MTA locations or other external sites.\n\nAccording to the New York State Public Officers Law & the MTA Code of Ethics, all employees who hold a policymaking position must file an Annual Statement of Financial Disclosure (FDS) with the NYS Commission on Ethics and Lobbying in Government (the “Commission”).\n\nEmployees driving company vehicles must complete defensive driver training once every three years for current MNR drivers, or within 180 days of hire or transfer for an employee entering an authorized driving position.\n\nEqual Employment Opportunity\n\nMTA and its subsidiary and affiliated agencies are Equal Opportunity Employers, including those concerning veteran status and individuals with disabilities.\n\nThe MTA encourages qualified applicants from diverse backgrounds, experiences, and abilities, including military service members, to apply.",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB94kz2kLrz_jTu22AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 110000,
    "job_max_salary": 123000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience programming languages (e.g., Python, R)",
        "Working experience in data extraction using API",
        "Strong understanding of data modeling principles, including Dimensional modeling, data normalization principles, etc",
        "Proficient understanding of SQL Engines and able to develop advanced queries and analytics",
        "Familiarity with data exploration/data visualization tools like MS Power BI, PeopleSoft HCM, JobVite,",
        "JDXpert, Jetdocs, or Oracle Analytics CloudAbility to think strategically, analyze, and interpret Human Capital Management and Financial data",
        "Strong communication skills – written and verbal presentations",
        "Excellent conceptual and analytical reasoning competencies",
        "Comfortable working in a fast-paced and highly collaborative environment",
        "Process-oriented with excellent documentation skills, including strong Excel & PowerPoint skills, Visio flows, and mock-up creation",
        "Bachelor's degree in Computer Science, Information Management, Statistics, or Finance or related field",
        "An equivalent combination of education and experience may be considered in lieu of a degree",
        "A minimum of four (4) years of relevant professional experience leading, implementing, and reporting on business key performance indicators in a data warehousing/lake/hub environment",
        "Minimum of four (4) years of experience using SQL for analytics, working with traditional relational databases and/or distributed systems such as PeopleSoft Enterprise Performance Management (EPM), Hadoop / Hive, BigQuery, Redshift, or Oracle Databases",
        "May need to work outside of normal work hours (i.e., evenings and weekends)",
        "Employees driving company vehicles must complete defensive driver training once every three years for current MNR drivers, or within 180 days of hire or transfer for an employee entering an authorized driving position"
      ],
      "Benefits": [
        "$110,000 - $123,000"
      ],
      "Responsibilities": [
        "This position will report to the Director of HR Data Science and support the day-to-day data pipeline initiatives to design, build, and maintain ease for data structures to facilitate reporting and monitor key performance indicators",
        "The incumbent will collaborate across Human Capital Management disciplines to identify internal/external data sources to design table structure, define ETL strategy, automate quality assurance checks, and implement scalable ETL solutions",
        "Ensure that all assignments are completed with the highest quality and within agreed-upon Service Level agreement guidelines and Key Performance Indicator (KPI) targets",
        "Create and conduct a project/architecture design review",
        "Proficient knowledge of Azure Data Factory, Databricks, & Azure Delta Lake",
        "Develop HR data pipelines and reports with advanced SQL programming language and maintain Data Warehousing/Data Lakes/Data Hubs and Analytical reporting Environment",
        "Work with IT teams to collect required data from internal and external systems and troubleshoot HR system issues",
        "Design and build modern data management solutions and create POC when necessary to test new approaches",
        "Create runbooks and actionable alerts as part of the development process",
        "Perform SQL and ETL tuning as necessary",
        "Perform ad hoc analysis as necessary",
        "Identify and implement continuous improvement initiatives as assigned",
        "Other duties as assigned",
        "Travel may be required to other MTA locations or other external sites"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-glassdoor-com-job-listing-manager-hr-data-engineer-metropolitan-transportation-authority-jv_ko0-24_ke25-62-htm",
    "_source": "new_jobs"
  },
  {
    "job_id": "ybwkukVOvvN7CZ4IAAAAAA==",
    "job_title": "Data Engineer Prin",
    "employer_name": "American Electric Power",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0RrFTOdrhzngcyQL-2zVSyVH1kAYBiD2Gj2L_&s=0",
    "employer_website": null,
    "job_publisher": "Tallo",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Tallo",
        "apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This job listing in Franklin - OH has been recently added. Tallo will add a summary here for this job shortly.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Franklin, OH",
    "job_city": "Franklin",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.5589474,
    "job_longitude": -84.30410739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DybwkukVOvvN7CZ4IAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "tallo-com-jobs-technology-data-engineer-oh-franklin-data-engineer-prin-5d6c21303973",
    "_source": "new_jobs"
  },
  {
    "job_id": "mQjzELr-AD780L5XAAAAAA==",
    "job_title": "IT Data Engineer - Westcor Land Title Insurance Company",
    "employer_name": "Ardán, Inc., A Community of Companies",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Learn4Good",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.learn4good.com/jobs/doral/florida/info_technology/4865773796/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/doral/florida/info_technology/4865773796/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Overview\n\nHybrid Role - must be able to commute to our Maitland, FL office\n\nPosition Summary:\n\nIn this position, you will play a crucial role in managing and optimizing our data processes within our emerging Data Warehouse/Data Lake environments. This position requires a strong foundation in ETL systems, SQL development, and a solid understanding of relational databases. You will work to ensure the efficiency and reliability of our data pipelines, contributing directly to the success of our projects.\n\nThe ideal candidate will have a strong background in data analysis and data visualization with specific expertise in using SQL to transform and analyze data. You will work closely with stakeholders across the organization to deliver actionable insights and support data-driven decision-making.\nEssential Functions\n• Collaborate with data engineers, analysts, and other stakeholders with data flows, scripts, and models using SQL, Snap Logic, and other ETL systems, optimizing for performance and reliability throughout the organization.\n• Assist in the development and testing of new features, models, or data processes within our data warehouses, leveraging feedback to drive continuous improvement.\n• Document data processes, models, and workflows within our data warehouses for internal knowledge sharing and to support ongoing learning and development.\n• Ensure integrity and quality of data throughout the entire data lifecycle, from ingestion to analysis, by implementing data validation and cleansing processes.\n• Maintain security protocols and data governance practices to ensure compliance with organizational policies and industry regulations.\n• Monitor performance and troubleshoot issues within ETL projects and related data environments.\n• Create and maintain interactive dashboards and reports using SSRS, Power BI, or similar visualization tools.\n• Ensure data quality and integrity by implementing best practices in data governance and validation.\nEducation and Experience\n• Bachelor’s degree in Computer Science, Information Systems, Data Science, or a related field.\n• 3+ years of experience in a data engineering role, with a focus on SQL development (MS SQL Server experience preferred).\n• Strong proficiency in relational databases and data querying techniques.\n• Expertise in data modeling, ETL processes, and data integration techniques.\n• Experience with using programmatic tools for data manipulation and analysis.\n• Familiarity with cloud platforms such as Azure, AWS, or GCP.\n• Knowledge of data visualization tools like Power BI, Tableau, or similar.\n• Excellent problem-solving skills and attention to detail.\n• Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\n• Ability to learn quickly, be adaptable, flexible, and creative.\n• Demonstrated capability to work well independently and as part of a team.\nPhysical Demands\n• Ability to safely and successfully perform the essential job functions consistent with the ADA, FMLA, and other federal, state, and local standards, including meeting qualitative and/or quantitative productivity standards.\n• Ability to maintain reasonably regular, punctual attendance consistent with the ADA, FMLA, and other federal, state, and local standards.\n• Must be able to sit, stand, stoop, or bend for an extended period (8 hours).\n• Must be able to listen and speak clearly over the phone.\nWork Environment\n\nThe work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The noise level in the work environment is usually moderate.\nWe offer some great perks:\n• Health, dental, and vision benefits.\n• Employer-paid disability and life insurance.\n• Flexible spending accounts.\n• 401K with company match.\n• Paid time off and company-paid holidays.\n• Wellness resources.\n\nNote: This job description is not intended to be an exhaustive list of duties, responsibilities, or qualifications associated with the job.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Doral, FL",
    "job_city": "Doral",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 25.821663599999997,
    "job_longitude": -80.3341312,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmQjzELr-AD780L5XAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 60000,
    "job_max_salary": 80000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "This position requires a strong foundation in ETL systems, SQL development, and a solid understanding of relational databases",
        "Bachelor’s degree in Computer Science, Information Systems, Data Science, or a related field",
        "Strong proficiency in relational databases and data querying techniques",
        "Expertise in data modeling, ETL processes, and data integration techniques",
        "Experience with using programmatic tools for data manipulation and analysis",
        "Familiarity with cloud platforms such as Azure, AWS, or GCP",
        "Knowledge of data visualization tools like Power BI, Tableau, or similar",
        "Excellent problem-solving skills and attention to detail",
        "Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams",
        "Ability to learn quickly, be adaptable, flexible, and creative",
        "Demonstrated capability to work well independently and as part of a team",
        "Ability to safely and successfully perform the essential job functions consistent with the ADA, FMLA, and other federal, state, and local standards, including meeting qualitative and/or quantitative productivity standards",
        "Ability to maintain reasonably regular, punctual attendance consistent with the ADA, FMLA, and other federal, state, and local standards",
        "Must be able to sit, stand, stoop, or bend for an extended period (8 hours)",
        "Must be able to listen and speak clearly over the phone"
      ],
      "Benefits": [
        "We offer some great perks:",
        "Health, dental, and vision benefits",
        "Employer-paid disability and life insurance",
        "Flexible spending accounts",
        "401K with company match",
        "Paid time off and company-paid holidays",
        "Wellness resources"
      ],
      "Responsibilities": [
        "In this position, you will play a crucial role in managing and optimizing our data processes within our emerging Data Warehouse/Data Lake environments",
        "You will work to ensure the efficiency and reliability of our data pipelines, contributing directly to the success of our projects",
        "The ideal candidate will have a strong background in data analysis and data visualization with specific expertise in using SQL to transform and analyze data",
        "You will work closely with stakeholders across the organization to deliver actionable insights and support data-driven decision-making",
        "Collaborate with data engineers, analysts, and other stakeholders with data flows, scripts, and models using SQL, Snap Logic, and other ETL systems, optimizing for performance and reliability throughout the organization",
        "Assist in the development and testing of new features, models, or data processes within our data warehouses, leveraging feedback to drive continuous improvement",
        "Document data processes, models, and workflows within our data warehouses for internal knowledge sharing and to support ongoing learning and development",
        "Ensure integrity and quality of data throughout the entire data lifecycle, from ingestion to analysis, by implementing data validation and cleansing processes",
        "Maintain security protocols and data governance practices to ensure compliance with organizational policies and industry regulations",
        "Monitor performance and troubleshoot issues within ETL projects and related data environments",
        "Create and maintain interactive dashboards and reports using SSRS, Power BI, or similar visualization tools",
        "Ensure data quality and integrity by implementing best practices in data governance and validation",
        "Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions"
      ]
    },
    "job_onet_soc": "23209300",
    "job_onet_job_zone": "2",
    "id": "www-learn4good-com-jobs-doral-florida-info_technology-4865773796-e",
    "_source": "new_jobs"
  },
  {
    "job_id": "Po2HBu6hdvNEmq0-AAAAAA==",
    "job_title": "Senior Data Engineer",
    "employer_name": "Visium SA",
    "employer_logo": null,
    "employer_website": "https://www.visium.com",
    "job_publisher": "Snagajob",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.snagajob.com/jobs/1066921924?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Snagajob",
        "apply_link": "https://www.snagajob.com/jobs/1066921924?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Senior Data Engineer\n\nType: Full-time\n\nLocation: Valencia or Barcelona\n\nAbout us\n\nAt Visium, we enable enterprise executives in defining their AI & Data strategy, execute large scale transformations and implement AI across operations, ensuring their organization becomes future-proof.\n\nWith expertise in strategy, architecture, cloud engineering, analytics, artificial intelligence and machine learning, we empower our clients to unleash and scale the power of their data.\n\nWe’re on a mission to pioneer a bright future and build future-proof and ethical organizations . Join the curious, the ambitious, the doers, the good-hearted, the ones who build a world we’re all in awe of – our Visiumees.\n\nReady to become one?\n\nRole\n\nAs a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets. Starting from raw data, you will help build robust systems to make this data usable and accessible.\n\nAs a Senior Data Engineer, you will be responsible for:\n• Assess and understand client's data landscape and assess data quality\n• Work closely with business and IT stakeholders to understand business requirements\n• Participating in technical sales activities\n• Work with high volume of heterogeneous data with distributed systems\n• Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms\n• Build and manage data infrastructure, including data storage, processing, and access\n• Implement data models and schema designs to support data analysis and reporting needs\n• Develop and maintain data quality and data governance frameworks\n• Create data visualizations and reports to communicate insights to stakeholders\n• Stay up to date with state-of-the-art data processing and analysis technologies.\n• Troubleshoot and debug data-related issues\n• Contribute to an exciting environment with a challenging team\n\nRequirements\n\nWe are looking for\n• 5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake.\n• Proficient in cloud technologies (Azure, AWS) and related data services.\n• Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight.\n• Experienced with distributed data processing frameworks like Spark.\n• Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes.\n• Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns.\n• Possesses a growth mindset, excels in problem-solving, and\n• Fluent communication both in English.\n\nThe following are a plus\n• You own one of the main Data and AI platforms certification (Databricks, Snowflake)\n• Familiarity with dbt and how to use it effectively\n• Experience in consulting is a plus\n\nBenefits\n\nWhat we offer\n• A competitive compensation package\n• A yearly education budget to steep your learning curve\n• A yearly sport budget because a fit body leads to a fit mind\n• A flexible working culture because your work-life balance matters to us\n• A position that enables you to have an impact on 1’000s of people, and the whole company's growth.\n• An international, knowledgeable, and passionate team with a strong collaborative mindset\n\nCheck our and to learn more about us & don’t hesitate to contact us if you have any questions.\n\nTitle: Senior Data Engineer\n\nType: Full-time\n\nLocation: Valencia or Barcelona\n\nAbout us\n\nAt Visium, we enable enterprise executives in defining their AI & Data strategy, execute large scale transformations and implement AI across operations, ensuring their organization becomes future-proof.\n\nWith expertise in strategy, architecture, cloud engineering, analytics, artificial intelligence and machine learning, we empower our clients to unleash and scale the power of their data.\n\nWe’re on a mission to pioneer a bright future and build future-proof and ethical organizations . Join the curious, the ambitious, the doers, the good-hearted, the ones who build a world we’re all in awe of – our Visiumees.\n\nReady to become one?\n\nRole\n\nAs a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets. Starting from raw data, you will help build robust systems to make this data usable and accessible.\n\nAs a Senior Data Engineer, you will be responsible for:\n• Assess and understand client's data landscape and assess data quality\n• Work closely with business and IT stakeholders to understand business requirements\n• Participating in technical sales activities\n• Work with high volume of heterogeneous data with distributed systems\n• Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms\n• Build and manage data infrastructure, including data storage, processing, and access\n• Implement data models and schema designs to support data analysis and reporting needs\n• Develop and maintain data quality and data governance frameworks\n• Create data visualizations and reports to communicate insights to stakeholders\n• Stay up to date with state-of-the-art data processing and analysis technologies.\n• Troubleshoot and debug data-related issues\n• Contribute to an exciting environment with a challenging team\n\nRequirements\n\nWe are looking for\n• 5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake.\n• Proficient in cloud technologies (Azure, AWS) and related data services.\n• Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight.\n• Experienced with distributed data processing frameworks like Spark.\n• Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes.\n• Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns.\n• Possesses a growth mindset, excels in problem-solving, and\n• Fluent communication both in English.\n\nThe following are a plus\n• You own one of the main Data and AI platforms certification (Databricks, Snowflake)\n• Familiarity with dbt and how to use it effectively\n• Experience in consulting is a plus\n\nBenefits\n\nWhat we offer\n• A competitive compensation package\n• A yearly education budget to steep your learning curve\n• A yearly sport budget because a fit body leads to a fit mind\n• A flexible working culture because your work-life balance matters to us\n• A position that enables you to have an impact on 1’000s of people, and the whole company's growth.\n• An international, knowledgeable, and passionate team with a strong collaborative mindset\n\nCheck our and to learn more about us & don’t hesitate to contact us if you have any questions.",
    "job_is_remote": false,
    "job_posted_at": "18 hours ago",
    "job_posted_at_timestamp": 1771030800,
    "job_posted_at_datetime_utc": "2026-02-14T01:00:00.000Z",
    "job_location": "Tome-Adelino, NM",
    "job_city": "Tome-Adelino",
    "job_state": "New Mexico",
    "job_country": "US",
    "job_latitude": 34.7318311,
    "job_longitude": -106.7175669,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPo2HBu6hdvNEmq0-AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake",
        "Proficient in cloud technologies (Azure, AWS) and related data services",
        "Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight",
        "Experienced with distributed data processing frameworks like Spark",
        "Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes",
        "Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns",
        "Possesses a growth mindset, excels in problem-solving, and",
        "Fluent communication both in English",
        "You own one of the main Data and AI platforms certification (Databricks, Snowflake)",
        "Familiarity with dbt and how to use it effectively",
        "We are looking for",
        "5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake",
        "Proficient in cloud technologies (Azure, AWS) and related data services",
        "Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight",
        "Experienced with distributed data processing frameworks like Spark",
        "Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes",
        "Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns",
        "Possesses a growth mindset, excels in problem-solving, and",
        "Fluent communication both in English",
        "You own one of the main Data and AI platforms certification (Databricks, Snowflake)",
        "Familiarity with dbt and how to use it effectively"
      ],
      "Benefits": [
        "A competitive compensation package",
        "A yearly education budget to steep your learning curve",
        "A yearly sport budget because a fit body leads to a fit mind",
        "A flexible working culture because your work-life balance matters to us",
        "A position that enables you to have an impact on 1’000s of people, and the whole company's growth",
        "An international, knowledgeable, and passionate team with a strong collaborative mindset",
        "A competitive compensation package",
        "A yearly education budget to steep your learning curve",
        "A yearly sport budget because a fit body leads to a fit mind",
        "A flexible working culture because your work-life balance matters to us",
        "A position that enables you to have an impact on 1’000s of people, and the whole company's growth",
        "An international, knowledgeable, and passionate team with a strong collaborative mindset"
      ],
      "Responsibilities": [
        "As a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets",
        "Starting from raw data, you will help build robust systems to make this data usable and accessible",
        "As a Senior Data Engineer, you will be responsible for:",
        "Assess and understand client's data landscape and assess data quality",
        "Work closely with business and IT stakeholders to understand business requirements",
        "Participating in technical sales activities",
        "Work with high volume of heterogeneous data with distributed systems",
        "Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms",
        "Build and manage data infrastructure, including data storage, processing, and access",
        "Implement data models and schema designs to support data analysis and reporting needs",
        "Develop and maintain data quality and data governance frameworks",
        "Create data visualizations and reports to communicate insights to stakeholders",
        "Stay up to date with state-of-the-art data processing and analysis technologies",
        "Troubleshoot and debug data-related issues",
        "Contribute to an exciting environment with a challenging team",
        "As a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets",
        "Starting from raw data, you will help build robust systems to make this data usable and accessible",
        "Assess and understand client's data landscape and assess data quality",
        "Work closely with business and IT stakeholders to understand business requirements",
        "Participating in technical sales activities",
        "Work with high volume of heterogeneous data with distributed systems",
        "Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms",
        "Build and manage data infrastructure, including data storage, processing, and access",
        "Implement data models and schema designs to support data analysis and reporting needs",
        "Develop and maintain data quality and data governance frameworks",
        "Create data visualizations and reports to communicate insights to stakeholders",
        "Stay up to date with state-of-the-art data processing and analysis technologies",
        "Troubleshoot and debug data-related issues",
        "Contribute to an exciting environment with a challenging team"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-snagajob-com-jobs-1066921924",
    "_source": "new_jobs"
  }
]