[
  {
    "job_id": "3fku_9fysjJRZIuuAAAAAA==",
    "job_title": "AI Analytics Engineer Washington, DC REMOTE 40",
    "employer_name": "Ark Solutions",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTI4YVw6ssg5R4ZkgAWCWYQoktfgdavC7UZx4-N&s=0",
    "employer_website": "https://arksolutionsinc.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-ri-all-cities-ai-analytics-engineer-washington-dc-remote-40-ark?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-ri-all-cities-ai-analytics-engineer-washington-dc-remote-40-ark?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "AI Analytics Engineer\n\nLocation: Washington, DC (REMOTE)\n\nDuration: 7+ months\n\nTasks to be performed:\n• Conduct a thorough assessment of the current HRTS applications landscape and document existing gaps and opportunities for AI and Automation.\n• Propose solutions and recommendations for system enhancements, including software updates, integrations, and process improvements.\n• Generate AI Models and Algorithms for HR purposes.\n• Develop project documentation, including project plans, timelines, and progress reports.\n• Communicate effectively with other team members, stakeholders, and business partners.\nRequired Skills:\n• Minimum of 3-5 years' experience in software development leveraging AI, Analytics Support, and Leadership role.\n• Four years' proven work experience as an AI Engineer or similar role.\n• Knowledge of Oracle Forms/Reports, SAP ERP, WebSphere, Tomcat, Java, Python, and GitHub.\nEducation:\n• A four-year degree from an accredited College/University in the applicable field of services is preferred or additional 4 years of relevant work experience is required.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Rhode Island",
    "job_city": null,
    "job_state": "Rhode Island",
    "job_country": "US",
    "job_latitude": 41.5800945,
    "job_longitude": -71.4774291,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D3fku_9fysjJRZIuuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Minimum of 3-5 years' experience in software development leveraging AI, Analytics Support, and Leadership role",
        "Four years' proven work experience as an AI Engineer or similar role",
        "Knowledge of Oracle Forms/Reports, SAP ERP, WebSphere, Tomcat, Java, Python, and GitHub"
      ],
      "Responsibilities": [
        "Conduct a thorough assessment of the current HRTS applications landscape and document existing gaps and opportunities for AI and Automation",
        "Propose solutions and recommendations for system enhancements, including software updates, integrations, and process improvements",
        "Generate AI Models and Algorithms for HR purposes",
        "Develop project documentation, including project plans, timelines, and progress reports",
        "Communicate effectively with other team members, stakeholders, and business partners"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "www-jobilize-com-job-us-ri-all-cities-ai-analytics-engineer-washington-dc-remote-40-ark",
    "_source": "new_jobs"
  },
  {
    "job_id": "D-LGq5o0FMB3KQT1AAAAAA==",
    "job_title": "Analytics Engineer",
    "employer_name": "Advocate Aurora Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRmPrUobFkxZ6o2Sn3cafDGFnva_mdPRfZBzxfr&s=0",
    "employer_website": "https://www.advocatehealth.org",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Advocate-Aurora-Health/Job/Analytics-Engineer/-in-Winston-Salem,NC?jid=005a1efa1fd04622&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Advocate-Aurora-Health/Job/Analytics-Engineer/-in-Winston-Salem,NC?jid=005a1efa1fd04622&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/1318aed47e1192eb94800cfeecaa7e6f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/analytics-engineer-at-atrium-health-wake-forest-baptist-4364119117?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/analytics-engineer_7ea1a473c37bb38a657b5fd8072026eeed264?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/analytics-engineer/8494007?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/analytics-engineer-atrium-health-JV_IC1139151_KO0,18_KE19,32.htm?jl=1010033785363&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/G_JVTmQOUaeMh48lerkstc3sqW34SO6Fnteixh0m9yQsb3-OJ-cmpQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/analytics-engineer-winston-salem-north-carolina-us-advocate-aurora-health-r200711?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Department:\n\n85000 Wake Forest University Health Sciences - Academic Deans Office\n\nStatus:\n\nFull time\n\nBenefits Eligible:\n\nYes\n\nHours Per Week:\n\n40\n\nSchedule Details/Additional Information:\n\nTo achieve excellence and guide strategy, Wake Forest University School of Medicine (WFUSM) seeks to have measurable, transparent, and scalable indicators of our growth and performance in core missions of education, research, and academically integrated clinical care. The CAPTURE (Centralized Analytic Platform for Performance, Transparency, Utilization, and Real-Time Excellence) initiative will develop a series of integrated data dashboards across our core missions, enabling faculty, staff, and leadership to continuously monitor and improve their performance and effectiveness at the School and across the clinical enterprise.\n\nThe DIVE team (Data, Integration, Visualization, Evaluation) is seeking an analytics engineer to drive growth and maintenance of education and research data warehouses, academic applications, and the informatics components of strategic projects. This role will specifically support the CAPTURE initiative under the leadership of the inaugural Office of Institutional Learning and Transformation at WFUSM.\n\nThe analytics engineer will be responsible for transforming raw data into robust, high-level models that support reporting and dashboard creation. This role emphasizes model design and implementation, data quality, standardization, and documentation. The analytics engineer will collaborate closely with data engineers and BI/reporting teams, apply strong SQL and dimensional modeling skills, and may develop subject matter expertise in specific data domains while maintaining broad familiarity to ensure cross-team support and avoid silos.\n\nProficiency in Python required, Snowflake and dbt core preferred.\n\nPay Range\n$30.70 - $46.05\n\nEDUCATION/EXPERIENCE: Bachelor's degree with computer courses. Two years' experience in computer programming or operations research; or, an equivalent combination of education and experience in computer programming. LICENSURE, CERTIFICATION, and/or REGISTRATION: N/A ESSENTIAL FUNCTIONS: 1. Oversees all phases of database design, development, management and reporting for medium-sized projects. 2. Consults with users on project design needs. Maintains a professional relationship with project personnel. 3. Maintains software and troubleshoot simple software products. 4. Performs a range of ad hoc queries to databases and the implementation of automated reporting schemes. Analyzes quality control needs of moderate sized projects. 5. Performs a wide variety of data transfers and conversions. 6. Creates and maintains user manuals and documentation of software products. 7. Design and implements a comprehensive software testing plan for software products. 8. Performs other related duties incidental to work described herein. SKILLS/QUALIFICATIONS: Strong initiative and proven ability to work independently Ability to communicate on a professional level with customers and staff Superior problem solving skills WORK ENVIRONMENT: Comfortable, well-lit office setting Occasionally subject to long and/or irregular hours Subject to interruptions PHYSICAL REQUIREMENTS: 0% 35% 65% to to to 35% 65% 100% N/A Activity X Standing X Walking X Sitting X Bending X Reaching with arms X Finger and hand dexterity X Talking X Hearing X Seeing Lifting, carrying, pushing and or pulling: X 20 lbs. maximum X 50 lbs. maximum X 100 lbs. maximum\n\nOur CommitmenttoYou:\n\nAdvocate Health offers a comprehensive suite of Total Rewards: benefits and well-being programs, competitive compensation, generous retirement offerings, programs that invest in your career development and so much more - so you can live fully at and away from work, including:\n\nCompensation\n• Base compensation listed within the listed pay range based on factors such as qualifications, skills, relevant experience, and/or training\n• Premium pay such as shift, on call, and more based on a teammate's job\n• Incentive pay for select positions\n• Opportunity for annual increases based on performance\n\nBenefits and more\n• Paid Time Off programs\n• Health and welfare benefits such as medical, dental, vision, life, andShort- and Long-Term Disability\n• Flexible Spending Accounts for eligible health care and dependent care expenses\n• Family benefits such as adoption assistance and paid parental leave\n• Defined contribution retirement plans with employer match and other financial wellness programs\n• Educational Assistance Program\n\nAbout Advocate Health\n\nAdvocate Health is the third-largest nonprofit, integrated health system in the United States, created from the combination of Advocate Aurora Health and Atrium Health. Providing care under the names Advocate Health Care in Illinois; Atrium Health in the Carolinas, Georgia and Alabama; and Aurora Health Care in Wisconsin, Advocate Health is a national leader in clinical innovation, health outcomes, consumer experience and value-based care. Headquartered in Charlotte, North Carolina, Advocate Health services nearly 6 million patients and is engaged in hundreds of clinical trials and research studies, with Wake Forest University School of Medicine serving as the academic core of the enterprise. It is nationally recognized for its expertise in cardiology, neurosciences, oncology, pediatrics and rehabilitation, as well as organ transplants, burn treatments and specialized musculoskeletal programs. Advocate Health employs 155,000 teammates across 69 hospitals and over 1,000 care locations, and offers one of the nation's largest graduate medical education programs with over 2,000 residents and fellows across more than 200 programs. Committed to providing equitable care for all, Advocate Health provides more than $6 billion in annual community benefits.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Winston-Salem, NC",
    "job_city": "Winston-Salem",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 36.0948221,
    "job_longitude": -80.2434028,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DD-LGq5o0FMB3KQT1AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 30.7,
    "job_max_salary": 46.05,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "EDUCATION/EXPERIENCE: Bachelor's degree with computer courses",
        "Two years' experience in computer programming or operations research; or, an equivalent combination of education and experience in computer programming",
        "LICENSURE, CERTIFICATION, and/or REGISTRATION: N/A ESSENTIAL FUNCTIONS: 1",
        "Maintains a professional relationship with project personnel",
        "SKILLS/QUALIFICATIONS: Strong initiative and proven ability to work independently Ability to communicate on a professional level with customers and staff Superior problem solving skills WORK ENVIRONMENT: Comfortable, well-lit office setting Occasionally subject to long and/or irregular hours Subject to interruptions PHYSICAL REQUIREMENTS: 0% 35% 65% to to to 35% 65% 100% N/A Activity X Standing X Walking X Sitting X Bending X Reaching with arms X Finger and hand dexterity X Talking X Hearing X Seeing Lifting, carrying, pushing and or pulling: X 20 lbs",
        "maximum X 50 lbs",
        "maximum X 100 lbs"
      ],
      "Benefits": [
        "$30.70 - $46.05",
        "Advocate Health offers a comprehensive suite of Total Rewards: benefits and well-being programs, competitive compensation, generous retirement offerings, programs that invest in your career development and so much more - so you can live fully at and away from work, including:",
        "Base compensation listed within the listed pay range based on factors such as qualifications, skills, relevant experience, and/or training",
        "Premium pay such as shift, on call, and more based on a teammate's job",
        "Incentive pay for select positions",
        "Opportunity for annual increases based on performance",
        "Benefits and more",
        "Paid Time Off programs",
        "Health and welfare benefits such as medical, dental, vision, life, andShort- and Long-Term Disability",
        "Flexible Spending Accounts for eligible health care and dependent care expenses",
        "Family benefits such as adoption assistance and paid parental leave",
        "Defined contribution retirement plans with employer match and other financial wellness programs",
        "Educational Assistance Program",
        "Committed to providing equitable care for all, Advocate Health provides more than $6 billion in annual community benefits"
      ],
      "Responsibilities": [
        "The CAPTURE (Centralized Analytic Platform for Performance, Transparency, Utilization, and Real-Time Excellence) initiative will develop a series of integrated data dashboards across our core missions, enabling faculty, staff, and leadership to continuously monitor and improve their performance and effectiveness at the School and across the clinical enterprise",
        "The DIVE team (Data, Integration, Visualization, Evaluation) is seeking an analytics engineer to drive growth and maintenance of education and research data warehouses, academic applications, and the informatics components of strategic projects",
        "This role will specifically support the CAPTURE initiative under the leadership of the inaugural Office of Institutional Learning and Transformation at WFUSM",
        "The analytics engineer will be responsible for transforming raw data into robust, high-level models that support reporting and dashboard creation",
        "This role emphasizes model design and implementation, data quality, standardization, and documentation",
        "The analytics engineer will collaborate closely with data engineers and BI/reporting teams, apply strong SQL and dimensional modeling skills, and may develop subject matter expertise in specific data domains while maintaining broad familiarity to ensure cross-team support and avoid silos",
        "Oversees all phases of database design, development, management and reporting for medium-sized projects",
        "Consults with users on project design needs",
        "Maintains software and troubleshoot simple software products",
        "Performs a range of ad hoc queries to databases and the implementation of automated reporting schemes",
        "Analyzes quality control needs of moderate sized projects",
        "Performs a wide variety of data transfers and conversions",
        "Creates and maintains user manuals and documentation of software products",
        "Design and implements a comprehensive software testing plan for software products",
        "Performs other related duties incidental to work described herein"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-advocate-aurora-health-job-analytics-engineer-in-winston-salem-nc",
    "_source": "new_jobs"
  },
  {
    "job_id": "h4Px_slRHIXshjaHAAAAAA==",
    "job_title": "Data Analytics Engineer II - IM Enterprise Data",
    "employer_name": "Christus Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRcDADXYW44OCBLPAj4Y3AN8FEYdbU07Pe0pOTp&s=0",
    "employer_website": "https://www.christushealth.org",
    "job_publisher": "Women For Hire- Job Board",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.womenforhire.com/job/usa/carrollton-tx/data-analytics-engineer-ii-im-enterprise-data-237234/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Women For Hire- Job Board",
        "apply_link": "https://jobs.womenforhire.com/job/usa/carrollton-tx/data-analytics-engineer-ii-im-enterprise-data-237234/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description\n\nSummary:\n\nA Data and Analytics Engineer II is responsible for development, expansion, and maintenance of data pipelines of the echo system. A Data and Analytics Engineer uses programming skills to develop, customize and manage integration tools, databases, warehouses, and analytical systems.\n\nThe Data and Analytics Engineer II is responsible for implementation of optimal solutions to integrate, store, process and analyze huge data sets. This includes an understanding of methodology, specifications, programming, delivery, monitoring, and support standards.\n\nIndividual must have advanced knowledge of designing and developing data pipelines and delivering advanced analytics, with open-source Big Data processing frameworks such as Hadoop technologies. Individual must have proven competency in programming utilizing distributed computing principles.\n\nResponsibilities:\n• Meets expectations of the applicable OneCHRISTUS Competencies: Leader of Self, Leader of Others, or Leader of Leaders.\n• Responsible for analyzing and understanding data sources, participating in requirement gathering, and providing insights and guidance on data technology and data modeling best practices.\n• Analyzes ideas and business and functional requirements to formulate a design strategy.\n• Acts as a tenant to draw out a workable application design and coding parameters with essential functionalities.\n• Works in collaboration with the team members to identify and address the issues by implementing a viable technical solution that is time and cost-effective and ensuring that it does not affect performance quality.\n• Develops code following the industry's best practices and adhere to the organizational development rules and standards.\n• Involved in the evaluation of proposed system acquisitions or solutions development and provides input to the decision-making process relative to compatibility, cost, resource requirements, operations, and maintenance.\n• Integrates software components, subsystems, facilities, and services into the existing technical systems environment; assesses the impact on other systems and works with cross-functional teams within information Services to ensure positive project impact. Installs configure and verify the operation of software components.\n• Participates in the development of standards, design, and implementation of proactive processes to collect and report data and statistics on assigned systems.\n• Participates in the research, design, development, and implementation of application, database, and interface using technologies platforms provided.\n• Researching, designing, implementing, and managing programs.\n• Fixes problems arising across the test cycles and continuously improve the quality of deliverables.\n• References and documents each phase of development for further reference and maintenance operation.\n• Uses critical and analytical thinking skills and understanding of programming principles and design.\n• Uses strong technical knowledge of Enterprise Application/Integration Design and develop systems, databases, operating systems, and Information Services.\n• Uses experience in large scale data lake data and data warehouse implementations and demonstrate proficiency in open-source technology, for example, Python, Spark, Hive, HDFS, NiFi etc.\n• Demonstrates substantial experience and a deep knowledge of data mining techniques, relational, and non-relational databases.\n• Demonstrates experience with data integration with ETL techniques and frameworks.\n• Works with Big Data querying tools, such as Hive, Impala and Spark SQL.\n• Builds stream-processing systems, using solutions such as NiFi or Spark-Streaming.\n• Demonstrates good understanding of Lambda Architecture\n• Uses intermediate level of SQL programing and query performance tuning techniques for Data Integration and Consumption using design for optimum performance against large data asset within Transactional, MPP and columnar architecture.\n• Demonstrates solid understanding of BI and analytics landscape, preferable in large-scale development environments.\n\nJob Requirements:\n\nEducation/Skills\n• Bachelor’s degree in computer science, Engineering, Math, or related field is required.\n\nExperience\n• Minimum of three (3) years of experience in MapReduce, Spark programming.\n• Minimum of three (3) years of experience developing analytics solutions with large data sets within an OLAP and MPP architecture.\n• Minimum of five (5) years of experience with design, architecture and development of Enterprise scale platforms built on open-source frameworks.\n• Three (3) years of experience with working in a Microsoft SQL Server environment is preferred.\n• One (1) year of Healthcare IT experience is preferred.\n• Two (2) years of experience working with Microsoft SQL Server Integration Services (SSIS) is preferred.\n\nLicenses, Registrations, or Certifications\n• Certifications in Hadoop or Java are a plus.\n\nWork Schedule:\n\n8AM - 5PM Monday-Friday\n\nWork Type:\n\nFull Time",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Carrollton, TX",
    "job_city": "Carrollton",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.9756415,
    "job_longitude": -96.8899636,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dh4Px_slRHIXshjaHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Individual must have advanced knowledge of designing and developing data pipelines and delivering advanced analytics, with open-source Big Data processing frameworks such as Hadoop technologies",
        "Individual must have proven competency in programming utilizing distributed computing principles",
        "Uses strong technical knowledge of Enterprise Application/Integration Design and develop systems, databases, operating systems, and Information Services",
        "Uses experience in large scale data lake data and data warehouse implementations and demonstrate proficiency in open-source technology, for example, Python, Spark, Hive, HDFS, NiFi etc",
        "Demonstrates substantial experience and a deep knowledge of data mining techniques, relational, and non-relational databases",
        "Demonstrates experience with data integration with ETL techniques and frameworks",
        "Works with Big Data querying tools, such as Hive, Impala and Spark SQL",
        "Builds stream-processing systems, using solutions such as NiFi or Spark-Streaming",
        "Demonstrates solid understanding of BI and analytics landscape, preferable in large-scale development environments",
        "Bachelor’s degree in computer science, Engineering, Math, or related field is required",
        "Minimum of three (3) years of experience in MapReduce, Spark programming",
        "Minimum of three (3) years of experience developing analytics solutions with large data sets within an OLAP and MPP architecture",
        "Minimum of five (5) years of experience with design, architecture and development of Enterprise scale platforms built on open-source frameworks",
        "Licenses, Registrations, or Certifications"
      ],
      "Responsibilities": [
        "A Data and Analytics Engineer II is responsible for development, expansion, and maintenance of data pipelines of the echo system",
        "A Data and Analytics Engineer uses programming skills to develop, customize and manage integration tools, databases, warehouses, and analytical systems",
        "The Data and Analytics Engineer II is responsible for implementation of optimal solutions to integrate, store, process and analyze huge data sets",
        "This includes an understanding of methodology, specifications, programming, delivery, monitoring, and support standards",
        "Meets expectations of the applicable OneCHRISTUS Competencies: Leader of Self, Leader of Others, or Leader of Leaders",
        "Responsible for analyzing and understanding data sources, participating in requirement gathering, and providing insights and guidance on data technology and data modeling best practices",
        "Analyzes ideas and business and functional requirements to formulate a design strategy",
        "Acts as a tenant to draw out a workable application design and coding parameters with essential functionalities",
        "Works in collaboration with the team members to identify and address the issues by implementing a viable technical solution that is time and cost-effective and ensuring that it does not affect performance quality",
        "Develops code following the industry's best practices and adhere to the organizational development rules and standards",
        "Involved in the evaluation of proposed system acquisitions or solutions development and provides input to the decision-making process relative to compatibility, cost, resource requirements, operations, and maintenance",
        "Integrates software components, subsystems, facilities, and services into the existing technical systems environment; assesses the impact on other systems and works with cross-functional teams within information Services to ensure positive project impact",
        "Installs configure and verify the operation of software components",
        "Participates in the development of standards, design, and implementation of proactive processes to collect and report data and statistics on assigned systems",
        "Participates in the research, design, development, and implementation of application, database, and interface using technologies platforms provided",
        "Researching, designing, implementing, and managing programs",
        "Fixes problems arising across the test cycles and continuously improve the quality of deliverables",
        "References and documents each phase of development for further reference and maintenance operation",
        "Uses critical and analytical thinking skills and understanding of programming principles and design",
        "Demonstrates good understanding of Lambda Architecture",
        "Uses intermediate level of SQL programing and query performance tuning techniques for Data Integration and Consumption using design for optimum performance against large data asset within Transactional, MPP and columnar architecture"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "jobs-womenforhire-com-job-usa-carrollton-tx-data-analytics-engineer-ii-im-enterprise-data-237234",
    "_source": "new_jobs"
  },
  {
    "job_id": "tYKJ_R_W-BuLd6exAAAAAA==",
    "job_title": "Digital – Manager, Data and Analytics Engineer",
    "employer_name": "140 Pfizer Inc",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Workday",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://pfizer.wd1.myworkdayjobs.com/en-US/PfizerCareers/job/Manager---HR---Corporate-Functions-Data---Analytics-Engineering_4939096-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Workday",
        "apply_link": "https://pfizer.wd1.myworkdayjobs.com/en-US/PfizerCareers/job/Manager---HR---Corporate-Functions-Data---Analytics-Engineering_4939096-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=1ed9c59354463a7a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/digital-manager-data-and-analytics-engineer-pfizer-JV_IC1154429_KO0,43_KE44,50.htm?jl=1009944999383&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Xtalks",
        "apply_link": "https://xtalks.com/job/digital-manager-data-and-analytics-engineer/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/3XGeL-WXgBE-FTdj_osHbQ6EYiGJAyBKM4Z-3WaPWjRpllde0xE8PA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/621f15d6bda660c2123665511b18f9a7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/digital-%E2%80%93-manager-data-and-analytics-engineer-at-pfizer-4369276254?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "FactoryFix",
        "apply_link": "https://jobs.factoryfix.com/jobs/digital-manager-data-and-analytics-engineer--tampa--fl--4254991986--V2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Use Your Power for Purpose At Pfizer, our purpose—Breakthroughs that change patients’ lives—drives every decision we make. Digital & Technology accelerates this mission by turning data into insights that power smarter science, stronger operations, and an exceptional colleague experience. Within this organization, the Enabling Functions Creation Center (EFCC) supports HR, Finance, Global Business Services, and Legal with the digital capabilities they need to operate effectively and unlock value. As a hands‑on Manager - Data & Analytics Engineer, you will lead and build innovative data solutions that strengthen our enterprise data foundation, empower our enabling function partners, and help unleash the power of our people—ultimately supporting the breakthroughs that matter most to patients. What You Will Achieve As a hands-on engineer, you will build scalable data pipelines to provide accurate and impactful business analytics and insights Design and implementation of data architecture and infrastructure. Lead the development of data management strategies and policies. Manage a team of project data engineers and analysts, providing guidance and mentorship. Ensure data quality and integrity across all data platforms. Collaborate with cross-functional teams to align data initiatives with business goals. Develop and maintain data governance frameworks. Oversee the integration of new data technologies and tools. Ensure compliance with data privacy regulations and standards. Drive the optimization of data processing workflows and pipelines. Lead the development of analytics solutions to support business decision-making. Manage relationships with external data vendors and partners. Oversee the creation and maintenance of data documentation and metadata. Develop and monitor key performance indicators for data initiatives. Ensure the scalability and performance of data systems. Basic Qualifications: Candidates should possess a BA/BS with at least 4 years of experience, an MBA/MS with at least 2 years of experience, a PhD/JD with any years of experience, an associate's degree with at least 8 years of experience, or a high school diploma (or equivalent) with at least 10 years of relevant experience. Data Architecture Design: Designing and structuring modern databases and modern data systems: Expert Data Warehousing: Building and managing data warehouses (Preferably Snowflake): Expert SQL: Advanced querying and database management: Expert Data pipelines / ETL Processes: Designing and managing modern ETL (Extract, Transform, Load) processes and data engineering pipelines: Expert Data Integration: Combining and transforming data from different sources: Expert Cloud Platforms (e.g., AWS, Azure, Google Cloud): Managing data infrastructure on cloud platforms: Advanced Big Data Technologies (e.g., SnowFlake, Data Bricks, Spark): Handling and processing large datasets: Advanced Data Modeling: Creating data models to support analytics: Advanced Visual Analytics and Business Intelligence Tools: Using BI tools to derive insights from data: Advanced Data Governance: Implementing policies and procedures for data management: Advanced Data Visualization Tools (e.g., Tableau, Power BI): Creating visual representations of data and data story telling: Advanced Programming Languages (e.g., Python, R): Writing code for data manipulation and analysis: Expert Data Security: Implementing security measures to protect data: Intermediate Data Quality Management: Ensuring accuracy and consistency of data: Advanced Statistical Analysis: Applying statistical methods to analyze data: Intermediate Leadership: Guiding and motivating a team to achieve goals: Expert Strategic Thinking: Planning and executing long-term data strategies: Expert Communication: Clearly conveying complex data concepts to stakeholders: Advanced Problem Solving: Identifying and resolving data-related issues: Advanced Collaboration: Working effectively with cross-functional teams: Advanced Hands on experience with vibe coding and Generative AI based data pipeline and analytics solutions development to increase efficiency, reduce overall delivery cost and reduce time to market. Emerging skills Machine Learning: Applying machine learning techniques for data analysis: Intermediate Adaptability: Adjusting to new technologies and methodologies: Intermediate Critical Thinking: Analyzing data critically to derive insights: Advanced Time Management: Prioritizing tasks to meet deadlines: Advanced Decision Making: Making informed decisions based on data insights: Advanced Preferred Qualifications: People Analytics experience using SaaS tools such as Visier, One Model, Perceptyx, Workday Prism Analytics, Workday People Analytics, SAP Success Factors Workforce Analytics is a big plus. Familiarity with cloud/SaaS-based Human Capital Management (HCM) systems such as Workday is a big plus. Experience with Global HR data integration and prior experience with Mergers, Acquisitions, and Divestitures is a plus. Familiarity with EU Global Data Privacy Regulations (GDPR) and other related international regulations is nice to have. Prior experience with data architecture designs and data engineering development related to the GDPR and data privacy guiding principles such as data minimization, right to be forgotten, etc is nice to have. · Experience with Global HR data integration and prior experience with Mergers, Acquisitions, and Divestitures is a plus. Experience with Software engineering best practices, including but not limited to version control (Git/GitHub, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops is highly beneficial but not required. Experience with sourcing and modeling data from application APIs and publishing data and analytics services via APIs / Data Services is highly beneficial but not required · Experience deploying through an agile methodology and working in a SCRUM or SAFe team is highly beneficial but not required. 5 or more years of experience with one or more general-purpose data processing programming languages, including but not limited to: SQL, Scala, Python, Java, etc Architected end-to-end data pipelines with a major cloud stack is a plus · Experience in Cloud computing, machine learning, text analysis, NLP, and developing and deploying data and analytics services such as recommendation engines experience is a plus Domain experience in the Human Resources field Work Location Assignment: Hybrid The annual base salary for this position ranges from $99,200.00 to $160,500.00. In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 12.5% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided applies to the Tampa, FL location only. The salary range provided does not apply to any other United States location or locations outside of the United States. Relocation assistance may be available based on business needs and/or eligibility. Sunshine Act Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative. EEO & Employment Eligibility Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer. This position requires permanent work authorization in the United States. Pfizer endeavors to make www.pfizer.com/careers accessible to all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process and/or interviewing, please email disabilityrecruitment@pfizer.com. This is to be used solely for accommodation requests with respect to the accessibility of our website, online application process and/or interviewing. Requests for any other reason will not be returned. Information & Business Tech Pfizer careers are like no other. In our culture of individual ownership, we believe in our ability to improve future healthcare, and potential to transform millions of lives. We’re looking for new talent to join our global community, to unearth new innovative therapies that make the world a healthier place.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Tampa, FL",
    "job_city": "Tampa",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 27.951689599999998,
    "job_longitude": -82.45875269999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DtYKJ_R_W-BuLd6exAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "pfizer-wd1-myworkdayjobs-com-en-us-pfizercareers-job-manager-hr-corporate-functions-data-analytics-engineering_4939096-2",
    "_source": "new_jobs"
  },
  {
    "job_id": "_rvXxbAeuJ7yr3ORAAAAAA==",
    "job_title": "Staff Analytics Engineer",
    "employer_name": "Coinbase",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ5OL4_4viaeK1GNWKlusWTU0WPLNurfsu9Lsbt&s=0",
    "employer_website": "https://www.coinbase.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/staff-analytics-engineer?id=2454709003&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/staff-analytics-engineer?id=2454709003&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Ready to be pushed beyond what you think you’re capable of?\n\nAt Coinbase, our mission is to increase economic freedom in the world. It’s a massive, ambitious opportunity that demands the best of us, every day, as we build the emerging onchain platform — and with it, the future global financial system.\n\nTo achieve our mission, we’re seeking a very specific candidate. We want someone who is passionate about our mission and who believes in the power of crypto and blockchain technology to update the financial system. We want someone who is eager to leave their mark on the world, who relishes the pressure and privilege of working with high caliber colleagues, and who actively seeks feedback to keep leveling up. We want someone who will run towards, not away from, solving the company’s hardest problems.\n\nOur work culture is intense and isn’t for everyone. But if you want to build the future alongside others who excel in their disciplines and expect the same from you, there’s no better place to be.\n\nWhile many roles at Coinbase are remote-first, we are not remote-only. In-person participation is required throughout the year. Team and company-wide offsites are held multiple times annually to foster collaboration, connection, and alignment. Attendance is expected and fully supported.\n\nThe CX Analytics Engineering team bridges the gap between data engineering, data science, and business analytics by building scalable, impactful data solutions. We transform raw data into actionable insights through robust pipelines, well-designed data models, and tools that empower stakeholders across the organization to make data-driven decisions. As an Analytics Engineer on our team, you will function as a force multiplier, enabling Analytics and Operations to function seamlessly at scale. You’ll have the opportunity to translate complex technical and operational requirements into easily consumable front end data solutions, while also heavily influencing the overarching strategy for CX Analytics and its partners.\n\nOur team combines technical expertise with a deep understanding of the business to unlock the full potential of our data. We prioritize data quality, reliability, and usability, ensuring stakeholders can rely on our data to drive meaningful outcomes.\n\nWhat We Do\n\nTrusted Data Sources : Develop and maintain foundational data models that serve as the single source of truth for analytics across the organization.\n\nActionable Insights : Empower stakeholders by translating business requirements into scalable data models, dashboards, and tools.\n\nCross-Functional Collaboration : Partner with engineering, data science, product, and business teams to ensure alignment on priorities and data solutions.\n\nScalable Data Products : Build frameworks, tools, and workflows that maximize efficiency for data users, while maintaining high standards of data quality and performance.\n\nOutcome-Focused Solutions : Use modern development and analytics tools to deliver value quickly, while ensuring long-term maintainability.\n\nWhat you’ll be doing (ie. job duties):**.\n\nAnalytics engineer is a hybrid Data Engineer/Data Scientist/Business Analyst role that has the expertise to understand data flows end to end, and the engineering toolkit to extract the most value out of it indirectly (building tables) or directly (solving problems, delivering insights).\n\nExpectations\n• Be the expert: *Quickly build subject matter expertise in a specific business area and data domain. Understand the data flows from creation, ingestion, transformation, and delivery.\n\nExamples:\n\nStep into a new line of business and work with Engineering and Product partners to deliver first data pipelines and insights.Communicate with engineering teams to fix data gaps for downstream data users.Take initiative and accountability for fixing issues anywhere in the stack.\n\nGenerate business value: Interface with stakeholders on data and product teams to deliver the most commercial value from data (directly or indirectly).\n\nExamples:\n\nBuild out a new data model allowing multiple downstream DS teams to more easily unlock business value through experimentation and ad hoc analysis.Combine Eng details of the algo engine with stats and data expertise to come up with feasible solutions for Eng to make the algo better.Work with PMs to tie together new x-PG, and x-Product data into one holistic framework to optimize key financing product business metrics.\n\nFocus on outcomes not tools: Use a variety of frameworks and paradigms to identify the best-fit tools to deliver value.\n\nExamples:\n\nDevelop new abstractions (e.g. UDFs, Python packages, dashboards) to support scalable data workflows/infra.Stand up a framework for building data apps internally, enabling other DS teams to quickly add value.Use established tools with mastery (e.g. Google Sheets, SQL) to quickly deliver impact when speed is top priority.**\n\nWhat We Look For in You\n\nIn addition to out of the box thinking, attention to detail, a sense of urgency and a high degree of autonomy and accountability, we expect you to have the following skills:\n• Customer Support Data Experience: *Familiarity with data elements and processes supporting successful Customer Support initiatives, including employee performance monitoring, workforce/staffing inputs, and the handling of sensitive PII across a broad stakeholder base.\n\nData Modeling Expertise : Strong understanding of best practices for designing modular and reusable data models (e.g., star schemas, snowflake schemas).\n• Prompt Design and Engineering: *Expertise in prompt engineering and design for LLMs (e.g., GPT), including creating, refining, and optimizing prompts to improve response accuracy, relevance, and performance for internal tools and use cases.\n\nAdvanced SQL : Proficiency in advanced SQL techniques for data transformation, querying, and optimization.\n\nIntermediate to Advanced Python : Expertise in scripting and automation, with experience in Object-Oriented Programming (OOP) and building scalable frameworks.\n\nCollaboration and Communication : Strong ability to translate technical concepts into business value for cross-functional stakeholders. Proven ability to manage projects and communicate effectively across teams.\n\nData Pipeline Development : Experience building, maintaining, and optimizing ETL/ELT pipelines, using modern tools like dbt, Airflow, or similar.\n\nData Visualization : Proficiency in building polished dashboards using tools like Looker, Tableau, Superset, or Python visualization libraries (Matplotlib, Plotly).\n\nDevelopment Tools : Familiarity with version control (GitHub), CI/CD, and modern development workflows.\n\nData Architecture : Knowledge of modern data lake/warehouse architectures (e.g., Snowflake, Databricks) and transformation frameworks.\n\nBusiness Acumen : Ability to understand and address business challenges through analytics engineering.\n• Data savvy: *Familiarity with statistics and probability.\n\nBonus Skills :\n\nExperience with cloud platforms (e.g., AWS, GCP).\n\nFamiliarity with Docker or Kubernetes.\n\nJob #: P71393\n\nPay Transparency Notice : Depending on your work location, the target annual *base *salary for this position can range as detailed below. Full time offers from Coinbase also include bonus eligibility + equity eligibility + benefits (including medical, dental, vision and 401(k)).\n\nBase salary range shown. Total compensation also includes equity and bonus eligibility and benefits:\n\n$207,485—$244,100 USD\n\nPlease be advised that each candidate may submit a maximum of four applications within any 30-day period. We encourage you to carefully evaluate how your skills and interests align with Coinbase's roles before applying.\n\nCommitment to Equal Opportunity\n\nCoinbase is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. For US applicants, you may view the Employee Rights and the Know Your Rights notices by clicking on their corresponding links. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law.\n\nCoinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations(at)coinbase.com to let us know the nature of your request and your contact information. For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here).\n\nGlobal Data Privacy Notice for Job Candidates and Applicants\n\nDepending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. By submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.\n\nAI Disclosure\n\nFor select roles, Coinbase is piloting an AI tool based on machine learning technologies to conduct initial screening interviews to qualified applicants. The tool simulates realistic interview scenarios and engages in dynamic conversation. A human recruiter will review your interview responses, provided in the form of a voice recording and/or transcript, to assess them against the qualifications and characteristics outlined in the job description.\n\nFor select roles, Coinbase is also piloting an AI interview intelligence platform to transcribe and summarize interview notes, allowing our interviewers to fully focus on you as the candidate.\n\nThe above pilots are for testing purposes and Coinbase will not use AI to make decisions impacting employment . To request a reasonable accommodation due to disability, please contact accommodations(at)coinbase.com",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Willow Creek, MT",
    "job_city": "Willow Creek",
    "job_state": "Montana",
    "job_country": "US",
    "job_latitude": 45.825206,
    "job_longitude": -111.64469659999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_rvXxbAeuJ7yr3ORAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We want someone who is eager to leave their mark on the world, who relishes the pressure and privilege of working with high caliber colleagues, and who actively seeks feedback to keep leveling up",
        "We want someone who will run towards, not away from, solving the company’s hardest problems",
        "Use established tools with mastery (e.g",
        "Google Sheets, SQL) to quickly deliver impact when speed is top priority.**",
        "In addition to out of the box thinking, attention to detail, a sense of urgency and a high degree of autonomy and accountability, we expect you to have the following skills:",
        "Data Modeling Expertise : Strong understanding of best practices for designing modular and reusable data models (e.g., star schemas, snowflake schemas)",
        "Prompt Design and Engineering: *Expertise in prompt engineering and design for LLMs (e.g., GPT), including creating, refining, and optimizing prompts to improve response accuracy, relevance, and performance for internal tools and use cases",
        "Advanced SQL : Proficiency in advanced SQL techniques for data transformation, querying, and optimization",
        "Intermediate to Advanced Python : Expertise in scripting and automation, with experience in Object-Oriented Programming (OOP) and building scalable frameworks",
        "Collaboration and Communication : Strong ability to translate technical concepts into business value for cross-functional stakeholders",
        "Proven ability to manage projects and communicate effectively across teams",
        "Data Pipeline Development : Experience building, maintaining, and optimizing ETL/ELT pipelines, using modern tools like dbt, Airflow, or similar",
        "Data Visualization : Proficiency in building polished dashboards using tools like Looker, Tableau, Superset, or Python visualization libraries (Matplotlib, Plotly)",
        "Development Tools : Familiarity with version control (GitHub), CI/CD, and modern development workflows",
        "Data Architecture : Knowledge of modern data lake/warehouse architectures (e.g., Snowflake, Databricks) and transformation frameworks",
        "Business Acumen : Ability to understand and address business challenges through analytics engineering",
        "Data savvy: *Familiarity with statistics and probability",
        "Experience with cloud platforms (e.g., AWS, GCP)",
        "Familiarity with Docker or Kubernetes"
      ],
      "Benefits": [
        "Pay Transparency Notice : Depending on your work location, the target annual *base *salary for this position can range as detailed below",
        "Full time offers from Coinbase also include bonus eligibility + equity eligibility + benefits (including medical, dental, vision and 401(k))",
        "Base salary range shown",
        "Total compensation also includes equity and bonus eligibility and benefits:",
        "$207,485—$244,100 USD"
      ],
      "Responsibilities": [
        "In-person participation is required throughout the year",
        "Team and company-wide offsites are held multiple times annually to foster collaboration, connection, and alignment",
        "Attendance is expected and fully supported",
        "We transform raw data into actionable insights through robust pipelines, well-designed data models, and tools that empower stakeholders across the organization to make data-driven decisions",
        "As an Analytics Engineer on our team, you will function as a force multiplier, enabling Analytics and Operations to function seamlessly at scale",
        "You’ll have the opportunity to translate complex technical and operational requirements into easily consumable front end data solutions, while also heavily influencing the overarching strategy for CX Analytics and its partners",
        "Trusted Data Sources : Develop and maintain foundational data models that serve as the single source of truth for analytics across the organization",
        "Actionable Insights : Empower stakeholders by translating business requirements into scalable data models, dashboards, and tools",
        "Cross-Functional Collaboration : Partner with engineering, data science, product, and business teams to ensure alignment on priorities and data solutions",
        "Scalable Data Products : Build frameworks, tools, and workflows that maximize efficiency for data users, while maintaining high standards of data quality and performance",
        "Outcome-Focused Solutions : Use modern development and analytics tools to deliver value quickly, while ensuring long-term maintainability",
        "Analytics engineer is a hybrid Data Engineer/Data Scientist/Business Analyst role that has the expertise to understand data flows end to end, and the engineering toolkit to extract the most value out of it indirectly (building tables) or directly (solving problems, delivering insights)",
        "Be the expert: *Quickly build subject matter expertise in a specific business area and data domain",
        "Understand the data flows from creation, ingestion, transformation, and delivery",
        "Step into a new line of business and work with Engineering and Product partners to deliver first data pipelines and insights",
        "Communicate with engineering teams to fix data gaps for downstream data users",
        "Take initiative and accountability for fixing issues anywhere in the stack",
        "Generate business value: Interface with stakeholders on data and product teams to deliver the most commercial value from data (directly or indirectly)",
        "Build out a new data model allowing multiple downstream DS teams to more easily unlock business value through experimentation and ad hoc analysis",
        "Combine Eng details of the algo engine with stats and data expertise to come up with feasible solutions for Eng to make the algo better",
        "Work with PMs to tie together new x-PG, and x-Product data into one holistic framework to optimize key financing product business metrics",
        "Focus on outcomes not tools: Use a variety of frameworks and paradigms to identify the best-fit tools to deliver value",
        "Develop new abstractions (e.g",
        "UDFs, Python packages, dashboards) to support scalable data workflows/infra.Stand up a framework for building data apps internally, enabling other DS teams to quickly add value",
        "Customer Support Data Experience: *Familiarity with data elements and processes supporting successful Customer Support initiatives, including employee performance monitoring, workforce/staffing inputs, and the handling of sensitive PII across a broad stakeholder base"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-staff-analytics-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "f5J9sCWlOs9AFPYrAAAAAA==",
    "job_title": "Analytics Engineer",
    "employer_name": "Dune",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrDwcrDrXbg2KLtVmgZAK6PNe0xHfC5XD5fd-S&s=0",
    "employer_website": "https://dune.com",
    "job_publisher": "Jobgether",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobgether.com/offer/6984b8439242569942757dab-analytics-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobgether",
        "apply_link": "https://jobgether.com/offer/6984b8439242569942757dab-analytics-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee IE",
        "apply_link": "https://ie.bebee.com/job/cbb069b49e7d888e5cc6a03024377253?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This a Full Remote job, the offer is available from: Europe, United States\n\nAbout Dune\n\nDune is on a mission to make crypto data accessible. We’re a collaborative multi-chain analytics platform used by thousands of developers, analysts, & investors to understand the on-chain world and the frontiers of finance. We’re a team of ~60 employees, working together across Europe and eastern US timezones 🌍️. We believe in our mission, and in building a powerful, open product that allows individuals and communities to do deep research into important ecosystems like Bitcoin, Ethereum, Solana, and many more.\n\nWe’re backed by some of the world's best investors. In February 2022, we announced our Series B funding round led by Coatue and Union Square Ventures, an important milestone that allowed us to double down on our mission. We’re using the funds to educate, reward and empower a new generation of onchain analysts aka Wizards 🧙‍♀️\n\nIf you want to have one of the highest impact jobs on the planet, come join our wonderful team of Galaxy brains.\n\nLearn more about us:\n\nDune's Vision\n\nValues and working at Dune\n\nAbout the role\n\nDune’s data team turns raw on-chain activity into trustworthy, reusable datasets that power dashboards, products, and research. We’re hiring a Analytics Engineer to build and operate reliable pipelines, model complex protocol mechanics into durable schemas, and scale curated datasets across chains. You’ll collaborate with product and our customers, bringing strong data engineering fundamentals and curiosity, with blockchain data intuition as a plus.\n\nIn this role you will\n• Design, implement, and maintain data pipelines that standardize how on-chain data is consumed by Dune customers\n• Establish the right architecture to ensure data quality and timely delivery, including validation gates, SLAs, observability, and lineage-aware deployments\n• Triage, analyze, and fix pipeline issues independently, driving root-cause analysis and long-term reliability improvements\n• Own end-to-end transformation pipelines with robust testing, CI, documentation, and cost awareness from raw to curated layers\n• Build and orchestrate reliable pipelines using modern schedulers and control planes to handle high-volume community contributions\n• Partner with product, go-to-market, and community contributors to unlock clear user value and faster time to insight\n• Replicate curated datasets efficiently into Kafka, Snowflake, BigQuery, and Databricks with appropriate partitioning, compression, and change data capture strategies\n\nYou might be a great fit if you have\n• Strong SQL skills and experience modeling large datasets in modern warehouses, with expertise in at least one of Trino, Snowflake, or ClickHouse\n• Proficiency in Python for pipeline development, tooling, and automation\n• Proven track record operating robust pipelines and orchestration with tools like Prefect, Airflow, Elementary, or similar\n• Solid computer science fundamentals, system design skills, and experience in public cloud and container orchestration (e.g., Kubernetes)\n• Ability to analyze, debug, and resolve data pipeline and modeling issues independently in a remote, async environment\n• Strong blockchain data intuition, including reading transactions, events, and traces\n• Interest in query engine internals and performance optimization\n• Experience with data lakes, large-scale data processing, and systems performance\n• Familiarity with CI for data, observability, cost optimization, and leveraging AI tools to accelerate development and operations\n\nPerks & Benefits\n• A competitive salary and equity package 🚀. Both salary and equity is top 25% of companies in the space\n• Our employee equity scheme has world-class employee-friendly terms with a heavily discounted strike price (~90%) and a 10-year exercise window\n• 5 weeks PTO + local public holidays (that can be swapped to suit you) 🏖\n• A fully remote-first approach 🧑‍💻 within a distributed team with flexible working hours; you structure your own day\n• Say goodbye to meeting overload! We believe in a healthy mix of async and sync work, so you can focus on what truly matters—no more wasted time on endless meetings!\n• Good health is important, so we offer private medical insurance, dental & vision as standard 🩺\n• We believe in paid parental leave 👶 to help you celebrate this important milestone, transition to your new life, and bond with your new baby. We offer 16 weeks to primary and 6 weeks to secondary caregivers, fully paid. Plus a 2-week part-time phased return at full pay to help you get used to your new (and slightly more complex!) schedule\n• Quarterly offsites in various exciting locations as a company or team to connect, work together and have fun (so far in Tuscany 🇮🇹 Berlin 🇩🇪 Austria 🇦🇹 and Athens 🇬🇷).\n• On top of this 👆each person gets a yearly travel allowance to connect and co-work with someone or a team of people for a few days.\n• An allowance for your at-home setup, to ensure you are happy, comfortable and productive. If you prefer a local co-working space, we’ll pay for your desk.\n• Work with some of the best people you’ll ever get to meet!\n• And of course, you get some awesome Dune swag! ✌️😎\n\n------------------------------------------------------------------------------------------\n\nWe are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.\n\n#LI-Remote\nThis offer from \"Dune\" has been enriched by Jobgether.com and got a 83% flex score.",
    "job_is_remote": true,
    "job_posted_at": "10 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Washington, DC",
    "job_city": "Washington",
    "job_state": "District of Columbia",
    "job_country": "US",
    "job_latitude": 38.9072873,
    "job_longitude": -77.0369274,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Df5J9sCWlOs9AFPYrAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobgether-com-offer-6984b8439242569942757dab-analytics-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "LnGFK8e-I20MJtnHAAAAAA==",
    "job_title": "SCM Analytics Engineer - Full-time",
    "employer_name": "Southern Glazer's Wine and Spirits",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSo589aPoxsES92lx6gGXgSyhXQjevcTInpnhA0&s=0",
    "employer_website": "https://www.southernglazers.com",
    "job_publisher": "Snagajob",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.snagajob.com/jobs/1160086327?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Snagajob",
        "apply_link": "https://www.snagajob.com/jobs/1160086327?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "**What You Need To Know**\n\nShape a remarkable future with us. Build a career working for an industry leader that truly invests in their people – and equips them with leading technology, continuous learning, and the ability to bring their best selves to work. As the premier wine and spirits distributor, Southern Glazer’s isn't just one of Forbes’ Top Private Companies; it's a family-owned business with deep roots dating back to 1933.\n\nThe reputation of Southern Glazer’s is well-established, and it's no surprise that we are regularly recognized for our culture. Southern Glazer’s has been recognized by Newsweek as one of America’s Greatest Workplaces for Inclusion and Diversity, as well as for Women and Parents and Families. These accolades speak volumes about our commitment to creating a supportive and inclusive culture of belonging for all employees.\n\nAs a full-time employee, you can choose from a wide-ranging menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition assistance, a wellness program, parental leave, vacation accrual, paid sick leave, and more.\n\nBy joining Southern Glazer’s, you would be part of a team that values excellence, innovation, and community. This is more than just a job – it's an opportunity to build the future of beverage distribution and grow with a company that truly cares about its people.\n• *Must be eligible to work as a full-time employee in the US without sponsorship.**\n• *Overview**\n\nThe SCM Analytics Developer will partner with functional members of Supply Chain to understand the current and future business requirements. They will be responsible for gathering data across various sources, analyzing it, and providing insights used for informed decision-making. They will ensure the appropriate use of analytical tools so that analysis and reporting are accurate, timely, thorough, and professional. They will also optimize existing data processes and resources, setting clear goals and benchmarks for improvement, development, and effective management of sound business practices.\n• *Primary Responsibilities**\n\n+ Develop reporting on SAP Business Objects, Tableau Dashboards, and create Excel refreshable files through Open Database Connectivity (ODBC) connections and Power Queries\n\n+ Automate processes using Amazon Web Services (AWS), including Step Functions, EventBridge, Simple Storage Service (S3) Buckets, and AWS Glue. Additionally, leveraged SharePoint Site services—such as lists and workflows—integrated through Power Automate\n\n+ Maintain data quality through identifying issues, removing duplicates, data cleansing, and enriching data quality\n\n+ Develop, design, and manage efficient processes, tabular reports, list reports, score cards, dynamic models, and dashboards that effectively monitor various key performance indicators (KPI)s and assist the supply chain business teams\n\n+ Apply the appropriate use of analytical tools to optimize reporting for current business requirements and create new processes and solutions for future business requirements\n\n+ Provide mentoring and support to other analysts to assist in their development surrounding data, processes, reporting, and utilizing self-service tools\n\n+ Assist with timely, accurate, pertinent information and analysis, as well as aid in monitoring, evaluating, and improving performance and related metrics of the Supply Chain\n• *Additional Primary Responsibilities**\n\n+ Support the long-term vision of the supply chain through the effective implementation of data reporting\n\n+ Ability to understand and translate data into business insights\n\n+ Summarize analysis and findings for users\n\n+ Ability to join data from multiple systems and sources through the use of Excel, SQL, Python, AWS services, and other components\n\n+ Ability to mine large sets of data analytics and insights\n\n+ Perform other job-related duties as assigned\n• *Minimum Qualifications**\n\n+ Bachelor’s Degree and three years of experience or equivalent education and related experience\n\n+ Experience in data preparation, blending, discovery, and visualization using modern BI & Analytics tools such as AWS services like Step Functions, Glue, Sage Maker, and other components, SAP Business Objects, Tableau, Alteryx, and Excel\n\n+ Expert knowledge in Excel, including pivot tables, Power Queries, vlookup, and other advanced formulas\n\n+ Intermediate knowledge of SQL (Joins, Stored Procedures, Views, Jobs, and Troubleshooting)\n\n+ Experience developing, improving, and streamlining processes and reporting\n\n+ Thorough analytical, investigative, and problem-solving skills\n\n+ Timeliness, accuracy, and professionalism\n• *Physical Demands**\n\n+ Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or a mobile device\n\n+ Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping\n\n+ May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs\n• *EEO Statement**\n\nSouthern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. SGWS complies with all federal, state and local laws concerning consideration of a qualified applicant's arrest and/or criminal conviction records. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.\n\n\\#LI-JL1\n\n_If you have any questions or concerns about whether this posting complies/adheres with local pay transparency requirements, please contact the SGWS talent acquisition team at NationalTA@sgws.com_\n\nSouthern Glazer's Wine and Spirits provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\n• *What You Need To Know**\n\nShape a remarkable future with us. Build a career working for an industry leader that truly invests in their people – and equips them with leading technology, continuous learning, and the ability to bring their best selves to work. As the premier wine and spirits distributor, Southern Glazer’s isn't just one of Forbes’ Top Private Companies; it's a family-owned business with deep roots dating back to 1933.\n\nThe reputation of Southern Glazer’s is well-established, and it's no surprise that we are regularly recognized for our culture. Southern Glazer’s has been recognized by Newsweek as one of America’s Greatest Workplaces for Inclusion and Diversity, as well as for Women and Parents and Families. These accolades speak volumes about our commitment to creating a supportive and inclusive culture of belonging for all employees.\n\nAs a full-time employee, you can choose from a wide-ranging menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition assistance, a wellness program, parental leave, vacation accrual, paid sick leave, and more.\n\nBy joining Southern Glazer’s, you would be part of a team that values excellence, innovation, and community. This is more than just a job – it's an opportunity to build the future of beverage distribution and grow with a company that truly cares about its people.\n• *Must be eligible to work as a full-time employee in the US without sponsorship.**\n• *Overview**\n\nThe SCM Analytics Developer will partner with functional members of Supply Chain to understand the current and future business requirements. They will be responsible for gathering data across various sources, analyzing it, and providing insights used for informed decision-making. They will ensure the appropriate use of analytical tools so that analysis and reporting are accurate, timely, thorough, and professional. They will also optimize existing data processes and resources, setting clear goals and benchmarks for improvement, development, and effective management of sound business practices.\n• *Primary Responsibilities**\n\n+ Develop reporting on SAP Business Objects, Tableau Dashboards, and create Excel refreshable files through Open Database Connectivity (ODBC) connections and Power Queries\n\n+ Automate processes using Amazon Web Services (AWS), including Step Functions, EventBridge, Simple Storage Service (S3) Buckets, and AWS Glue. Additionally, leveraged SharePoint Site services—such as lists and workflows—integrated through Power Automate\n\n+ Maintain data quality through identifying issues, removing duplicates, data cleansing, and enriching data quality\n\n+ Develop, design, and manage efficient processes, tabular reports, list reports, score cards, dynamic models, and dashboards that effectively monitor various key performance indicators (KPI)s and assist the supply chain business teams\n\n+ Apply the appropriate use of analytical tools to optimize reporting for current business requirements and create new processes and solutions for future business requirements\n\n+ Provide mentoring and support to other analysts to assist in their development surrounding data, processes, reporting, and utilizing self-service tools\n\n+ Assist with timely, accurate, pertinent information and analysis, as well as aid in monitoring, evaluating, and improving performance and related metrics of the Supply Chain\n• *Additional Primary Responsibilities**\n\n+ Support the long-term vision of the supply chain through the effective implementation of data reporting\n\n+ Ability to understand and translate data into business insights\n\n+ Summarize analysis and findings for users\n\n+ Ability to join data from multiple systems and sources through the use of Excel, SQL, Python, AWS services, and other components\n\n+ Ability to mine large sets of data analytics and insights\n\n+ Perform other job-related duties as assigned\n• *Minimum Qualifications**\n\n+ Bachelor’s Degree and three years of experience or equivalent education and related experience\n\n+ Experience in data preparation, blending, discovery, and visualization using modern BI & Analytics tools such as AWS services like Step Functions, Glue, Sage Maker, and other components, SAP Business Objects, Tableau, Alteryx, and Excel\n\n+ Expert knowledge in Excel, including pivot tables, Power Queries, vlookup, and other advanced formulas\n\n+ Intermediate knowledge of SQL (Joins, Stored Procedures, Views, Jobs, and Troubleshooting)\n\n+ Experience developing, improving, and streamlining processes and reporting\n\n+ Thorough analytical, investigative, and problem-solving skills\n\n+ Timeliness, accuracy, and professionalism\n• *Physical Demands**\n\n+ Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or a mobile device\n\n+ Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping\n\n+ May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs\n• *EEO Statement**\n\nSouthern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. SGWS complies with all federal, state and local laws concerning consideration of a qualified applicant's arrest and/or criminal conviction records. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.\n\n\\#LI-JL1\n\n_If you have any questions or concerns about whether this posting complies/adheres with local pay transparency requirements, please contact the SGWS talent acquisition team at NationalTA@sgws.com_\n\nSouthern Glazer's Wine and Spirits provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Miramar, FL",
    "job_city": "Miramar",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 25.9860762,
    "job_longitude": -80.30356019999999,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DLnGFK8e-I20MJtnHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Ability to understand and translate data into business insights",
        "Ability to join data from multiple systems and sources through the use of Excel, SQL, Python, AWS services, and other components",
        "Ability to mine large sets of data analytics and insights",
        "Bachelor’s Degree and three years of experience or equivalent education and related experience",
        "Experience in data preparation, blending, discovery, and visualization using modern BI & Analytics tools such as AWS services like Step Functions, Glue, Sage Maker, and other components, SAP Business Objects, Tableau, Alteryx, and Excel",
        "Expert knowledge in Excel, including pivot tables, Power Queries, vlookup, and other advanced formulas",
        "Intermediate knowledge of SQL (Joins, Stored Procedures, Views, Jobs, and Troubleshooting)",
        "Experience developing, improving, and streamlining processes and reporting",
        "Thorough analytical, investigative, and problem-solving skills",
        "Timeliness, accuracy, and professionalism",
        "Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or a mobile device",
        "Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping",
        "May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs",
        "Ability to understand and translate data into business insights",
        "Summarize analysis and findings for users",
        "Ability to join data from multiple systems and sources through the use of Excel, SQL, Python, AWS services, and other components",
        "Ability to mine large sets of data analytics and insights",
        "Bachelor’s Degree and three years of experience or equivalent education and related experience",
        "Experience in data preparation, blending, discovery, and visualization using modern BI & Analytics tools such as AWS services like Step Functions, Glue, Sage Maker, and other components, SAP Business Objects, Tableau, Alteryx, and Excel",
        "Expert knowledge in Excel, including pivot tables, Power Queries, vlookup, and other advanced formulas",
        "Intermediate knowledge of SQL (Joins, Stored Procedures, Views, Jobs, and Troubleshooting)",
        "Experience developing, improving, and streamlining processes and reporting",
        "Thorough analytical, investigative, and problem-solving skills",
        "Timeliness, accuracy, and professionalism",
        "Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or a mobile device",
        "Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping",
        "May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs"
      ],
      "Benefits": [
        "As a full-time employee, you can choose from a wide-ranging menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan",
        "We also offer tuition assistance, a wellness program, parental leave, vacation accrual, paid sick leave, and more",
        "As a full-time employee, you can choose from a wide-ranging menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan",
        "We also offer tuition assistance, a wellness program, parental leave, vacation accrual, paid sick leave, and more",
        "*Must be eligible to work as a full-time employee in the US without sponsorship.**"
      ],
      "Responsibilities": [
        "The SCM Analytics Developer will partner with functional members of Supply Chain to understand the current and future business requirements",
        "They will be responsible for gathering data across various sources, analyzing it, and providing insights used for informed decision-making",
        "They will ensure the appropriate use of analytical tools so that analysis and reporting are accurate, timely, thorough, and professional",
        "They will also optimize existing data processes and resources, setting clear goals and benchmarks for improvement, development, and effective management of sound business practices",
        "Develop reporting on SAP Business Objects, Tableau Dashboards, and create Excel refreshable files through Open Database Connectivity (ODBC) connections and Power Queries",
        "Automate processes using Amazon Web Services (AWS), including Step Functions, EventBridge, Simple Storage Service (S3) Buckets, and AWS Glue",
        "Additionally, leveraged SharePoint Site services—such as lists and workflows—integrated through Power Automate",
        "Maintain data quality through identifying issues, removing duplicates, data cleansing, and enriching data quality",
        "Develop, design, and manage efficient processes, tabular reports, list reports, score cards, dynamic models, and dashboards that effectively monitor various key performance indicators (KPI)s and assist the supply chain business teams",
        "Apply the appropriate use of analytical tools to optimize reporting for current business requirements and create new processes and solutions for future business requirements",
        "Provide mentoring and support to other analysts to assist in their development surrounding data, processes, reporting, and utilizing self-service tools",
        "Assist with timely, accurate, pertinent information and analysis, as well as aid in monitoring, evaluating, and improving performance and related metrics of the Supply Chain",
        "*Additional Primary Responsibilities**",
        "Support the long-term vision of the supply chain through the effective implementation of data reporting",
        "Summarize analysis and findings for users",
        "Perform other job-related duties as assigned",
        "The SCM Analytics Developer will partner with functional members of Supply Chain to understand the current and future business requirements",
        "They will be responsible for gathering data across various sources, analyzing it, and providing insights used for informed decision-making",
        "They will ensure the appropriate use of analytical tools so that analysis and reporting are accurate, timely, thorough, and professional",
        "They will also optimize existing data processes and resources, setting clear goals and benchmarks for improvement, development, and effective management of sound business practices",
        "Develop reporting on SAP Business Objects, Tableau Dashboards, and create Excel refreshable files through Open Database Connectivity (ODBC) connections and Power Queries",
        "Automate processes using Amazon Web Services (AWS), including Step Functions, EventBridge, Simple Storage Service (S3) Buckets, and AWS Glue",
        "Additionally, leveraged SharePoint Site services—such as lists and workflows—integrated through Power Automate",
        "Maintain data quality through identifying issues, removing duplicates, data cleansing, and enriching data quality",
        "Develop, design, and manage efficient processes, tabular reports, list reports, score cards, dynamic models, and dashboards that effectively monitor various key performance indicators (KPI)s and assist the supply chain business teams",
        "Apply the appropriate use of analytical tools to optimize reporting for current business requirements and create new processes and solutions for future business requirements",
        "Provide mentoring and support to other analysts to assist in their development surrounding data, processes, reporting, and utilizing self-service tools",
        "Assist with timely, accurate, pertinent information and analysis, as well as aid in monitoring, evaluating, and improving performance and related metrics of the Supply Chain",
        "*Additional Primary Responsibilities**",
        "Support the long-term vision of the supply chain through the effective implementation of data reporting",
        "Perform other job-related duties as assigned"
      ]
    },
    "job_onet_soc": "13108100",
    "job_onet_job_zone": "4",
    "id": "www-snagajob-com-jobs-1160086327",
    "_source": "new_jobs"
  },
  {
    "job_id": "DnJQNbAuRHlzHfYrAAAAAA==",
    "job_title": "Software Engineer, Data Analytics",
    "employer_name": "Intelliforce-IT Solutions Group, LLC.",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSztnTy3445oeJJPPShY_ifG8fv49mr8rUJZf4w&s=0",
    "employer_website": "https://intelliforce-itsg.com",
    "job_publisher": "BeBee",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://us.bebee.com/job/24659c2a46e74f6da35287fa63f6dfaf?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/24659c2a46e74f6da35287fa63f6dfaf?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Make an Impact Where It Matters Most\n\nAt Intelliforce, we build mission-focused solutions where engineering excellence directly supports national security outcomes.\n\nIn this role, you will help modernize and extend a critical data analytics user interface, transforming a prototype into a production-ready capability.\n\nYou will work alongside mission partners to enhance how data is explored, visualized, and delivered, ensuring analysts have intuitive, reliable tools that scale beyond the enterprise.\nThis position sits squarely within Intelliforce's core strengths in software engineering, data analytics, and mission delivery.\n\nSchedule and Work Details\n\nLocation:\nOPS 1\n\nSchedule:\nOnsite\n\nTelework:\nNot Available\n\nClearance:\nActive Top Secret Clearance with Full Scope Polygraph required\n\nHere's What Your Day-to-Day Might Include\n\nYou will enhance and mature a Streamlit-based UI prototype into a robust, scalable interface with a forward path to external availability.\nYou will design and implement new features, refine existing workflows, and ensure performance, usability, and data integrity.\n\nYour work will include developing front-end components, integrating back-end services, handling diverse data formats, and collaborating with engineers and stakeholders to translate requirements into clean, functional solutions.\nYou will also contribute to testing, documentation, and continuous improvement of the platform.\n\nMinimum Qualifications\n\nClearance:\nActive Top Secret Clearance with Full Scope Polygraph required\n\nCitizenship:\nMust be a U.S. Citizen\n\nEducation And Experience\n\nBachelor's degree in Computer Science or a related technical discipline with seven years of software engineering experience supporting programs of similar scope, type, and complexity, or a Master's degree with five years of experience, or nine years of relevant experience in lieu of a degree.\nRequired Skills\n\nYou bring strong experience developing software in Linux environments, including use of the Linux command line and Bash scripting to automate manual processes.\nYou have recent hands-on experience with Python and Java, and familiarity building interactive applications using Streamlit.\n\nYou are comfortable developing front-end applications using TypeScript, HTML, and CSS, and have experience with modern JavaScript frameworks such as React, Angular, or Vue.\n\nYou have worked with distributed data processing engines like Apache Spark, use Jupyter Notebooks effectively, and perform data wrangling with tools such as pandas and NumPy.\n\nYou are experienced working with structured, semi-structured, and unstructured data formats including Parquet, JSON, CSV, and XML, and understand data quality, validation, and anomaly detection concepts.\nYou use Git for source control and collaborative development.\n\nDesired Skills\n\nExperience orchestrating workflows with Apache Airflow, including DAG design and scheduling. Experience querying and aggregating data using SQL technologies such as MySQL, MariaDB, or PostgreSQL. Familiarity with HPC job scheduling tools such as Slurm. Experience using the Atlassian tool suite, including Jira and Confluence.\n\nTechnology Stack\n\nThis role operates across Python and Java development, Streamlit for UI construction, TypeScript, HTML, CSS, and modern JavaScript frameworks such as React, Angular, or Vue.\n\nThe environment includes Apache Spark for distributed processing, Jupyter Notebooks for analysis, pandas and NumPy for data preparation, Git for source control, Linux-based systems, and supporting technologies such as SQL databases, Airflow, and HPC scheduling tools.\n\nCompensation Range:\n$179, $237,000.00\n\nThe salary range provided reflects an estimate based on current market trends and may be adjusted based on factors such as the candidate's experience, skills, and qualifications.\nThe final offer will be tailored after a thorough evaluation of the candidate's background and suitability for the role. Please note that this range is intended as a guideline and is subject to flexibility\n\nWhy Intelliforce? Because you matter—your work, your growth, and your well-being.\n\nAt Intelliforce, we don't just push the boundaries of technology—we partner with some of the most mission-driven teams in defense and beyond to solve challenges that truly matter.\nAs a Systems Engineer here, you won't just contribute to projects—you'll help shape outcomes that make a real-world impact.\n\nWe also know that great work starts with a great environment.\n\nThat's why we invest in you:\nAmple PTO to rest and recharge—plus all federal holidays and your birthday off, just because.\nMultiple medical plan options, including ones with zero deductible or premium for employees.\nGenerous 401(k) with immediate vesting—because your future matters now.\nExciting bonus opportunities, from profit sharing to quarterly awards and President's Club recognition.\nA culture of collaboration, connection, and fun, with regular team activities that go beyond the work.\n\nReady to grow with purpose?\n\nAt Intelliforce, your career will flourish in a place where innovation thrives and people come first. Join us—and let's build something meaningful together.\n\nYou can reach us at - or schedule a call with our Director of Recruitment, just visit this link to view their calendar:\n.\n\nEqual Opportunity Matters\n\nIntelliforce-IT Solutions Group, LLC is proud to be an Equal Opportunity/Affirmative Action Employer. U.S. Citizenship is required for most positions.\n\nNeed accommodations during the application process? We're happy to help. Reach out to us at - with your specific request.\n\nPowered by JazzHR\n\nCoIz4oRKgb\n\nSeniority level\nMid-Senior level\nEmployment type\nFull-time\nJob function\nEngineering and Information Technology\nIndustries\nInternet Publishing",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770422400,
    "job_posted_at_datetime_utc": "2026-02-07T00:00:00.000Z",
    "job_location": "Annapolis Junction, MD",
    "job_city": "Annapolis Junction",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.1209588,
    "job_longitude": -76.77564459999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDnJQNbAuRHlzHfYrAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Active Top Secret Clearance with Full Scope Polygraph required",
        "Active Top Secret Clearance with Full Scope Polygraph required",
        "Must be a U.S. Citizen",
        "Bachelor's degree in Computer Science or a related technical discipline with seven years of software engineering experience supporting programs of similar scope, type, and complexity, or a Master's degree with five years of experience, or nine years of relevant experience in lieu of a degree",
        "You bring strong experience developing software in Linux environments, including use of the Linux command line and Bash scripting to automate manual processes",
        "You have recent hands-on experience with Python and Java, and familiarity building interactive applications using Streamlit",
        "You are comfortable developing front-end applications using TypeScript, HTML, and CSS, and have experience with modern JavaScript frameworks such as React, Angular, or Vue",
        "You have worked with distributed data processing engines like Apache Spark, use Jupyter Notebooks effectively, and perform data wrangling with tools such as pandas and NumPy",
        "You are experienced working with structured, semi-structured, and unstructured data formats including Parquet, JSON, CSV, and XML, and understand data quality, validation, and anomaly detection concepts",
        "You use Git for source control and collaborative development",
        "Experience orchestrating workflows with Apache Airflow, including DAG design and scheduling",
        "Experience querying and aggregating data using SQL technologies such as MySQL, MariaDB, or PostgreSQL",
        "Familiarity with HPC job scheduling tools such as Slurm",
        "Experience using the Atlassian tool suite, including Jira and Confluence",
        "U.S. Citizenship is required for most positions"
      ],
      "Benefits": [
        "Compensation Range:",
        "$179, $237,000.00",
        "The salary range provided reflects an estimate based on current market trends and may be adjusted based on factors such as the candidate's experience, skills, and qualifications",
        "Ample PTO to rest and recharge—plus all federal holidays and your birthday off, just because",
        "Multiple medical plan options, including ones with zero deductible or premium for employees",
        "Generous 401(k) with immediate vesting—because your future matters now",
        "Exciting bonus opportunities, from profit sharing to quarterly awards and President's Club recognition",
        "A culture of collaboration, connection, and fun, with regular team activities that go beyond the work"
      ],
      "Responsibilities": [
        "In this role, you will help modernize and extend a critical data analytics user interface, transforming a prototype into a production-ready capability",
        "You will work alongside mission partners to enhance how data is explored, visualized, and delivered, ensuring analysts have intuitive, reliable tools that scale beyond the enterprise",
        "This position sits squarely within Intelliforce's core strengths in software engineering, data analytics, and mission delivery",
        "You will enhance and mature a Streamlit-based UI prototype into a robust, scalable interface with a forward path to external availability",
        "You will design and implement new features, refine existing workflows, and ensure performance, usability, and data integrity",
        "Your work will include developing front-end components, integrating back-end services, handling diverse data formats, and collaborating with engineers and stakeholders to translate requirements into clean, functional solutions",
        "You will also contribute to testing, documentation, and continuous improvement of the platform",
        "This role operates across Python and Java development, Streamlit for UI construction, TypeScript, HTML, CSS, and modern JavaScript frameworks such as React, Angular, or Vue",
        "The environment includes Apache Spark for distributed processing, Jupyter Notebooks for analysis, pandas and NumPy for data preparation, Git for source control, Linux-based systems, and supporting technologies such as SQL databases, Airflow, and HPC scheduling tools"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "us-bebee-com-job-24659c2a46e74f6da35287fa63f6dfaf",
    "_source": "new_jobs"
  },
  {
    "job_id": "WT7WCedlTWN5MzaOAAAAAA==",
    "job_title": "Senior BI & Analytics Engineer",
    "employer_name": "Karsun Solutions, LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQnnqnWMxl5Nf-a30Q-JIB8w_DdzQGIE6s9hZl_&s=0",
    "employer_website": "https://karsun-llc.com",
    "job_publisher": "Learn4Good",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.learn4good.com/jobs/winston-salem/north-carolina/info_technology/4865908481/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/winston-salem/north-carolina/info_technology/4865908481/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A respected technology transformation company is seeking a Senior BI Developer to enhance workflows and dashboards, document BI requirements, and implement BI best practices. The ideal candidate will have at least 8 years of experience with expert knowledge in Python, Alteryx, Tableau, and Power BI, and will work towards maintaining a Public Trust Security Clearance. The position offers a salary range of $125,000 to $135,000 and seeks individuals residing in authorized US states.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Winston-Salem, NC",
    "job_city": "Winston-Salem",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 36.0948221,
    "job_longitude": -80.2434028,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DWT7WCedlTWN5MzaOAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 125000,
    "job_max_salary": 135000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have at least 8 years of experience with expert knowledge in Python, Alteryx, Tableau, and Power BI, and will work towards maintaining a Public Trust Security Clearance"
      ],
      "Benefits": [
        "The position offers a salary range of $125,000 to $135,000 and seeks individuals residing in authorized US states"
      ],
      "Responsibilities": [
        "A respected technology transformation company is seeking a Senior BI Developer to enhance workflows and dashboards, document BI requirements, and implement BI best practices"
      ]
    },
    "job_onet_soc": "15119900",
    "job_onet_job_zone": "4",
    "id": "www-learn4good-com-jobs-winston-salem-north-carolina-info_technology-4865908481-e",
    "_source": "new_jobs"
  },
  {
    "job_id": "aFi9I2wuRtpwexnLAAAAAA==",
    "job_title": "Analytics Engineer Hybrid 43",
    "employer_name": "WeedMaps",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQpHMJ_XBl8h2_45sDfzi3fdrC7sEmmeZlNeXkU&s=0",
    "employer_website": "https://weedmaps.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-tn-all-cities-analytics-engineer-hybrid-43-weedmaps-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-tn-all-cities-analytics-engineer-hybrid-43-weedmaps-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Analytics Engineer (Hybrid)\n\nOverview:\n\nThe Analytics Engineer at Weedmaps will be a key technical contributor within our Data organization, supporting various aspects of the business. In this role, you will build and maintain data pipelines and analytics frameworks to answer business questions and enable root cause problem solving. You will collaborate extensively with business, engineering, product, and data science teams to drive data-informed decision making across our customer acquisition and retention strategies. The ideal candidate combines strong technical skills with e-commerce domain knowledge and can translate business requirements into scalable data solutions that deliver measurable impact on our growth metrics.\n\nAs part of our Data organization, you will develop robust analytics systems that address unique challenges in our marketplace, including user journey exploration, customer and product analytics. The explosive growth of the cannabis industry requires increasingly sophisticated analytics solutions that can scale with our business and comply with complex regulatory requirements.\n\nThe impact you'll make:\n• Design fault-tolerant dbt models to synthesize data from multiple sources into mart tables\n• Design and implement Sigma dashboards and Streamplit apps to provide clear insights into performance\n• Automate regular reporting workflows to reduce manual effort and increase data consistency\n• Create self-service tools that empower business teams to access insights independently\n• Analyze experiment results to identify opportunities for improving ROI\n• Develop and maintain data pipelines using SQL within modern data stack tools (Snowflake, DBT, Metaplane)\n• Create and document data models that transform raw data into reliable, business-ready datasets with accompanying dictionaries and testing\n• Implement data quality checks and testing frameworks to ensure the accuracy and reliability of analytics\nWhat you've accomplished:\n• 4+ years of experience in data engineering, or similar technical role\n• 2+ years of experience in analytics engineering\n• Experience with modern data warehouse platforms, preferably Snowflake\n• Expertise in SQL and data modeling with dbt\n• Proficiency in Python\nBonus points:\n• Experience with business intelligence tools, preferably Sigma\n• E-commerce or marketplace business experience preferred\n• Regulated industry experience - nice to have\nThe base pay range for this position is $172,000- $193,363 per year\n\n2026 US Benefits for Full Time, Regular Employees:\n• Physical Health (Medical, Dental & Vision)\n• 100% employer-paid premium for employees\n• Up to 80% coverage for dependents\n• Company HSA contribution with the High Deductible Health Plan\n• 401(k) Retirement Plan (employer will match contribution up to 3. 5% of employee contribution)\n• Basic Life, Voluntary Life and AD&D Insurance options\n• Supplemental, voluntary benefits\n• Student Loan Repayment/529 Education Savings with a monthly company contribution\n• FSA (Medical, Dependent, Transit and Parking)\n• Voluntary Life and AD&D Insurance\n• Critical Illness Insurance\n• Accident Insurance\n• Short- and Long-term Disability Insurance\n• Pet Insurance\n• Identity theft protection\n• Legal access to a network of attorneys\n• PTO, paid sick leave, and company holidays (including a 2026 holiday shutdown)\n• Paid parental leave\nWhy Work at Weedmaps?\n\nLife at Weedmaps means innovation and heart. Come join us if you care about the plant, the people who love it, and are ready to let your talent shine. We foster a bustling and collaborative culture that revolves around an environment that focuses on the benefits of weed, and the community that supports it.\n\nYou too can have a hand in shaping the industry's future; ready to roll with us?\n\nSee how we've grown-our journey, leadership team, and what's next at Weedmaps. com/corporate\n\nAbout Weedmaps:\n\nFounded in 2008, we've grown from a small startup to a global leader in the cannabis industry. Our dedication to transparency, education, and community has set us apart, and today, we proudly serve cannabis to consumers and businesses in the U. S. and worldwide.\n\n\"Freedom to choose. Freedom to access. Freedom to enjoy. \"\n\nNotice to prospective Weedmaps job applicants:\n\nOur team has been made aware of incidents involving LinkedIn, Telegram, and Facebook accounts impersonating Weedmaps recruiters. These individuals are attempting to use our company name to solicit payment from prospective candidates interested in applying for jobs at our company. Our team is actively working to combat these attempts, but in the meantime, please be mindful of the following:\n• Our recruiters will always communicate with candidates through an @weedmaps. com email address.\n• CORRECT: . @weedmaps. com\n• INCORRECT: . @gmail. com\n• Our recruiters will NEVER ask for or attempt to solicit payment from applicants in order to apply, interview, or work for Weedmaps.\n• If you are interested in a role at Weedmaps, please apply through our established channels.\n• Weedmaps Careers Page or LinkedIn\n\nIf you are unsure if a communication is legitimate, please contact our recruitment team at . @weedmaps. com and they will happily confirm for you. Thank you for your vigilance and we appreciate your interest in working with us!\n\nWeedmaps is an equal opportunity employer and makes employment decisions on the basis of merit. The Company prohibits unlawful discrimination against employees or applicants based on race (including traits historically associated with race, such as hair texture and protective hairstyles), religion and religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, age, military status, veteran status, uniformed service member status, sexual orientation, transgender identity, citizenship status, pregnancy, or any other consideration made unlawful by federal, state, or local laws. The Company also prohibits unlawful discrimination based on the perception that anyone has any of those characteristics, or is associated with a person who has or is perceived as having any of those characteristics. Our company uses E-Verify to confirm the employment eligibility of all newly hired employees. To learn more about E-Verify, including your rights and responsibilities, please visit dhs. gov/E-Verify.\n\nApplicants are entitled to reasonable accommodations under the terms of the Americans with Disabilities Act and applicable state/local laws, unless the accommodation presents undue hardship. Please email us at peopleoperations at weedmaps. com if you would like to confidentially discuss a potential accommodation during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Tennessee",
    "job_city": null,
    "job_state": "Tennessee",
    "job_country": "US",
    "job_latitude": 35.517491299999996,
    "job_longitude": -86.5804473,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaFi9I2wuRtpwexnLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate combines strong technical skills with e-commerce domain knowledge and can translate business requirements into scalable data solutions that deliver measurable impact on our growth metrics",
        "4+ years of experience in data engineering, or similar technical role",
        "2+ years of experience in analytics engineering",
        "Experience with modern data warehouse platforms, preferably Snowflake",
        "Expertise in SQL and data modeling with dbt",
        "Proficiency in Python",
        "Experience with business intelligence tools, preferably Sigma",
        "Regulated industry experience - nice to have",
        "Legal access to a network of attorneys"
      ],
      "Benefits": [
        "The base pay range for this position is $172,000- $193,363 per year",
        "2026 US Benefits for Full Time, Regular Employees:",
        "Physical Health (Medical, Dental & Vision)",
        "100% employer-paid premium for employees",
        "Up to 80% coverage for dependents",
        "Company HSA contribution with the High Deductible Health Plan",
        "401(k) Retirement Plan (employer will match contribution up to 3",
        "5% of employee contribution)",
        "Basic Life, Voluntary Life and AD&D Insurance options",
        "Supplemental, voluntary benefits",
        "Student Loan Repayment/529 Education Savings with a monthly company contribution",
        "FSA (Medical, Dependent, Transit and Parking)",
        "Voluntary Life and AD&D Insurance",
        "Critical Illness Insurance",
        "Accident Insurance",
        "Short- and Long-term Disability Insurance",
        "Pet Insurance",
        "PTO, paid sick leave, and company holidays (including a 2026 holiday shutdown)",
        "Paid parental leave",
        "Freedom to access",
        "Freedom to enjoy"
      ],
      "Responsibilities": [
        "The Analytics Engineer at Weedmaps will be a key technical contributor within our Data organization, supporting various aspects of the business",
        "In this role, you will build and maintain data pipelines and analytics frameworks to answer business questions and enable root cause problem solving",
        "You will collaborate extensively with business, engineering, product, and data science teams to drive data-informed decision making across our customer acquisition and retention strategies",
        "Design fault-tolerant dbt models to synthesize data from multiple sources into mart tables",
        "Design and implement Sigma dashboards and Streamplit apps to provide clear insights into performance",
        "Automate regular reporting workflows to reduce manual effort and increase data consistency",
        "Create self-service tools that empower business teams to access insights independently",
        "Analyze experiment results to identify opportunities for improving ROI",
        "Develop and maintain data pipelines using SQL within modern data stack tools (Snowflake, DBT, Metaplane)",
        "Create and document data models that transform raw data into reliable, business-ready datasets with accompanying dictionaries and testing",
        "Implement data quality checks and testing frameworks to ensure the accuracy and reliability of analytics"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-jobilize-com-job-us-tn-all-cities-analytics-engineer-hybrid-43-weedmaps-hiring-now",
    "_source": "new_jobs"
  },
  {
    "job_id": "XAY1ZGCPVXZ2djEhAAAAAA==",
    "job_title": "Senior Analytics Engineer, Revenue Operations",
    "employer_name": "DNSFilter",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSNX15ECaIW708N2bP7xkfHys6oDsq3bdDMp-jE&s=0",
    "employer_website": "https://www.dnsfilter.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/DNSFilter/Job/Senior-Analytics-Engineer,-Revenue-Operations/-in-Tampa,FL?jid=2bd1004a3b8d5fcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/DNSFilter/Job/Senior-Analytics-Engineer,-Revenue-Operations/-in-Tampa,FL?jid=2bd1004a3b8d5fcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Energy Jobline",
        "apply_link": "https://www.energyjobline.com/job/senior-analytics-engineer-revenue-operations-tampa-29339333?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-analytics-engineer-revenue-operations-tampa-fl--5b88feb5-7b6b-4dae-88b1-d4fe5bc04a4b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/b349576177e17bb28ec69bd9b5a00a28?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6983dc5b01214b4cdacbfbfc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/senior-analytics-engineer-revenue-operations--tampa--e3fcc5e3d10e3b4e113602d5a797a69c3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/tampa/florida/info_technology/4862446604/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "DNSFilter's mission is to protect our customers and partners with products they love to use! We are revolutionizing network security by providing fast, accurate, and reliable threat protection and content filtering. We're a rapidly growing company dedicated to creating a safer internet for businesses and organizations worldwide. Leveraging AI-driven threat intelligence, DNSFilter empowers our customers to proactively block threats before they impact their networks. We foster a collaborative, innovative, and results-oriented culture where every team member contributes to our mission of making the internet safer.\n\nAs we continue our product-fueled growth by adding new features and broadening our solution to meet the needs of the global market, it's clear there's a missing piece. That's where you come in!\n\nWe're seeking a Senior Analytics Engineer, Revenue Operations, to own and scale the RevOps data domain. This role sits on the Revenue Operations team, will report to the Senior Director, RevOps, and partners closely with Sales, Marketing, Customer Support, Customer Success, Finance, Product, and the Data Platform team to deliver trusted, analytics-ready data that powers GTM systems, reporting, and decision-making.\n\nThis is a senior individual contributor role with deep ownership. You'll treat the RevOps data domain as a product, not a collection of dashboards—balancing rigor, scalability, and stakeholder alignment in a fast-moving environment.\n\nEligible candidates have and can work successfully in a small to mid-sized fast-paced, hyper-growth, SaaS start-up or scale-up. This is a full-time role open to candidates in the United States and Canada.\n\nWe recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If you feel like this job is for you, please apply. We believe diversity of experience and skills, including transferable skills, combined with passion, is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions!\n\nAt DNSFilter, You Will:\n\nOwn the RevOps Data Domain\n• Architect and own the RevOps / BizOps data zone within our Data Mesh, treating it as a product that is high-quality, discoverable, well-documented, and the authoritative source of truth for revenue data\n• Establish clear ownership, documentation, and governance for core RevOps datasets and metrics\n\nBuild & Scale Analytics Foundations\n• Own the end-to-end design, development, and maintenance of analytics solutions that power downstream GTM systems, reporting, and decision-making\n• Partner with the Data Platform (Data Engineering) team to transform data landed from internal databases, third-party APIs, and Airflow-managed pipelines into analytics-ready models\n• Develop, optimize, and maintain ELT pipelines using dbt Cloud, transforming PostgreSQL and other source data into analytics-ready datasets in Amazon Athena\n• Ensure reliable data availability, performance, and quality within our Athena-based analytics data zone\n\nOwn Data State, History, and Performance\n• Lead our approach to capturing data evolution and history, making heavy use of dbt snapshots and incremental models to support point-in-time and trend analysis\n• Write performant, cost-aware SQL optimized for distributed query engines (Amazon Athena), including queries operating over tens of billions of rows\n• Design models with scalability and maintainability in mind, prioritizing long-term clarity over clever but fragile solutions\n\nDefine, Govern & Operationalize Metrics\n• Design, implement, and maintain a semantic layer (e.g., dbt semantic models/metrics) that serves as the authoritative source for business definitions and revenue metrics\n• Introduce and steward a centralized data dictionary and metric catalog, ensuring consistent definitions across dashboards, reports, and GTM systems\n• Partner closely with RevOps, Finance, Product, and GTM stakeholders to define, govern, and evolve shared metrics—aligning on \"what is what\" as the business grows\n• Act as a trusted arbiter for metric definitions, managing versioning, documentation, and change communication\n\nModel Revenue Data for Downstream GTM Systems\n• Design data models that are consumable by the GTM Systems team for use in downstream operational tools such as Salesforce, Hubspot, and Zendesk\n• Expose clean, well-documented datasets and metrics that can be reliably reused across reporting, automation, and operational workflows\n• Partner with the GTM Systems team to ensure data models meet operational needs, performance requirements, and system constraints\n• Ensure consistency between analytical models and operational system logic, minimizing metric drift between analytics and GTM tooling\n\nOwn BI Reporting & Visualization\n• Own the design, development, and ongoing maintenance of RevOps dashboards, reports, and visualizations in our BI tool\n• Ensure all reporting is powered by governed, well-modeled, and tested datasets—avoiding one-off queries and metric drift\n• Partner with GTM, RevOps, and executive stakeholders to translate business needs into scalable, self-service dashboards\n• Continuously audit and improve existing dashboards for accuracy, performance, usability, and clarity\n• Define standards and best practices for dashboard design, metric presentation, and reporting governance\n\nDrive Accuracy, Simplicity & Trust\n• Get \"down in the weeds\" to validate data end-to-end, using SQL, spreadsheets, and source-system analysis to trace and resolve discrepancies\n• Uphold a \"simplicity at scale\" philosophy—choosing readable, maintainable SQL over over-engineered abstractions that accrue technical debt\n• Maintain logic at the appropriate layer: when metrics require new or corrected product data, partner with upstream owners rather than introducing brittle downstream workarounds\n• Review, audit, and refactor existing dbt models, ELT jobs, and dashboards to improve accuracy, performance, and maintainability\n\nEnable the Business & Look Forward\n• Design, build, and maintain dashboards and reports powered by well-modeled, tested data—avoiding one-off queries and metric drift\n• Support the GTM Systems team by ensuring analytics and reporting dependencies are reliable and well-documented\n• Leverage AI-assisted development tools to accelerate SQL development, dbt modeling, testing, documentation, and refactoring\n• Explore and pilot AI-enabled approaches to improve data quality, observability, and operational efficiency with sound judgment around accuracy and governance\n• Establish and promote best practices for data modeling, testing, documentation, and dashboard governance\n\nTo Qualify for this Role, You Have:\n• 5+ years of experience in analytics engineering or data engineering, specifically supporting GTM, RevOps, or BizOps functions\n• Expert dbt knowledge, including advanced use of incremental strategies, snapshotting, and modular project structure—you know when to use a macro and when not to\n• Deep proficiency in SQL with experience optimizing queries for modern distributed warehouses (e.g., Amazon Athena, BigQuery, Snowflake), including partitioning and cost optimization\n• Hands-on experience designing and maintaining analytics-ready data models and ELT pipelines from application and operational data sources\n• Experience implementing or working with semantic layers or governed metrics frameworks (e.g., dbt semantic layer or equivalent)\n• A meticulous, almost obsessive approach to data accuracy—you aren't satisfied until the numbers tie out 1:1 and you can prove it\n• Demonstrated ability to reconcile complex datasets across systems and identify root causes of discrepancies\n• A strong \"do it right\" mindset, including the ability to push back on unscalable requests and prioritize durable solutions over short-term fixes\n• Understanding of data mesh principles, domain ownership, and the discipline required to maintain a standalone analytics data zone\n• Strong communication skills and comfort level in influencing both technical and non-technical stakeholders\n\nBonus points for:\n• Direct experience working on a Revenue Operations team\n• Experience supporting Sales, Marketing, Customer Support, and Customer Success analytics\n• Experience introducing or maturing a centralized data dictionary and driving organizational adoption of governed metrics\n• Exposure to data mesh or domain-oriented data ownership models in production environments\n• Experience applying AI-assisted development tools to analytics engineering workflows (SQL, dbt, testing, documentation, refactoring)\n• Familiarity with revenue lifecycle metrics (pipeline, conversion rates, ARR/MRR, churn, expansion, forecasting)\n\nWe Offer:\n• Pathway to promotion to additional organizational positions and responsibilities based upon results and performance, not just time in the chair. You help us grow, and we will help you grow.\n• Passionate and intelligent colleagues who work hard and have a good time doing it\n• Paid company-wide week off at the end of each year\n• Flexible Vacation Policy\n• Awesome company swag\n• Full medical, dental, and vision benefits for US, UK, and Canada-based employees\n• Full short-term disability and life benefits; available long-term disability\n• Retirement savings account options with vested company matching for qualifying employees\n• In-person annual gatherings. Last time we all spent a week on a beach in Cancun!\n\nDNSFilter is a pay-for-performance organization, which means there is an opportunity to advance your compensation based on performance over time. The hiring base pay is dependent on several factors, including level, function, training, transferable skills, work experience, business needs, and geographic location. As a hybrid company, our compensation reflects the cost of labor across several U.S. and global geographic markets. We pay differently based on those defined markets. Our Talent Team can share more about the specific salary range for the job location during the hiring process.\n\nDNSFilter participates in the E-Verify program.\n\nAt DNSFilter, we utilize sophisticated software and tools to identify and eliminate Deepfake candidates. This approach helps us maintain the integrity of our hiring process, ensuring that we select the most qualified and genuine individuals to join our team.\nU.S. hiring salary range\n$130,000—$170,000 USD",
    "job_is_remote": false,
    "job_posted_at": "10 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Tampa, FL",
    "job_city": "Tampa",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 27.951689599999998,
    "job_longitude": -82.45875269999999,
    "job_benefits": [
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXAY1ZGCPVXZ2djEhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Eligible candidates have and can work successfully in a small to mid-sized fast-paced, hyper-growth, SaaS start-up or scale-up",
        "Drive Accuracy, Simplicity & Trust",
        "Get \"down in the weeds\" to validate data end-to-end, using SQL, spreadsheets, and source-system analysis to trace and resolve discrepancies",
        "5+ years of experience in analytics engineering or data engineering, specifically supporting GTM, RevOps, or BizOps functions",
        "Expert dbt knowledge, including advanced use of incremental strategies, snapshotting, and modular project structure—you know when to use a macro and when not to",
        "Deep proficiency in SQL with experience optimizing queries for modern distributed warehouses (e.g., Amazon Athena, BigQuery, Snowflake), including partitioning and cost optimization",
        "Hands-on experience designing and maintaining analytics-ready data models and ELT pipelines from application and operational data sources",
        "Experience implementing or working with semantic layers or governed metrics frameworks (e.g., dbt semantic layer or equivalent)",
        "A meticulous, almost obsessive approach to data accuracy—you aren't satisfied until the numbers tie out 1:1 and you can prove it",
        "Demonstrated ability to reconcile complex datasets across systems and identify root causes of discrepancies",
        "A strong \"do it right\" mindset, including the ability to push back on unscalable requests and prioritize durable solutions over short-term fixes",
        "Understanding of data mesh principles, domain ownership, and the discipline required to maintain a standalone analytics data zone",
        "Strong communication skills and comfort level in influencing both technical and non-technical stakeholders",
        "Direct experience working on a Revenue Operations team",
        "Experience supporting Sales, Marketing, Customer Support, and Customer Success analytics",
        "Experience introducing or maturing a centralized data dictionary and driving organizational adoption of governed metrics",
        "Exposure to data mesh or domain-oriented data ownership models in production environments",
        "Experience applying AI-assisted development tools to analytics engineering workflows (SQL, dbt, testing, documentation, refactoring)",
        "Familiarity with revenue lifecycle metrics (pipeline, conversion rates, ARR/MRR, churn, expansion, forecasting)"
      ],
      "Benefits": [
        "Pathway to promotion to additional organizational positions and responsibilities based upon results and performance, not just time in the chair",
        "Passionate and intelligent colleagues who work hard and have a good time doing it",
        "Paid company-wide week off at the end of each year",
        "Flexible Vacation Policy",
        "Awesome company swag",
        "Full medical, dental, and vision benefits for US, UK, and Canada-based employees",
        "Full short-term disability and life benefits; available long-term disability",
        "Retirement savings account options with vested company matching for qualifying employees",
        "In-person annual gatherings",
        "DNSFilter is a pay-for-performance organization, which means there is an opportunity to advance your compensation based on performance over time",
        "The hiring base pay is dependent on several factors, including level, function, training, transferable skills, work experience, business needs, and geographic location",
        "$130,000—$170,000 USD"
      ],
      "Responsibilities": [
        "We're seeking a Senior Analytics Engineer, Revenue Operations, to own and scale the RevOps data domain",
        "This role sits on the Revenue Operations team, will report to the Senior Director, RevOps, and partners closely with Sales, Marketing, Customer Support, Customer Success, Finance, Product, and the Data Platform team to deliver trusted, analytics-ready data that powers GTM systems, reporting, and decision-making",
        "This is a senior individual contributor role with deep ownership",
        "You'll treat the RevOps data domain as a product, not a collection of dashboards—balancing rigor, scalability, and stakeholder alignment in a fast-moving environment",
        "Architect and own the RevOps / BizOps data zone within our Data Mesh, treating it as a product that is high-quality, discoverable, well-documented, and the authoritative source of truth for revenue data",
        "Establish clear ownership, documentation, and governance for core RevOps datasets and metrics",
        "Build & Scale Analytics Foundations",
        "Own the end-to-end design, development, and maintenance of analytics solutions that power downstream GTM systems, reporting, and decision-making",
        "Partner with the Data Platform (Data Engineering) team to transform data landed from internal databases, third-party APIs, and Airflow-managed pipelines into analytics-ready models",
        "Develop, optimize, and maintain ELT pipelines using dbt Cloud, transforming PostgreSQL and other source data into analytics-ready datasets in Amazon Athena",
        "Ensure reliable data availability, performance, and quality within our Athena-based analytics data zone",
        "Own Data State, History, and Performance",
        "Lead our approach to capturing data evolution and history, making heavy use of dbt snapshots and incremental models to support point-in-time and trend analysis",
        "Write performant, cost-aware SQL optimized for distributed query engines (Amazon Athena), including queries operating over tens of billions of rows",
        "Design models with scalability and maintainability in mind, prioritizing long-term clarity over clever but fragile solutions",
        "Design, implement, and maintain a semantic layer (e.g., dbt semantic models/metrics) that serves as the authoritative source for business definitions and revenue metrics",
        "Introduce and steward a centralized data dictionary and metric catalog, ensuring consistent definitions across dashboards, reports, and GTM systems",
        "Partner closely with RevOps, Finance, Product, and GTM stakeholders to define, govern, and evolve shared metrics—aligning on \"what is what\" as the business grows",
        "Act as a trusted arbiter for metric definitions, managing versioning, documentation, and change communication",
        "Model Revenue Data for Downstream GTM Systems",
        "Design data models that are consumable by the GTM Systems team for use in downstream operational tools such as Salesforce, Hubspot, and Zendesk",
        "Expose clean, well-documented datasets and metrics that can be reliably reused across reporting, automation, and operational workflows",
        "Partner with the GTM Systems team to ensure data models meet operational needs, performance requirements, and system constraints",
        "Ensure consistency between analytical models and operational system logic, minimizing metric drift between analytics and GTM tooling",
        "Own BI Reporting & Visualization",
        "Own the design, development, and ongoing maintenance of RevOps dashboards, reports, and visualizations in our BI tool",
        "Ensure all reporting is powered by governed, well-modeled, and tested datasets—avoiding one-off queries and metric drift",
        "Partner with GTM, RevOps, and executive stakeholders to translate business needs into scalable, self-service dashboards",
        "Continuously audit and improve existing dashboards for accuracy, performance, usability, and clarity",
        "Define standards and best practices for dashboard design, metric presentation, and reporting governance",
        "Uphold a \"simplicity at scale\" philosophy—choosing readable, maintainable SQL over over-engineered abstractions that accrue technical debt",
        "Maintain logic at the appropriate layer: when metrics require new or corrected product data, partner with upstream owners rather than introducing brittle downstream workarounds",
        "Review, audit, and refactor existing dbt models, ELT jobs, and dashboards to improve accuracy, performance, and maintainability",
        "Enable the Business & Look Forward",
        "Design, build, and maintain dashboards and reports powered by well-modeled, tested data—avoiding one-off queries and metric drift",
        "Support the GTM Systems team by ensuring analytics and reporting dependencies are reliable and well-documented",
        "Leverage AI-assisted development tools to accelerate SQL development, dbt modeling, testing, documentation, and refactoring",
        "Explore and pilot AI-enabled approaches to improve data quality, observability, and operational efficiency with sound judgment around accuracy and governance",
        "Establish and promote best practices for data modeling, testing, documentation, and dashboard governance"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-dnsfilter-job-senior-analytics-engineer-revenue-operations-in-tampa-fl",
    "_source": "new_jobs"
  },
  {
    "job_id": "GJe7t_NJZN-bb34PAAAAAA==",
    "job_title": "GTM Engineer (Clay & Cold Email Experience)",
    "employer_name": "Pump",
    "employer_logo": null,
    "employer_website": "https://www.pump.co",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/analytics-engineer?id=2454857449&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/analytics-engineer?id=2454857449&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-ca-san-francisco-gtm-engineer-clay-cold-email-experience-pump-hiring?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Overview\n\nJoin to apply for the GTM Engineer (Clay & Cold Email Experience) role at Pump.co .\n\nThis range is provided by Pump.co. Your actual pay will be based on your skills and experience — talk with your recruiter to learn more.\nBase pay range\n\n$80,000.00/yr - $120,000.00/yr\n\nThis role is based in our San Francisco office 5 days a week. Through commission and our highly valuable equity, OTE will be around $300K.\n\nWe’re growing fast and looking for a scrappy GTM Engineer to supercharge our outbound motion and fuel sales growth. This is an onsite role that blends technical chops with go-to-market instinct—perfect for someone who thrives at the intersection of code and hustle.\n\nDeep familiarity with Clay is a must. You should know how to build, automate, and scale workflows in Clay without hand-holding. We’re looking for someone who can push the limits of what Clay can do and turn it into a competitive advantage across our outbound strategy.\nResponsibilities Partner with sales and marketing to accelerate outbound efforts. Build scrappy tools, scripts, and automations that improve how we find, convert, and support customers. Create and maintain demo setups, sales enablement docs, and outbound playbooks. Enrich lead data and contribute to outreach strategies using Clay , LinkedIn, and scraping utilities. Troubleshoot blockers during handoff or onboarding and relay insights to the GTM team. Continuously test new outbound approaches and find creative ways to scale what works. Qualifications Must have hands-on experience using Clay for outbound workflows. Bachelor’s in STEM (Computer Science, Engineering, Data Science, or similar). Strong technical foundation—comfortable with APIs, no-code tools, and writing lightweight scripts. Fast learner who enjoys diving into new tools, problems, and workflows. Excellent communicator—able to explain technical ideas in simple terms. Excited to work cross-functionally with sales & marketing. Bonus: Internship at a startup or B2B SaaS company. Bonus: Side projects, hackathons, or real-world builds that show initiative and follow-through. Ability to work 5 days a week in office. Competitive salary and equity. Full benefits package, including premium healthcare coverage. Professional development and learning reimbursements. 3–4x/year company-paid team retreats. A smart, kind, and high-performing team that loves building together. Compensation\n\n$80,000 USD - $120,000 USD\nEmployment details Seniority level: Entry level Employment type: Full-time Job function: Engineering and Information Technology Industries: Technology, Information and Internet\n\nSan Francisco, CA and related compensation ranges were provided within the posting. We’re unlocking community knowledge in a new way. Experts add insights directly into each article, started with the help of AI.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "San Francisco, CA",
    "job_city": "San Francisco",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.7749295,
    "job_longitude": -122.4194155,
    "job_benefits": [
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGJe7t_NJZN-bb34PAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Deep familiarity with Clay is a must",
        "You should know how to build, automate, and scale workflows in Clay without hand-holding",
        "We’re looking for someone who can push the limits of what Clay can do and turn it into a competitive advantage across our outbound strategy",
        "Qualifications Must have hands-on experience using Clay for outbound workflows",
        "Bachelor’s in STEM (Computer Science, Engineering, Data Science, or similar)",
        "Strong technical foundation—comfortable with APIs, no-code tools, and writing lightweight scripts",
        "Fast learner who enjoys diving into new tools, problems, and workflows",
        "Excellent communicator—able to explain technical ideas in simple terms",
        "Bonus: Side projects, hackathons, or real-world builds that show initiative and follow-through",
        "Ability to work 5 days a week in office"
      ],
      "Benefits": [
        "Your actual pay will be based on your skills and experience — talk with your recruiter to learn more",
        "Base pay range",
        "$80,000.00/yr - $120,000.00/yr",
        "Through commission and our highly valuable equity, OTE will be around $300K",
        "Bonus: Internship at a startup or B2B SaaS company",
        "Competitive salary and equity",
        "Full benefits package, including premium healthcare coverage",
        "Professional development and learning reimbursements",
        "3–4x/year company-paid team retreats",
        "$80,000 USD - $120,000 USD"
      ],
      "Responsibilities": [
        "Responsibilities Partner with sales and marketing to accelerate outbound efforts",
        "Build scrappy tools, scripts, and automations that improve how we find, convert, and support customers",
        "Create and maintain demo setups, sales enablement docs, and outbound playbooks",
        "Enrich lead data and contribute to outreach strategies using Clay , LinkedIn, and scraping utilities",
        "Troubleshoot blockers during handoff or onboarding and relay insights to the GTM team",
        "Continuously test new outbound approaches and find creative ways to scale what works"
      ]
    },
    "job_onet_soc": "41903100",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-analytics-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "QMFI59Aeon5OiMxQAAAAAA==",
    "job_title": "Data Solutions Engineer",
    "employer_name": "Early Career",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Citi Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.citi.com/job/irving/data-solutions-engineer/287/91594717280?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Citi Careers",
        "apply_link": "https://jobs.citi.com/job/irving/data-solutions-engineer/287/91594717280?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9ead36078a466c4a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-solutions-engineer-citi-JV_IC1140006_KO0,23_KE24,28.htm?jl=1010030029481&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Citi/Job/Data-Solutions-Engineer/-in-Irving,TX?jid=dbbae6d41b22e401&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-solutions-engineer-at-citi-4370775693?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-solutions-engineer-irving-tx-usa-56103968?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/18342d1b3a5230bb8ebc6b74cb10e255?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "PowerToFly",
        "apply_link": "https://powertofly.com/jobs/detail/2498189?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This Data Solutions Engineer (Applications Development Senior Programmer Analyst - C12) is responsible for building next-generation Data Engineering solutions. This intermediate-level position involves active participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team. A key aspect of this role is liaising between business users and technologists to facilitate the exchange of information regarding solutions, including requirements and usage.\n\nResponsibilities:\n• Serve as an integral team member of our Data Engineering team, responsible for the design and development of Big Data solutions.\n• Partner with domain experts, product managers, analysts, and data scientists to develop robust Big Data pipelines in Hadoop or Snowflake environments.\n• Responsible for delivering a data-as-a-service framework.\n• Responsible for moving all legacy workloads to cloud platform.\n• Lead the migration of all legacy workloads to cloud platforms.\n• Engage with key stakeholders to elicit and document requirements, including detailed data flow specifications.\n• Assess appropriate solutions and collaborate with relevant teams to drive optimal implementations.\n• Work with data scientists to build client pipelines using heterogeneous sources and provide essential engineering services for data science applications.\n• Research and evaluate open-source technologies and components, recommending and integrating them into design and implementation efforts.\n• Act as a technical expert, mentoring other team members on Big Data and Cloud technology stacks.\n• Define comprehensive requirements for maintainability, testability, performance, security, quality, and usability across the data platform.\n• Drive the implementation of consistent patterns, reusable components, and coding standards for all data engineering processes.\n• Convert SAS-based pipelines into modern languages like PySpark and Scala for execution on Hadoop and non-Hadoop ecosystems.\n• Optimize Big Data applications on both Hadoop and non-Hadoop platforms for peak performance.\n• Evaluate new IT developments and evolving business requirements, recommending appropriate system alternatives and/or enhancements to current systems through analysis of business processes, systems, and industry standards.\n• Appropriately assess risk when making business decisions, demonstrating consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets. This includes driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing, and reporting control issues with transparency.\n\nRequired Qualifications:\n• 5+ years of experience with Hadoop and Big Data technologies\n• Demonstrated proficiency in Python, PySpark, and Scala, including practical experience with fundamental machine learning libraries\n• Experience in developing robust data solutions leveraging Google Cloud or AWS platforms; relevant certifications are preferred\n• Experience with SAS\n• Experience with containerization and related technologies (e.g., Docker, Kubernetes)\n• Comprehensive understanding of software engineering and data analytics\n• In-depth knowledge and hands-on experience with the Hadoop ecosystem and Big Data technologies (e.g., HDFS, MapReduce, Hive, Pig, Impala, Kafka, Kudu, Solr)\n• Knowledge of Agile (Scrum) development methodologies.\n• Strong development and automation skills.\n• System-level understanding of data structures, algorithms, distributed storage, and compute.\n• A proactive approach to solving complex business problems, complemented by strong interpersonal and teamwork skills.\n\nDesired Qualifications\n• Familiarity with Hadoop administration and Snowflake.\n• Proficiency in Java or additional experience with Apache Beam.\n\nEducation:\n• Bachelor’s degree/University degree or equivalent experience\n\nApplicants must be authorized to work in the U.S for this position; Citi will not sponsor applicants for U.S. work authorization for this role. Candidate must be located within commuting distance or be willing to relocate to the area.\n\nThis job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.\n\n------------------------------------------------------\n\nJob Family Group:\nTechnology\n\n------------------------------------------------------\n\nJob Family:\nApplications Development\n\n------------------------------------------------------\n\nTime Type:\nFull time\n\n------------------------------------------------------\n\nPrimary Location:\nIrving Texas United States\n\n------------------------------------------------------\n\nPrimary Location Full Time Salary Range:\n$107,120.00 - $160,680.00\n\nIn addition to salary, Citi’s offerings may also include, for eligible employees, discretionary and formulaic incentive and retention awards. Citi offers competitive employee benefits, including: medical, dental & vision coverage; 401(k); life, accident, and disability insurance; and wellness programs. Citi also offers paid time off packages, including planned time off (vacation), unplanned time off (sick leave), and paid holidays. For additional information regarding Citi employee benefits, please visit citibenefits.com. Available offerings may vary by jurisdiction, job level, and date of hire.\n\n------------------------------------------------------\n\nMost Relevant Skills\nPlease see the requirements listed above.\n\n------------------------------------------------------\n\nOther Relevant Skills\nFor complementary skills, please see above and/or contact the recruiter.\n\n------------------------------------------------------\n\nAnticipated Posting Close Date:\nFeb 24, 2026\n\n------------------------------------------------------\n\nCiti is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law.\n\nIf you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.\n\nView Citi’s EEO Policy Statement and the Know Your Rights poster.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Jacksonville, FL",
    "job_city": "Jacksonville",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 30.3297566,
    "job_longitude": -81.6591529,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DQMFI59Aeon5OiMxQAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience with Hadoop and Big Data technologies",
        "Demonstrated proficiency in Python, PySpark, and Scala, including practical experience with fundamental machine learning libraries",
        "Experience with SAS",
        "Experience with containerization and related technologies (e.g., Docker, Kubernetes)",
        "Comprehensive understanding of software engineering and data analytics",
        "In-depth knowledge and hands-on experience with the Hadoop ecosystem and Big Data technologies (e.g., HDFS, MapReduce, Hive, Pig, Impala, Kafka, Kudu, Solr)",
        "Knowledge of Agile (Scrum) development methodologies",
        "Strong development and automation skills",
        "System-level understanding of data structures, algorithms, distributed storage, and compute",
        "A proactive approach to solving complex business problems, complemented by strong interpersonal and teamwork skills",
        "Bachelor’s degree/University degree or equivalent experience",
        "Applicants must be authorized to work in the U.S for this position; Citi will not sponsor applicants for U.S. work authorization for this role",
        "Candidate must be located within commuting distance or be willing to relocate to the area",
        "Please see the requirements listed above",
        "For complementary skills, please see above and/or contact the recruiter"
      ],
      "Benefits": [
        "$107,120.00 - $160,680.00",
        "In addition to salary, Citi’s offerings may also include, for eligible employees, discretionary and formulaic incentive and retention awards",
        "Citi offers competitive employee benefits, including: medical, dental & vision coverage; 401(k); life, accident, and disability insurance; and wellness programs",
        "Citi also offers paid time off packages, including planned time off (vacation), unplanned time off (sick leave), and paid holidays"
      ],
      "Responsibilities": [
        "This intermediate-level position involves active participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team",
        "A key aspect of this role is liaising between business users and technologists to facilitate the exchange of information regarding solutions, including requirements and usage",
        "Serve as an integral team member of our Data Engineering team, responsible for the design and development of Big Data solutions",
        "Partner with domain experts, product managers, analysts, and data scientists to develop robust Big Data pipelines in Hadoop or Snowflake environments",
        "Responsible for delivering a data-as-a-service framework",
        "Responsible for moving all legacy workloads to cloud platform",
        "Lead the migration of all legacy workloads to cloud platforms",
        "Engage with key stakeholders to elicit and document requirements, including detailed data flow specifications",
        "Assess appropriate solutions and collaborate with relevant teams to drive optimal implementations",
        "Work with data scientists to build client pipelines using heterogeneous sources and provide essential engineering services for data science applications",
        "Research and evaluate open-source technologies and components, recommending and integrating them into design and implementation efforts",
        "Act as a technical expert, mentoring other team members on Big Data and Cloud technology stacks",
        "Define comprehensive requirements for maintainability, testability, performance, security, quality, and usability across the data platform",
        "Drive the implementation of consistent patterns, reusable components, and coding standards for all data engineering processes",
        "Convert SAS-based pipelines into modern languages like PySpark and Scala for execution on Hadoop and non-Hadoop ecosystems",
        "Optimize Big Data applications on both Hadoop and non-Hadoop platforms for peak performance",
        "Evaluate new IT developments and evolving business requirements, recommending appropriate system alternatives and/or enhancements to current systems through analysis of business processes, systems, and industry standards",
        "Appropriately assess risk when making business decisions, demonstrating consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets",
        "This includes driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing, and reporting control issues with transparency",
        "This job description provides a high-level review of the types of work performed"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-citi-com-job-irving-data-solutions-engineer-287-91594717280",
    "_source": "new_jobs"
  },
  {
    "job_id": "4PMXCF0Zj3csVibCAAAAAA==",
    "job_title": "Data Engineering Manager - Advanced Analytics",
    "employer_name": "Niagara Bottling",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQD1lp7Y_nPmktOrGBYuyI9Zyj9tBqyLbRlieZB&s=0",
    "employer_website": "https://www.niagarawater.com",
    "job_publisher": "Niagara Bottling",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.niagarawater.com/us/en/job/R45735/Data-Engineering-Manager-Advanced-Analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Niagara Bottling",
        "apply_link": "https://careers.niagarawater.com/us/en/job/R45735/Data-Engineering-Manager-Advanced-Analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=b3a692c155871761&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In LA",
        "apply_link": "https://www.builtinla.com/job/lead-power-bi-developer/4193106?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/data-engineering-manager-advanced-analytics_7ea1afe75026a4bcd9f82ae335cbe32f2704f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineering-manager-advanced-analytics-at-niagara-bottling-4354561864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Milwaukee Jobs",
        "apply_link": "https://www.milwaukeejobs.com/j/t-Lead-Power-BI-Developer-e-Niagara-Bottling-LLC-l-Diamond-Bar,-CA-jobs-j81200452.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/fR7vBikqLIQowPy7vOhjEnP_TUG7KOmonv8qkEnyCUuf19LKhgmE1Q?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/manager-of-data-engineering-diamond-bar-california-us-niagara-bottling-r45735?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Niagara, we’re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.\n\nConsider applying here, if you want to:\n• Work in an entrepreneurial and dynamic environment with a chance to make an impact.\n• Develop lasting relationships with great people.\n• Have the opportunity to build a satisfying career.\n\nWe offer competitive compensation and benefits packages for our Team Members.\n\nData Engineering Manager - Advanced Analytics\n\nAs a key people leader within our data and analytics function, the Advanced Analytics Manager plays a critical role in architecting and maintaining the data infrastructure that underpins enterprise analytics. This role leads a team of data engineers and analytics professionals, focusing on the design, implementation, and optimization of scalable, reliable, and secure data pipelines, especially for complex, high-volume sources such as IoT and sensor-based systems. Working cross-functionally with operations, IT, and business stakeholders, the Advanced Analytics Manager ensures that data from diverse sources, including real-time telemetry, manufacturing systems, and traditional enterprise platforms are efficiently ingested, transformed, and made accessible for analytical consumption. In addition, the role includes light but growing exposure to Generative AI use cases, such as document summarization, chat interfaces for data access, and large language model (LLM) integration—especially in scenarios that augment data accessibility and user experience across the organization. This position requires not only technical expertise but also strategic thinking and leadership skills to evolve data infrastructure, support analytics scalability, and drive a data-first culture. A proven ability to lead high-performing teams and deliver impactful solutions in dynamic, data-rich environments is essential for success.\n• Lead and manage a team of data engineers and analytics professionals, providing strategic direction, mentorship, train, coach, performance reviews, and hands-on support to foster a collaborative, high-performing team environment focused on delivering impactful data solutions.\n• Establish clear objectives and key results (OKRs) for the team that align with the enterprise analytics and data strategy, ensuring close coordination with business goals and evolving priorities.\n• Conduct regular performance reviews, deliver constructive feedback, and champion continuous learning by identifying training, upskilling, and development opportunities for team members.\n• Partner with cross-functional teams, including business analysts, data scientists, IT, and key business stakeholders, to understand data requirements and deliver solutions that support business intelligence, operational reporting, and advanced analytics use cases.\n• Define and implement best practices in data engineering, covering data modeling, pipeline orchestration (ETL/ELT), data integration, and data quality, while ensuring reliable access to data from diverse sources such as databases, APIs, cloud platforms, and IoT systems.\n• Drive continuous improvement and innovation in data architecture and engineering techniques, with a focus on increasing scalability, performance, and reusability across the analytics ecosystem.\n• Oversee project planning and execution, balancing team capacity with priority management, delivery timelines, and quality standards to ensure successful outcomes for strategic and operational analytics initiatives.\n• Collaborate with cross-functional business and IT stakeholders to define and implement data and analytics strategies that support enterprise decision-making, ensuring data integrity, security, and governance across all analytical initiatives.\n• Provide technical leadership in data engineering and analytics infrastructure, leveraging modern cloud-native tools to enable high-performance data platforms that support BI, reporting, and advanced analytics.\n• Champion a culture of data innovation and continuous learning, encouraging the team to explore new data patterns, tooling, and architectural practices that improve scalability, reliability, and time-to-insight.\n• Oversee resource planning, vendor coordination, and budget management for the analytics engineering function, ensuring alignment with strategic priorities and operational efficiency.\n• Lead the design and implementation of high-volume, scalable data pipelines, models, and data marts across Data Lakes and Data Warehouses to support reporting, dashboarding, and analytics workloads.\n• Translate technical and project outcomes into business context through executive-ready presentations and analytics steering committee briefings, facilitating alignment and decision-making.\n• Demonstrate proficiency in Microsoft Azure Cloud (preferred) and/or Amazon Web Services, particularly within data engineering and analytics service ecosystems (e.g., Azure Data Factory, Synapse, Databricks, Redshift).\n• Build and manage DevOps pipelines for CI/CD and analytics solution deployment, ensuring consistency, automation, and compliance with change control standards.\n• Serve as a proactive leader who delivers on commitments and identifies opportunities for process improvement, team development, and organizational data literacy.\n• Coordinate and collaborate with onsite and offshore consultants to support scalable delivery and knowledge transfer across global analytics initiatives.\n• Maintain flexibility and responsiveness to evolving business needs; this job description may be adjusted to reflect emerging priorities or strategic direction.\n\nPlease note that this job description is not designed to contain a comprehensive list of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without prior notice.\n\nAdditionally, the Advanced Analytics Manager is expected to demonstrate:\n• Excellent communication, leadership and collaboration skills\n• Possesses solid project management skills\n• Advanced decision making and problem-solving skills\n• Ability to guide technical projects successfully from inception to completion\n• Experienced with Microsoft Word, Excel, PowerPoint, Visio and Outlook\n• Team Work - balances team and individual responsibilities; gives and welcomes feedback; contributes to building a positive team spirit; puts success of team above own interests; able to build morale and group commitments to goals and objectives\n• Adaptability - adapts to changes in the work environment; able to deal with frequent change, delays, or unexpected events\n• Planning/Organizing - uses time efficiently; plans for additional resources; sets goals and objectives; organizes or schedules other people and their tasks\n• Project Management - coordinates projects; communicates changes and progress; completes projects on time; budget management; manages project team activities\n• Oral Communication - speaks clearly and persuasively in positive or negative situations; listens and gets clarification; responds well to questions\n• Excellent team player\n\nWork Experience\n• Required:\n• 8-10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities\n• 8-10 years – Experience in data modeling, ETL processes, and data warehousing\n• 8-10 years – Experience in building and managing enterprise scale data pipelines\n• 8-10 years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)\n• 8-10 years – Experience in DevOps, CI/CD pipelines\n• 8-10 years – Experience in Python/R, Spark/Kafka\n• 8-10 years – Experience with Azure Databricks/Data Factory or similar technology\n• 8-10 years – Experience in Project management\n• Preferred:\n• 10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities\n• 10+ years – Experience in data modeling, ETL processes, and data warehousing\n• 10+ years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)\n• 10+ years – Experience in DevOps, CI/CD pipelines\n• 10+ years – Experience in Python/R, Spark/Kafka\n• 10+ years – Experience with Azure Databricks/Data Factory or similar technology\n• 10+ years – Experience in Project management\n• 2 years – Experience with Large language Models and building Gen AI applications\n\nEducation\n• Minimum Required:\n• Bachelor's Degree in Computer Science or Engineering\n• Preferred:\n• Master's Degree in Computer Science or Engineering\n\nCertification/License:\n• Required: None Required\n• Preferred: Data Engineering/Cloud certifications/Gen AI\n\nTypical Compensation Range\nPay Rate Type: Salary\n\n$136,778.46 - $198,328.77 / Yearly\n\nBonus Target: 10% Annual\n\nBenefits\n\nhttps://careers.niagarawater.com/us/en/benefits\n\nAny employment agency, person or entity that submits a résumé into this career site or to a hiring manager does so with the understanding that the applicant's résumé will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.\n\nEmployment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit résumé to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit résumé into this career site to be eligible for placement fees.\nNiagara Plant Name\nCORP-MAIN",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Diamond Bar, CA",
    "job_city": "Diamond Bar",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.0009951,
    "job_longitude": -117.8112041,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4PMXCF0Zj3csVibCAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This position requires not only technical expertise but also strategic thinking and leadership skills to evolve data infrastructure, support analytics scalability, and drive a data-first culture",
        "Possesses solid project management skills",
        "Advanced decision making and problem-solving skills",
        "Ability to guide technical projects successfully from inception to completion",
        "Experienced with Microsoft Word, Excel, PowerPoint, Visio and Outlook",
        "Team Work - balances team and individual responsibilities; gives and welcomes feedback; contributes to building a positive team spirit; puts success of team above own interests; able to build morale and group commitments to goals and objectives",
        "Excellent team player",
        "8-10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities",
        "8-10 years – Experience in data modeling, ETL processes, and data warehousing",
        "8-10 years – Experience in building and managing enterprise scale data pipelines",
        "8-10 years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)",
        "8-10 years – Experience in DevOps, CI/CD pipelines",
        "8-10 years – Experience in Python/R, Spark/Kafka",
        "8-10 years – Experience with Azure Databricks/Data Factory or similar technology",
        "8-10 years – Experience in Project management",
        "10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities",
        "10+ years – Experience in data modeling, ETL processes, and data warehousing",
        "10+ years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)",
        "10+ years – Experience in DevOps, CI/CD pipelines",
        "10+ years – Experience in Python/R, Spark/Kafka",
        "10+ years – Experience with Azure Databricks/Data Factory or similar technology",
        "10+ years – Experience in Project management",
        "2 years – Experience with Large language Models and building Gen AI applications",
        "Bachelor's Degree in Computer Science or Engineering",
        "Master's Degree in Computer Science or Engineering",
        "Required: None Required"
      ],
      "Benefits": [
        "Work in an entrepreneurial and dynamic environment with a chance to make an impact",
        "Develop lasting relationships with great people",
        "Have the opportunity to build a satisfying career",
        "We offer competitive compensation and benefits packages for our Team Members",
        "Typical Compensation Range",
        "Pay Rate Type: Salary",
        "$136,778.46 - $198,328.77 / Yearly",
        "Bonus Target: 10% Annual"
      ],
      "Responsibilities": [
        "This role leads a team of data engineers and analytics professionals, focusing on the design, implementation, and optimization of scalable, reliable, and secure data pipelines, especially for complex, high-volume sources such as IoT and sensor-based systems",
        "Working cross-functionally with operations, IT, and business stakeholders, the Advanced Analytics Manager ensures that data from diverse sources, including real-time telemetry, manufacturing systems, and traditional enterprise platforms are efficiently ingested, transformed, and made accessible for analytical consumption",
        "In addition, the role includes light but growing exposure to Generative AI use cases, such as document summarization, chat interfaces for data access, and large language model (LLM) integration—especially in scenarios that augment data accessibility and user experience across the organization",
        "A proven ability to lead high-performing teams and deliver impactful solutions in dynamic, data-rich environments is essential for success",
        "Lead and manage a team of data engineers and analytics professionals, providing strategic direction, mentorship, train, coach, performance reviews, and hands-on support to foster a collaborative, high-performing team environment focused on delivering impactful data solutions",
        "Establish clear objectives and key results (OKRs) for the team that align with the enterprise analytics and data strategy, ensuring close coordination with business goals and evolving priorities",
        "Conduct regular performance reviews, deliver constructive feedback, and champion continuous learning by identifying training, upskilling, and development opportunities for team members",
        "Partner with cross-functional teams, including business analysts, data scientists, IT, and key business stakeholders, to understand data requirements and deliver solutions that support business intelligence, operational reporting, and advanced analytics use cases",
        "Define and implement best practices in data engineering, covering data modeling, pipeline orchestration (ETL/ELT), data integration, and data quality, while ensuring reliable access to data from diverse sources such as databases, APIs, cloud platforms, and IoT systems",
        "Drive continuous improvement and innovation in data architecture and engineering techniques, with a focus on increasing scalability, performance, and reusability across the analytics ecosystem",
        "Oversee project planning and execution, balancing team capacity with priority management, delivery timelines, and quality standards to ensure successful outcomes for strategic and operational analytics initiatives",
        "Collaborate with cross-functional business and IT stakeholders to define and implement data and analytics strategies that support enterprise decision-making, ensuring data integrity, security, and governance across all analytical initiatives",
        "Provide technical leadership in data engineering and analytics infrastructure, leveraging modern cloud-native tools to enable high-performance data platforms that support BI, reporting, and advanced analytics",
        "Champion a culture of data innovation and continuous learning, encouraging the team to explore new data patterns, tooling, and architectural practices that improve scalability, reliability, and time-to-insight",
        "Oversee resource planning, vendor coordination, and budget management for the analytics engineering function, ensuring alignment with strategic priorities and operational efficiency",
        "Lead the design and implementation of high-volume, scalable data pipelines, models, and data marts across Data Lakes and Data Warehouses to support reporting, dashboarding, and analytics workloads",
        "Translate technical and project outcomes into business context through executive-ready presentations and analytics steering committee briefings, facilitating alignment and decision-making",
        "Build and manage DevOps pipelines for CI/CD and analytics solution deployment, ensuring consistency, automation, and compliance with change control standards",
        "Serve as a proactive leader who delivers on commitments and identifies opportunities for process improvement, team development, and organizational data literacy",
        "Coordinate and collaborate with onsite and offshore consultants to support scalable delivery and knowledge transfer across global analytics initiatives",
        "Maintain flexibility and responsiveness to evolving business needs; this job description may be adjusted to reflect emerging priorities or strategic direction",
        "Duties, responsibilities and activities may change at any time with or without prior notice",
        "Additionally, the Advanced Analytics Manager is expected to demonstrate:",
        "Excellent communication, leadership and collaboration skills",
        "Adaptability - adapts to changes in the work environment; able to deal with frequent change, delays, or unexpected events",
        "Planning/Organizing - uses time efficiently; plans for additional resources; sets goals and objectives; organizes or schedules other people and their tasks",
        "Project Management - coordinates projects; communicates changes and progress; completes projects on time; budget management; manages project team activities",
        "Oral Communication - speaks clearly and persuasively in positive or negative situations; listens and gets clarification; responds well to questions"
      ]
    },
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "careers-niagarawater-com-us-en-job-r45735-data-engineering-manager-advanced-analytics",
    "_source": "new_jobs"
  },
  {
    "job_id": "6fRKlkpblZ3M064nAAAAAA==",
    "job_title": "Staff Software Engineer, Data Platform",
    "employer_name": "Circle",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcREwaqjwdFFUeAPrw22pQIgjuO0Rm1gqtjzhB3v&s=0",
    "employer_website": "https://www.circle.com",
    "job_publisher": "Circle",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.circle.com/us/en/job/CIICIRUSJR100826EXTERNALENUS/Staff-Software-Engineer-Data-Platform?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Circle",
        "apply_link": "https://careers.circle.com/us/en/job/CIICIRUSJR100826EXTERNALENUS/Staff-Software-Engineer-Data-Platform?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Circle/Job/Staff-Software-Engineer,-Data-Platform/-in-Salt-Lake-City,UT?jid=ba5bde77d2dbe7d5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/8cc400d0c11615f647cb9ab3cb57e70e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/staff-software-engineer-data-platform--salt-lake-city--e10ee40eaf72cf92a0d704070c0ff44de?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Circle (NYSE: CRCL) is one of the world’s leading internet financial platform companies, building the foundation of a more open, global economy through digital assets, payment applications, and programmable blockchain infrastructure. Circle’s platform includes the world’s largest regulated stablecoin network anchored by USDC, Circle Payments Network for global money movement, and Arc, an enterprise-grade blockchain designed to become the Economic OS for the internet. Enterprises, financial institutions, and developers use Circle to power trusted, internet-scale financial innovation. Learn more at circle.com.\n\nWhat you’ll be part of:\n\nCircle is committed to visibility and stability in everything we do. As we grow as an organization, we're expanding into some of the world's strongest jurisdictions. Speed and efficiency are motivators for our success and our employees live by our company values: High Integrity, Future Forward, Multistakeholder, Mindful, and Driven by Excellence. We have built a flexible work environment where new ideas are encouraged and everyone is a stakeholder.\n\nHere is our team hierarchy for individual contributors:\n\nSenior Software Engineer (III)\n\nStaff Software Engineer (IV)\n\nYour team is responsible for:\n\nAs a member of the Data Platform Engineering team, you own the core Data warehouse platform, data ingestion and processing, ETL/ELT pipelines orchestration platform, data cataloging, data governance. These components power our Product, Engineering, Analytics, and Data Science teams by enabling experimentation, operational excellence, and actionable insights to accelerate business growth.\n\nYou'll work on:\n• Design, build, and operate data platform services (warehousing, orchestration, and catalogs). Continuously enhance platform operations by improving monitoring, performance, reliability, and resource optimization.\n• Design, build and maintain batch and streaming data ingestion framework to source the required data for analytical and operational needs, which include onchain data, internal system data, and partner data.\n• Be a domain expert in streaming processing, data pipelines, data warehousing and quality. Work closely across multiple stakeholders–including Product, Engineering, Data Science, Security and Compliance teams–on data contract modeling, data lifecycle management, governance and regulatory/legal compliance.\n• Provide ML data platform capabilities for AI/Data Science teams to perform data preparation, model preparation and serving, and performance monitoring.\n• Develop and maintain core services and libraries to enhance critical platform functionalities, such as cataloging data assets and lineage, tracking data versioning and quality, managing auto-backfilling, implementing access controls on data assets.\n\nYou'll bring to Circle:\n\nSenior Software Engineer (III):\n• 4+ years of software engineering experience building data-intensive systems\n• Hands-on experience designing and operating scalable batch, micro-batch, or streaming data pipelines\n• Experience in business domains such as payment systems, credit cards, bank transfers, or blockchains.\n• Familiarity with data governance, lineage, and provenance concepts\n• Strong understanding of open-source data technologies and cloud-native data platforms\n• Ability to tackle complex and ambiguous problems.\n• Self-starter who takes ownership and enjoys moving at a fast pace.\n• Excellent communication skills, with the ability to collaborate across multiple remote teams, share ideas and present concepts effectively.\n\nNice to have:\n• Experience with with streaming frameworks such as Apache Flink or Google Cloud Dataflow\n• Experience with NoSQL databases such as Bigtable, Cassandra\n\n​\n\nStaff Software Engineer (IV):\n\nIncludes all the requirements of a Senior Software Engineer, and:\n• 7+ years in software engineering experience for large-scale and complex data systems\n• Proven technical leadership in architecture and system design, influencing designs across multiple teams\n• Deep expertise in one or more of: streaming systems, data warehousing, data modeling, or large-scale ingestion platforms\n• Ability to identify high-impact technical opportunities independently and drive them from concept to production\n• Strong experience in:\n• Data platforms integrated with downstream consumers, tools, and services\n• Data quality, validation, and observability mechanisms across pipelines\n• Comfortable making and defending long-term architectural tradeoffs in ambiguous environments\n\nNice to have:\n• Hands-on Experience taking an operational, data-intensive application from initial design to production (0→1), or scaling and operating it at production scale.\n• Experience developing real-time analytics or near-real-time decisioning systems\n\nCircle is on a mission to create an inclusive financial future, with transparency at our core. We consider a wide variety of elements when crafting our compensation ranges and total compensation packages.\n\nStarting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs. Please note that compensation ranges may differ for candidates in other locations.\n\nBase Pay Range: $195,000 - $257,500\n\nWe are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status, or any other protected status required by the laws in the locations where we hire. Additionally, Circle participates in the E-Verify Program in certain locations, as required by law.\n\nShould you require accommodations or assistance in our interview process because of a disability, please reach out to accommodations@circle.com for support. We respect your privacy and will connect with you separately from our interview process to accommodate your needs.\n\n#LI-Remote",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Washington, DC",
    "job_city": "Washington",
    "job_state": "District of Columbia",
    "job_country": "US",
    "job_latitude": 38.9072873,
    "job_longitude": -77.0369274,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D6fRKlkpblZ3M064nAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "4+ years of software engineering experience building data-intensive systems",
        "Hands-on experience designing and operating scalable batch, micro-batch, or streaming data pipelines",
        "Experience in business domains such as payment systems, credit cards, bank transfers, or blockchains",
        "Familiarity with data governance, lineage, and provenance concepts",
        "Strong understanding of open-source data technologies and cloud-native data platforms",
        "Ability to tackle complex and ambiguous problems",
        "Self-starter who takes ownership and enjoys moving at a fast pace",
        "Excellent communication skills, with the ability to collaborate across multiple remote teams, share ideas and present concepts effectively",
        "Experience with with streaming frameworks such as Apache Flink or Google Cloud Dataflow",
        "Experience with NoSQL databases such as Bigtable, Cassandra",
        "Includes all the requirements of a Senior Software Engineer, and:",
        "7+ years in software engineering experience for large-scale and complex data systems",
        "Proven technical leadership in architecture and system design, influencing designs across multiple teams",
        "Deep expertise in one or more of: streaming systems, data warehousing, data modeling, or large-scale ingestion platforms",
        "Ability to identify high-impact technical opportunities independently and drive them from concept to production",
        "Data platforms integrated with downstream consumers, tools, and services",
        "Data quality, validation, and observability mechanisms across pipelines",
        "Comfortable making and defending long-term architectural tradeoffs in ambiguous environments",
        "Hands-on Experience taking an operational, data-intensive application from initial design to production (0→1), or scaling and operating it at production scale",
        "Experience developing real-time analytics or near-real-time decisioning systems"
      ],
      "Benefits": [
        "We consider a wide variety of elements when crafting our compensation ranges and total compensation packages",
        "Starting pay is determined by various factors, including but not limited to: relevant experience, skill set, qualifications, and other business and organizational needs",
        "Base Pay Range: $195,000 - $257,500"
      ],
      "Responsibilities": [
        "As a member of the Data Platform Engineering team, you own the core Data warehouse platform, data ingestion and processing, ETL/ELT pipelines orchestration platform, data cataloging, data governance",
        "Design, build, and operate data platform services (warehousing, orchestration, and catalogs)",
        "Continuously enhance platform operations by improving monitoring, performance, reliability, and resource optimization",
        "Design, build and maintain batch and streaming data ingestion framework to source the required data for analytical and operational needs, which include onchain data, internal system data, and partner data",
        "Be a domain expert in streaming processing, data pipelines, data warehousing and quality",
        "Work closely across multiple stakeholders–including Product, Engineering, Data Science, Security and Compliance teams–on data contract modeling, data lifecycle management, governance and regulatory/legal compliance",
        "Provide ML data platform capabilities for AI/Data Science teams to perform data preparation, model preparation and serving, and performance monitoring",
        "Develop and maintain core services and libraries to enhance critical platform functionalities, such as cataloging data assets and lineage, tracking data versioning and quality, managing auto-backfilling, implementing access controls on data assets",
        "You'll bring to Circle:"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-circle-com-us-en-job-ciicirusjr100826externalenus-staff-software-engineer-data-platform",
    "_source": "new_jobs"
  },
  {
    "job_id": "4GHeUqb67eilko0QAAAAAA==",
    "job_title": "Simulation Engineer- Supply Chain",
    "employer_name": "Cencora",
    "employer_logo": null,
    "employer_website": "https://www.cencora.com",
    "job_publisher": "Cencora Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.cencora.com/us/en/job/R2523242/Simulation-Engineer-Supply-Chain?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Cencora Careers",
        "apply_link": "https://careers.cencora.com/us/en/job/R2523242/Simulation-Engineer-Supply-Chain?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/cCMwXHk6HcfhsFhWMSOkJvsMbRYzJa1VoQehAqbdWDeno7Y_Oz0JNQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/8a691ec6b6682787516e82bdcee83da8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nWe are seeking a talented and experienced Supply Chain Simulation Engineer to lead and collaborate on the design, development, and deployment of tools and strategies that will enable more effective data-driven decision making.\n\nThis individual will provide guidance through advanced analytics modeling and optimizing facilities within the Supply Chain.\n\nThe Simulation Engineer should have a firm understanding of distribution operational and automation processes.\n\nThe ideal candidate will be a skilled communicator with experience telling compelling stories through data-driven insights.\n\nRoles and Responsibilities:\n• Create simulation models of our distribution centers that will help determine efficiency opportunities.\n• Understand and have experience with design and / or implementation of automation solutions such as conveyors, sorters, high speed picking and packaging lines, robotic work cells.\n• Work closely with other teams, such as Engineering and Operations, in the validation of the models and in the design of potential scenarios that will improve the total outcome.\n• Collaborate in developing a cost to serve model.\n• Implement and support the new inhouse simulation capabilities – process, tools, training.\n• Collaborate in the creation of business cases to close gaps and define capex requirements.\n• Provide advice and insights on improving existing simulation capabilities.\n• Design interactive business intelligence dashboards to share input and outputs.\n• Develop and train team to utilize self-service modeling functionality.\n• Stay up to date with modeling and simulations trends.\n• Perform other duties as assigned.\n\nEducation:\n• A Bachelors degree in Industrial Engineering, Operations Research, Quantitative Methods, Data Science or related field or equivalent professional qualification/workplace skills.\n• Master’s degree preferred.\n\nExperience:\n• Requires five (5) to seven (7) years of directly related and progressively responsible experience.\n• Hands-on experience using simulation and analytical tools to solve operational problems.\n• Practical experience with Simulation frameworks (discrete-event, agent-based) and technologies such as AnyLogic/Arena/EnterpriseDynamics/FlexSim/etc\n• Project Management / Agile hands-on experience.\n• Experience working in an Operations environment.\n• Experience with business intelligence tools (PowerBI, Tableau, Qlik).\n• Experience pulling and cleaning data from Cloud Platforms (Google, Azure), or Supply Chain Tools (including ERP, WMS, TMS, Inventory & Forecasting Tools).\n• Experience programming with data analytic tools such as SQL, Python, DataBricks, etc.\n• Experience transforming CAD data into simulation tools.\n• Experience building business cases and cost/benefit analysis to support strategic decision making.\n\nSkills and Abilities:\n• Strong analytical and critical thinking skills, with the ability to identify and resolve complex issues quickly and innovative. Experience leveraging analytic based approaches to problem solving, such as Statistical Process Control, Design of Experiments, Six Sigma, or other statistical analysis frameworks.\n• Team oriented and collaborative working style.\n• Growth mindset, positive attitude and strong interest in solving business challenges and adapting to a changing work environment.\n• Ability to communicate effectively both orally and in writing; ability to communicate (and work) effectively with people from different technical and business backgrounds, acting as a liaison, understanding, and appreciating different perspectives and translating into terms necessary for any group or individual to understand.\n• Presentation skills: ability to present and discuss technical information in a manner that establishes rapport, persuades others, and establishes understanding for technical and non-technical audiences.\n• Strong organizational skills; attention to detail.\n• Must be able to interact well with others both directly (face-to-face) and remotely within a multi-discipline shared work environment.\n• Ability to influence through the use of data and insights while showing tactful discretion with difficult/sensitive information.\n• Travel required per project, estimated at 2 weeks per quarter.\n• Ability to manage multiple projects\n\nWhat Cencora offers\n\nWe provide compensation, benefits, and resources that enable a highly inclusive culture and support our team members’ ability to live with purpose every day. In addition to traditional offerings like medical, dental, and vision care, we also provide a comprehensive suite of benefits that focus on the physical, emotional, financial, and social aspects of wellness. This encompasses support for working families, which may include backup dependent care, adoption assistance, infertility coverage, family building support, behavioral health solutions, paid parental leave, and paid caregiver leave. To encourage your personal growth, we also offer a variety of training programs, professional development resources, and opportunities to participate in mentorship programs, employee resource groups, volunteer activities, and much more. For details, visit https://www.virtualfairhub.com/cencora\n\nFull time\n\nSalary Range*\n\n$88,700 - 126,940\n• This Salary Range reflects a National Average for this job. The actual range may vary based on your locale. Ranges in Colorado/California/Washington/New York/Hawaii/Vermont/Minnesota/Massachusetts/Illinois State-specific locations may be up to 10% lower than the minimum salary range, and 12% higher than the maximum salary range.\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned\n\n.\nAffiliated Companies:\nAffiliated Companies: AmerisourceBergen Drug Corporation",
    "job_is_remote": false,
    "job_posted_at": "10 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "W CNSHOHOCKEN, PA",
    "job_city": "Conshohocken",
    "job_state": "Pennsylvania",
    "job_country": "US",
    "job_latitude": 40.0792766,
    "job_longitude": -75.3015714,
    "job_benefits": [
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4GHeUqb67eilko0QAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will be a skilled communicator with experience telling compelling stories through data-driven insights",
        "A Bachelors degree in Industrial Engineering, Operations Research, Quantitative Methods, Data Science or related field or equivalent professional qualification/workplace skills",
        "Requires five (5) to seven (7) years of directly related and progressively responsible experience",
        "Hands-on experience using simulation and analytical tools to solve operational problems",
        "Practical experience with Simulation frameworks (discrete-event, agent-based) and technologies such as AnyLogic/Arena/EnterpriseDynamics/FlexSim/etc",
        "Project Management / Agile hands-on experience",
        "Experience working in an Operations environment",
        "Experience with business intelligence tools (PowerBI, Tableau, Qlik)",
        "Experience pulling and cleaning data from Cloud Platforms (Google, Azure), or Supply Chain Tools (including ERP, WMS, TMS, Inventory & Forecasting Tools)",
        "Experience programming with data analytic tools such as SQL, Python, DataBricks, etc",
        "Experience transforming CAD data into simulation tools",
        "Experience building business cases and cost/benefit analysis to support strategic decision making",
        "Strong analytical and critical thinking skills, with the ability to identify and resolve complex issues quickly and innovative",
        "Experience leveraging analytic based approaches to problem solving, such as Statistical Process Control, Design of Experiments, Six Sigma, or other statistical analysis frameworks",
        "Growth mindset, positive attitude and strong interest in solving business challenges and adapting to a changing work environment",
        "Ability to communicate effectively both orally and in writing; ability to communicate (and work) effectively with people from different technical and business backgrounds, acting as a liaison, understanding, and appreciating different perspectives and translating into terms necessary for any group or individual to understand",
        "Presentation skills: ability to present and discuss technical information in a manner that establishes rapport, persuades others, and establishes understanding for technical and non-technical audiences",
        "Strong organizational skills; attention to detail",
        "Must be able to interact well with others both directly (face-to-face) and remotely within a multi-discipline shared work environment"
      ],
      "Benefits": [
        "We provide compensation, benefits, and resources that enable a highly inclusive culture and support our team members’ ability to live with purpose every day",
        "In addition to traditional offerings like medical, dental, and vision care, we also provide a comprehensive suite of benefits that focus on the physical, emotional, financial, and social aspects of wellness",
        "This encompasses support for working families, which may include backup dependent care, adoption assistance, infertility coverage, family building support, behavioral health solutions, paid parental leave, and paid caregiver leave",
        "To encourage your personal growth, we also offer a variety of training programs, professional development resources, and opportunities to participate in mentorship programs, employee resource groups, volunteer activities, and much more",
        "Salary Range*",
        "$88,700 - 126,940",
        "This Salary Range reflects a National Average for this job"
      ],
      "Responsibilities": [
        "This individual will provide guidance through advanced analytics modeling and optimizing facilities within the Supply Chain",
        "The Simulation Engineer should have a firm understanding of distribution operational and automation processes",
        "Create simulation models of our distribution centers that will help determine efficiency opportunities",
        "Understand and have experience with design and / or implementation of automation solutions such as conveyors, sorters, high speed picking and packaging lines, robotic work cells",
        "Work closely with other teams, such as Engineering and Operations, in the validation of the models and in the design of potential scenarios that will improve the total outcome",
        "Collaborate in developing a cost to serve model",
        "Implement and support the new inhouse simulation capabilities – process, tools, training",
        "Collaborate in the creation of business cases to close gaps and define capex requirements",
        "Provide advice and insights on improving existing simulation capabilities",
        "Design interactive business intelligence dashboards to share input and outputs",
        "Develop and train team to utilize self-service modeling functionality",
        "Stay up to date with modeling and simulations trends",
        "Perform other duties as assigned",
        "Team oriented and collaborative working style",
        "Ability to influence through the use of data and insights while showing tactful discretion with difficult/sensitive information",
        "Travel required per project, estimated at 2 weeks per quarter",
        "Ability to manage multiple projects"
      ]
    },
    "job_onet_soc": "13108100",
    "job_onet_job_zone": "4",
    "id": "careers-cencora-com-us-en-job-r2523242-simulation-engineer-supply-chain",
    "_source": "new_jobs"
  },
  {
    "job_id": "_6T7FdFfKx98hoI6AAAAAA==",
    "job_title": "Data Engineer - On-Site [see locations]",
    "employer_name": "Regions",
    "employer_logo": null,
    "employer_website": "https://www.regions.com",
    "job_publisher": "Careers At Regions - Regions Bank",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.regions.com/us/en/job/R99979/Data-Engineer-On-Site-see-locations?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At Regions - Regions Bank",
        "apply_link": "https://careers.regions.com/us/en/job/R99979/Data-Engineer-On-Site-see-locations?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/N5LKNSOHEwFywoR8pHtGAJHSrdWDFFUwwWBqS7eFQ9YYmncTcygY0A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs For Stevenage Fans",
        "apply_link": "https://jobs.stevenagefc.com/jobs/data-engineer-observability-tooling-charlotte-north-carolina/2594802627-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/1b1e55ba04bfe2d2de1c7bf4ab7eb7a5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-on-site-see-locations-at-regions-bank-4371687945?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SaluteMyJob",
        "apply_link": "https://salutemyjob.com/jobs/data-engineer-observability-tooling-charlotte-north-carolina/2594802627-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Nexxt",
        "apply_link": "https://www.nexxt.com/jobs/data-engineer-on-site-see-locations-charlotte-nc-3162540523-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "KGET Jobs",
        "apply_link": "https://jobs.kget.com/jobs/data-engineer-observability-tooling-charlotte-north-carolina/2594802627-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Thank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.\n\nAt Regions, the Data Engineer focuses on the evaluation, design, and execution of data structures, processes, and logic to deliver business value through operational and analytical data assets. The Data Engineer uses advanced data design and technical skills to work with business subject matter experts to create enterprise data assets utilizing state of the art data techniques and tools.\n\nPrimary Responsibilities\n• Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases\n• Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment\n• Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark\n• Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores\n• Ensures data is prepared, arranged and ready for each defined business use case\n• Designs and deploys frameworks and micro services to serve data assets to data consumers\n• Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity\n• Provides consultation to all areas of the organization that plan to use data to make decisions\n• Supports any team members in the development of such information delivery and aid in the automation of data products\n• Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy\n\nThis position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.\n\nRequirements\n• Bachelor's degree and six (6) years of experience in a quantitative/analytical/STEM field or technical related field\n• Or Master’s degree and four (4) years of experience in a quantitative/analytical/STEM field or technical related field\n• Or Ph.D. and two (2) years of experience in a quantitative/analytical/STEM field\n• Three (3) years of working programming experience in Python/PySpark, Scala, SQL\n• Three (3) years of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka\n\nPreferences\n• Background in Big Data Engineering and Advanced Data Analytics\n• Experience developing solutions for the financial services industry\n• Experience in Agile Software Development\n• Experience or exposure to cloud technologies and migrations\n• Prior banking or financial services experience\n\nSkills and Competencies\n• Experience building data solutions at scale\n• Experience designing and building relational data structures in multiple environments\n• Experience with DevOps principals, CI/CD, and Software Development Lifecycle\n• Experience with No-SQL databases\n• Experience with large-scale data Lakehouses at Enterprise scale\n• Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets\n• Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making\n• Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks\n• Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment\n• Strong technical background including database and business intelligence skills\n• Strong communication skills through written and oral presentations\n\nAdditional Job Description\n\nMust-Have Qualifications\n• Strong experience with SQL databases, including development and support of SSIS/SSRS packages\n• Hands-on experience with PostgreSQL in a production environment\n• Proven ability to develop, manage, and maintain ETL pipelines\n• Working experience in AWS cloud environments\n• Experience using Python for data processing and automation\n• Required experience with Apache Kafka for data streaming\n• Required experience with Snowflake for cloud data warehousing\n\nThis position is intended to be onsite, now or in the near future. Associates will have regular work hours, including full days in the office three or more days a week. The manager will set the work schedule for this position, including in-office expectations. Regions will not provide relocation assistance for this position, and relocation would be at your expense. This position must be within a reasonable driving distance to a Branch, Consumer Operations, or Professional Office Building with the primary locations being for Birmingham, AL, Atlanta, GA or Charlotte, NC. Exceptions to the geographic location requirement may be made for current Regions associates who work remotely.\n\nThis position may be filled at a higher level depending on the candidate's qualifications and relevant experience.\n\nRegions will not sponsor applicants for work visas for this position at this time. Applicants for this position must currently be authorized to work in the United States on a full-time basis.\n\nCompensation Details\n\nPay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.\n\nThe target information listed below is based on the Metropolitan Statistical Area Market Range for where the position is located and level of the position.\n\nJob Range Target:\n\nMinimum:\n$109,525.90 USD\n\n50th Percentile:\n$139,430.00 USD\n\nIncentive Pay Plans:\nOpportunity to participate in the Long Term Incentive Plan.\n\nLocation:\nHoover, Alabama\n\nEqual Opportunity Employer/including Disabled/Veterans\n\nJob applications at Regions are accepted electronically through our career site for a minimum of five business days from the date of posting. Job postings for higher-volume positions may remain active for longer than the minimum period due to business need and may be closed at any time thereafter at the discretion of the company.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Charlotte, NC",
    "job_city": "Charlotte",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.2270768,
    "job_longitude": -80.84089329999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_6T7FdFfKx98hoI6AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree and six (6) years of experience in a quantitative/analytical/STEM field or technical related field",
        "Or Master’s degree and four (4) years of experience in a quantitative/analytical/STEM field or technical related field",
        "Or Ph.D. and two (2) years of experience in a quantitative/analytical/STEM field",
        "Three (3) years of working programming experience in Python/PySpark, Scala, SQL",
        "Three (3) years of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka",
        "Background in Big Data Engineering and Advanced Data Analytics",
        "Experience developing solutions for the financial services industry",
        "Experience in Agile Software Development",
        "Experience or exposure to cloud technologies and migrations",
        "Prior banking or financial services experience",
        "Experience building data solutions at scale",
        "Experience designing and building relational data structures in multiple environments",
        "Experience with DevOps principals, CI/CD, and Software Development Lifecycle",
        "Experience with No-SQL databases",
        "Experience with large-scale data Lakehouses at Enterprise scale",
        "Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets",
        "Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making",
        "Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks",
        "Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment",
        "Strong technical background including database and business intelligence skills",
        "Strong communication skills through written and oral presentations",
        "Strong experience with SQL databases, including development and support of SSIS/SSRS packages",
        "Hands-on experience with PostgreSQL in a production environment",
        "Proven ability to develop, manage, and maintain ETL pipelines",
        "Working experience in AWS cloud environments",
        "Experience using Python for data processing and automation",
        "Required experience with Apache Kafka for data streaming",
        "Required experience with Snowflake for cloud data warehousing",
        "Exceptions to the geographic location requirement may be made for current Regions associates who work remotely"
      ],
      "Benefits": [
        "Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions",
        "Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role",
        "As these factors vary by individuals, pay will also vary among individual associates within the same job",
        "$109,525.90 USD",
        "$139,430.00 USD",
        "Incentive Pay Plans:",
        "Opportunity to participate in the Long Term Incentive Plan"
      ],
      "Responsibilities": [
        "The Data Engineer uses advanced data design and technical skills to work with business subject matter experts to create enterprise data assets utilizing state of the art data techniques and tools",
        "Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases",
        "Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment",
        "Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark",
        "Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores",
        "Ensures data is prepared, arranged and ready for each defined business use case",
        "Designs and deploys frameworks and micro services to serve data assets to data consumers",
        "Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity",
        "Provides consultation to all areas of the organization that plan to use data to make decisions",
        "Supports any team members in the development of such information delivery and aid in the automation of data products",
        "Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy",
        "This position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay",
        "Associates will have regular work hours, including full days in the office three or more days a week",
        "The manager will set the work schedule for this position, including in-office expectations",
        "This position must be within a reasonable driving distance to a Branch, Consumer Operations, or Professional Office Building with the primary locations being for Birmingham, AL, Atlanta, GA or Charlotte, NC"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-regions-com-us-en-job-r99979-data-engineer-on-site-see-locations",
    "_source": "new_jobs"
  },
  {
    "job_id": "5DdsccHWVhuOvi7ZAAAAAA==",
    "job_title": "Senior Data Analytics Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=5344040d69e5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=5344040d69e5&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Senior Data Analytics Engineer to join their Data Solutions team.\n\nKey Responsibilities\n\nLead data analytics projects to build innovative solutions while ensuring adherence to budget, schedule, and scope\n\nMentor members in the Analytics Center of Excellence and assist with data catalog and visualization software oversight\n\nDrive data and analytics solutions from conception to deployment with clear ROI impact\n\nRequired Qualifications\n\nBachelor's degree in computer or information science preferred or relevant experience\n\n5+ years of relevant experience in a data-driven professional setting\n\nStrong command of databases and SQL, with proficiency in Python or R for data manipulation and analysis\n\nThorough understanding of project management principles and information technology practices\n\nStrong data analytics background, including experience with SQL, Snowflake Database, Azure cloud, and data visualization tools",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Hartford, CT",
    "job_city": "Hartford",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.7658043,
    "job_longitude": -72.6733723,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D5DdsccHWVhuOvi7ZAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of relevant experience in a data-driven professional setting",
        "Strong command of databases and SQL, with proficiency in Python or R for data manipulation and analysis",
        "Thorough understanding of project management principles and information technology practices",
        "Strong data analytics background, including experience with SQL, Snowflake Database, Azure cloud, and data visualization tools"
      ],
      "Responsibilities": [
        "Lead data analytics projects to build innovative solutions while ensuring adherence to budget, schedule, and scope",
        "Mentor members in the Analytics Center of Excellence and assist with data catalog and visualization software oversight",
        "Drive data and analytics solutions from conception to deployment with clear ROI impact"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "zwAG5QQq-v9l2VvrAAAAAA==",
    "job_title": "Data Analytics Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=23eaffa71bc9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=23eaffa71bc9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Data / Analytics Engineer to support the VHA Veteran Family Member Program Modernization initiative.\n\nKey Responsibilities\n\nBuild and maintain data pipelines and workflows\n\nSupport analytics, modeling, and visualization efforts\n\nEnsure data integrity, security, and alignment with VA architecture standards\n\nRequired Qualifications\n\nBachelor's degree\n\n5+ years in analytics, data engineering, and visualization tools such as Azure and Databricks\n\nAbility to work without sponsorship in the US indefinitely",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Lake Charles, LA",
    "job_city": "Lake Charles",
    "job_state": "Louisiana",
    "job_country": "US",
    "job_latitude": 30.2265949,
    "job_longitude": -93.2173758,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzwAG5QQq-v9l2VvrAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree",
        "5+ years in analytics, data engineering, and visualization tools such as Azure and Databricks",
        "Ability to work without sponsorship in the US indefinitely"
      ],
      "Responsibilities": [
        "Build and maintain data pipelines and workflows",
        "Support analytics, modeling, and visualization efforts",
        "Ensure data integrity, security, and alignment with VA architecture standards"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "gM1jOo1uqpB0p6BnAAAAAA==",
    "job_title": "Senior Data Engineer - Omni Channel",
    "employer_name": "Discount Tire",
    "employer_logo": null,
    "employer_website": "https://www.discounttire.com",
    "job_publisher": "Discount Tire Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.discounttire.com/job/senior-data-engineer-omni-channel/scottsdale-AZ/16110/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Discount Tire Careers",
        "apply_link": "https://careers.discounttire.com/job/senior-data-engineer-omni-channel/scottsdale-AZ/16110/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "America's Tire Careers",
        "apply_link": "https://careers.americastire.com/job/senior-data-engineer-omni-channel/scottsdale-AZ/16110/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-data-engineer-omni-channel-scottsdale-az--669065a5-8365-497f-bb28-aab2a7cf1f2a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/senior-data-engineer-omni-channel-at-discount-tire-4368062612?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/69813b95abff4c399aa02339?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/senior-data-engineer-omni-channel--scottsdale--eaae1fcba41ae62fc3560895f30baab79?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido",
        "apply_link": "https://us.jobrapido.com/jobpreview/7509145751016964096?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/scottsdale/arizona/info_technology/4846873868/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Overview:\n\nHere at Discount Tire, we celebrate the spirit of our people with extraordinary pride and enthusiasm. Our business has been growing for more than 60 years and now is the best time in our history to join us. We recognize that to remain the industry leader we must continue to grow and evolve our business in a rapidly changing industry. We are achieving this, not only by opening new stores, but by transforming our technological landscape and making data a central component of our strategy. The Business Analytics team, one of the fastest growing teams in the company, is leading this change. We are responsible for driving the insights, recommendations, and developing the decision support tools that influence the strategic direction of the company.\n\nUnder minimal supervision, the Senior Data Engineer designs, builds, and operates scalable data solutions on Discount Tire's Databricks-based lakehouse platform. This role supports enterprise analytics, business intelligence, and data science initiatives by delivering high-quality, governed, and performance data products across Omni-channel domains. The Senior Data Engineer is responsible for developing and maintaining ELT pipelines, data models, and lakehouse architecture using modern cloud and big data technologies, with a strong focus on reliability, performance, and production ownership.\n\nEssential Duties and Responsibilities:\n• Lakehouse & Data Architecture\n• Designs, develops, and maintains Databricks-based lakehouse solutions leveraging medallion architecture (Bronze, Silver, Gold) and Delta Lake.\n• Builds scalable, high-performance analytical data models to support reporting, analytics, and downstream consumption.\n• Ensures data solutions align with enterprise standards for security, governance, scalability, and cost efficiency.\n• Data Engineering & ELT Pipelines\n• Develops, tests, and maintains batch and streaming ELT pipelines using Spark SQL.\n• Implements incremental data processing, dependency management, and idempotent pipeline patterns across multiple data sources.\n• Integrates structured and semi-structured data from transactional, digital, and third-party systems into the lakehouse.\n• Databricks Platform Development\n• Develops and maintains Databricks notebooks, workflows, and jobs across development, QA, and production environments.\n• Leverages Delta Lake features including ACID transactions, schema enforcement and evolution, and time travel.\n• Partners with platform and DevOps teams to support environment promotion, scheduling, and deployment automation.\n• Data Quality & Reliability\n• Implements automated data quality checks, validation frameworks, and monitoring to ensure accuracy, completeness, and timeliness of data.\n• Proactively identifies and resolves data issues, ensuring reliable data availability for business partners.\n• Performance Optimization & Tuning\n• Troubleshoots data pipeline and query performance issues and implements optimizations related to Spark execution, partitioning, file sizing, and data layout.\n• Documents root cause analysis and corrective actions and provide recommendations to improve system performance and reliability.\n• Production Ownership & Operations\n• Owns data pipelines end-to-end in production, including monitoring, alerting, incident response, and operational support.\n• Contributes to runbooks, operational documentation, and continuous improvement of production readiness practices.\n• Programming & Standards\n• Writes clean, maintainable, and well-tested code using Python (PySpark) and SQL.\n• Contributes to data engineering standards, reusable frameworks, and best practices across the Analytics Engineering team.\n• Documentation & Reviews\n• Produces clear technical documentation, including design specifications and data flow diagrams.\n• Participates in design reviews and code reviews, providing constructive feedback to ensure quality and consistency.\n• Collaboration & Stakeholder Engagement\n• Collaborates closely with data engineers, analytics engineers, data scientists, digital analysts, and business stakeholders to translate business needs into scalable data solutions.\n• Works cross-functionally with project managers, platform teams, and external partners to deliver high-impact analytics capabilities.\n• Mentorship & Technical Leadership\n• Mentors and supports less experienced data engineers through technical guidance and best-practice sharing.\n• Leads technical design discussions for complex datasets and influences architectural decisions within the team.\n• Continuous Improvement\n• Stays current with emerging data engineering technologies, trends, and industry best practices.\n• Identifies opportunities to improve efficiency, reliability, and scalability across data engineering workflows.\n• Performs other duties as assigned.\n\nQualifications:\n• 5+ years of experience in data engineering, data integration, or analytics engineering roles.\n• Experience with cloud-based analytics platforms and modern data architecture.\n• Technical Skills\n• Advanced experience with SQL and Spark (PySpark and Spark SQL) in large-scale data environments.\n• Strong experience designing and maintaining ELT pipelines in a cloud-based lakehouse architecture.\n• Deep understanding of data modeling for analytics, including star schemas and slowly changing dimensions.\n• Experience with streaming or near-real-time data processing is preferred.\n• Strong knowledge of data governance, security, and compliance best practices.\n• Tools and Platforms\n• Extensive experience with Databricks and Delta Lake for analytics and data engineering workloads.\n• Experience with cloud object storage platforms such as AWS S3.\n• Familiarity with orchestration, scheduling, and monitoring of data pipelines.\n• Experience working with large, complex datasets in analytics and reporting contexts.\n• Familiarity with BI and visualization tools such as Tableau, Looker, or Power BI.\n• Experience with digital analytics data (e.g., Adobe Analytics or Google Analytics) is a plus.\n• Methodologies and Frameworks\n• Experience working within SDLC methodologies such as Agile and Waterfall.\n• Familiarity with CI/CD and version control practices for data engineering workflows.\n• Communication and Presentation Skills\n• Ability to clearly communicate complex technical concepts to technical and non-technical audiences, including leadership.\n• Strong written and verbal communication skills, including documentation and presentation of technical solutions.\n• Customer Service & Collaboration\n• Demonstrated ability to work collaboratively across teams and with business stakeholders.\n• Strong customer service mindset with a focus on delivering reliable and high-quality data solutions.\n• Adaptability & Problem Solving\n• Strong troubleshooting and analytical skills with a proactive, solution-oriented approach.\n• Ability to work effectively in a fast-paced, evolving environment and manage multiple priorities.\n• Curiosity\n• Intellectual curiosity and passion for data, analytics, and continuous learning.\n\nEducational Requirements:\n• Bachelor's or Master's degree in Computer Science, Data Engineering, Information Systems, or a related field, or equivalent practical experience.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1769990400,
    "job_posted_at_datetime_utc": "2026-02-02T00:00:00.000Z",
    "job_location": "Scottsdale, AZ",
    "job_city": "Scottsdale",
    "job_state": "Arizona",
    "job_country": "US",
    "job_latitude": 33.494876399999995,
    "job_longitude": -111.92167339999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DgM1jOo1uqpB0p6BnAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience in data engineering, data integration, or analytics engineering roles",
        "Experience with cloud-based analytics platforms and modern data architecture",
        "Technical Skills",
        "Advanced experience with SQL and Spark (PySpark and Spark SQL) in large-scale data environments",
        "Strong experience designing and maintaining ELT pipelines in a cloud-based lakehouse architecture",
        "Deep understanding of data modeling for analytics, including star schemas and slowly changing dimensions",
        "Strong knowledge of data governance, security, and compliance best practices",
        "Tools and Platforms",
        "Extensive experience with Databricks and Delta Lake for analytics and data engineering workloads",
        "Experience with cloud object storage platforms such as AWS S3",
        "Familiarity with orchestration, scheduling, and monitoring of data pipelines",
        "Experience working with large, complex datasets in analytics and reporting contexts",
        "Familiarity with BI and visualization tools such as Tableau, Looker, or Power BI",
        "Methodologies and Frameworks",
        "Experience working within SDLC methodologies such as Agile and Waterfall",
        "Familiarity with CI/CD and version control practices for data engineering workflows",
        "Communication and Presentation Skills",
        "Ability to clearly communicate complex technical concepts to technical and non-technical audiences, including leadership",
        "Strong written and verbal communication skills, including documentation and presentation of technical solutions",
        "Customer Service & Collaboration",
        "Demonstrated ability to work collaboratively across teams and with business stakeholders",
        "Strong customer service mindset with a focus on delivering reliable and high-quality data solutions",
        "Adaptability & Problem Solving",
        "Strong troubleshooting and analytical skills with a proactive, solution-oriented approach",
        "Ability to work effectively in a fast-paced, evolving environment and manage multiple priorities",
        "Curiosity",
        "Intellectual curiosity and passion for data, analytics, and continuous learning",
        "Bachelor's or Master's degree in Computer Science, Data Engineering, Information Systems, or a related field, or equivalent practical experience"
      ],
      "Responsibilities": [
        "Under minimal supervision, the Senior Data Engineer designs, builds, and operates scalable data solutions on Discount Tire's Databricks-based lakehouse platform",
        "The Senior Data Engineer is responsible for developing and maintaining ELT pipelines, data models, and lakehouse architecture using modern cloud and big data technologies, with a strong focus on reliability, performance, and production ownership",
        "Lakehouse & Data Architecture",
        "Designs, develops, and maintains Databricks-based lakehouse solutions leveraging medallion architecture (Bronze, Silver, Gold) and Delta Lake",
        "Builds scalable, high-performance analytical data models to support reporting, analytics, and downstream consumption",
        "Ensures data solutions align with enterprise standards for security, governance, scalability, and cost efficiency",
        "Data Engineering & ELT Pipelines",
        "Develops, tests, and maintains batch and streaming ELT pipelines using Spark SQL",
        "Implements incremental data processing, dependency management, and idempotent pipeline patterns across multiple data sources",
        "Integrates structured and semi-structured data from transactional, digital, and third-party systems into the lakehouse",
        "Databricks Platform Development",
        "Develops and maintains Databricks notebooks, workflows, and jobs across development, QA, and production environments",
        "Leverages Delta Lake features including ACID transactions, schema enforcement and evolution, and time travel",
        "Partners with platform and DevOps teams to support environment promotion, scheduling, and deployment automation",
        "Data Quality & Reliability",
        "Implements automated data quality checks, validation frameworks, and monitoring to ensure accuracy, completeness, and timeliness of data",
        "Proactively identifies and resolves data issues, ensuring reliable data availability for business partners",
        "Performance Optimization & Tuning",
        "Troubleshoots data pipeline and query performance issues and implements optimizations related to Spark execution, partitioning, file sizing, and data layout",
        "Documents root cause analysis and corrective actions and provide recommendations to improve system performance and reliability",
        "Production Ownership & Operations",
        "Owns data pipelines end-to-end in production, including monitoring, alerting, incident response, and operational support",
        "Contributes to runbooks, operational documentation, and continuous improvement of production readiness practices",
        "Programming & Standards",
        "Writes clean, maintainable, and well-tested code using Python (PySpark) and SQL",
        "Contributes to data engineering standards, reusable frameworks, and best practices across the Analytics Engineering team",
        "Documentation & Reviews",
        "Produces clear technical documentation, including design specifications and data flow diagrams",
        "Participates in design reviews and code reviews, providing constructive feedback to ensure quality and consistency",
        "Collaboration & Stakeholder Engagement",
        "Collaborates closely with data engineers, analytics engineers, data scientists, digital analysts, and business stakeholders to translate business needs into scalable data solutions",
        "Works cross-functionally with project managers, platform teams, and external partners to deliver high-impact analytics capabilities",
        "Mentorship & Technical Leadership",
        "Mentors and supports less experienced data engineers through technical guidance and best-practice sharing",
        "Leads technical design discussions for complex datasets and influences architectural decisions within the team",
        "Continuous Improvement",
        "Stays current with emerging data engineering technologies, trends, and industry best practices",
        "Identifies opportunities to improve efficiency, reliability, and scalability across data engineering workflows",
        "Performs other duties as assigned"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-discounttire-com-job-senior-data-engineer-omni-channel-scottsdale-az-16110",
    "_source": "new_jobs"
  },
  {
    "job_id": "RuTayARBH-iyKftdAAAAAA==",
    "job_title": "Data Science and IoT Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=399920009970&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=399920009970&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for an Analytics Data Science and IoT Engineer.\n\nKey Responsibilities\n\nDesign, build, and maintain efficient ETL / ELT pipelines for structured and unstructured data\n\nDevelop and optimize data workflows for advanced analytics and machine learning models\n\nCollaborate with stakeholders to ensure data quality and availability across various platforms\n\nRequired Qualifications\n\nExperience with cloud platforms (AWS, Azure, GCP) and data warehouses (e.g., Databricks)\n\nHands-on experience with LLMs and generative AI, particularly OpenAI\n\nKnowledge of data architecture patterns such as data lakes and data warehouses\n\nFamiliarity with data visualization and BI tools like PowerBI and Tableau\n\nExperience in developing and maintaining AI agents using Azure OpenAI and related technologies",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Huntington Beach, CA",
    "job_city": "Huntington Beach",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 33.6594835,
    "job_longitude": -117.99880259999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRuTayARBH-iyKftdAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Experience with cloud platforms (AWS, Azure, GCP) and data warehouses (e.g., Databricks)",
        "Hands-on experience with LLMs and generative AI, particularly OpenAI",
        "Knowledge of data architecture patterns such as data lakes and data warehouses",
        "Familiarity with data visualization and BI tools like PowerBI and Tableau",
        "Experience in developing and maintaining AI agents using Azure OpenAI and related technologies"
      ],
      "Responsibilities": [
        "Design, build, and maintain efficient ETL / ELT pipelines for structured and unstructured data",
        "Develop and optimize data workflows for advanced analytics and machine learning models",
        "Collaborate with stakeholders to ensure data quality and availability across various platforms"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "Hh3sYVJawgugevAGAAAAAA==",
    "job_title": "Thermal Analysis Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=02ae34287d13&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=02ae34287d13&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Thermal Analysis Engineer for a fully remote contract position.\n\nKey Responsibilities\n\nDevelop and modify thermal models for reactor structures and evaluate thermal profiles\n\nAssess specific areas of challenge in configurations and provide recommendations for modifications\n\nCollaborate with design leads and analysis engineers to address component issues and provide feedback\n\nRequired Qualifications\n\nBachelor's degree in mechanical engineering or a relevant equivalent program\n\n5-10 years of relevant experience, with a preference for candidates with a PE license\n\nExperience in ASME BPVC Section III, Division 1, and high temperature applications\n\nQualification indoctrination to the client's quality program\n\nStrong integrity and teamwork orientation",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Ocala, FL",
    "job_city": "Ocala",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 29.185078299999997,
    "job_longitude": -82.1342596,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHh3sYVJawgugevAGAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree in mechanical engineering or a relevant equivalent program",
        "5-10 years of relevant experience, with a preference for candidates with a PE license",
        "Experience in ASME BPVC Section III, Division 1, and high temperature applications",
        "Qualification indoctrination to the client's quality program",
        "Strong integrity and teamwork orientation"
      ],
      "Responsibilities": [
        "Develop and modify thermal models for reactor structures and evaluate thermal profiles",
        "Assess specific areas of challenge in configurations and provide recommendations for modifications",
        "Collaborate with design leads and analysis engineers to address component issues and provide feedback"
      ]
    },
    "job_onet_soc": "17214100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "Z6i2SPKRWyXY7aRZAAAAAA==",
    "job_title": "Senior Analytics Developer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=623d6703c402&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=623d6703c402&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Senior Analytics Developer, Brokerage & Ledger Product Data Science.\n\nKey Responsibilities\n\nDesign, build, and maintain high-quality analytical data models for brokerage and ledger workflows\n\nOwn critical datasets that support client-facing metrics, operational decision-making, and regulatory reporting\n\nCollaborate with cross-functional teams to translate stakeholder needs into durable data solutions\n\nRequired Qualifications\n\nExperience working with large, complex analytical datasets in a production environment\n\nStrong SQL skills and experience building analytical models or data marts\n\nProduct-oriented mindset focused on data usability and long-term ownership\n\nFamiliarity with modern analytics tooling such as dbt, Airflow, Python, and cloud data warehouses\n\nExperience in financial services, trading, or accounting is a plus",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Cleveland, OH",
    "job_city": "Cleveland",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 41.49932,
    "job_longitude": -81.6943605,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DZ6i2SPKRWyXY7aRZAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Experience working with large, complex analytical datasets in a production environment",
        "Strong SQL skills and experience building analytical models or data marts",
        "Product-oriented mindset focused on data usability and long-term ownership",
        "Familiarity with modern analytics tooling such as dbt, Airflow, Python, and cloud data warehouses"
      ],
      "Responsibilities": [
        "Design, build, and maintain high-quality analytical data models for brokerage and ledger workflows",
        "Own critical datasets that support client-facing metrics, operational decision-making, and regulatory reporting",
        "Collaborate with cross-functional teams to translate stakeholder needs into durable data solutions"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "xIy-w4lp_7eHIBzJAAAAAA==",
    "job_title": "CTIO-AI Engineer-Sr Associate",
    "employer_name": "Line of Service:Internal Firm Services",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQbISADYwodxUByAAwOWWSxMKSDRSb_NgVyIy99&s=0",
    "employer_website": null,
    "job_publisher": "PwC | US Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.us.pwc.com/job/new-york/ctio-ai-engineer-sr-associate/932/91339725632?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "PwC | US Careers",
        "apply_link": "https://jobs.us.pwc.com/job/new-york/ctio-ai-engineer-sr-associate/932/91339725632?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/PwC/Job/CTIO-AI-Engineer-Sr-Associate/-in-Saint-Louis,MO?jid=4b5d8fcf834930b0&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9eecfea443c4a281&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/ctio-ai-engineer-sr-associate-at-pwc-4368756425?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/ctio-ai-engineer-sr-associate-pwc-JV_IC1131270_KO0,29_KE30,33.htm?jl=1010022854043&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/ctio-ai-engineer-sr-associate-st-louis-pwc-8edb9e3828997f3663ef9ecfdaed92ff?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/pRVYXuDZYs6LRg5SJPdUOa1MCpmR7cH-WINKhFaqPqNX14FEKBcrCw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/ctio-ai-engineer-sr-associate-st-louis-missouri-us-pwc-702342wd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth. Those in data science and machine learning engineering at PwC will focus on leveraging advanced analytics and machine learning techniques to extract insights from large datasets and drive data-driven decision making. You will work on developing predictive models, conducting statistical analysis, and creating data visualisations to solve complex business problems.\n\nFocused on relationships, you are building meaningful client connections, and learning how to manage and inspire others. Navigating increasingly complex situations, you are growing your personal brand, deepening technical expertise and awareness of your strengths. You are expected to anticipate the needs of your teams and clients, and to deliver quality. Embracing increased ambiguity, you are comfortable when the path forward isn’t clear, you ask questions, and you use these moments as opportunities to grow.\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n• Respond effectively to the diverse perspectives, needs, and feelings of others.\n• Use a broad range of tools, methodologies and techniques to generate new ideas and solve problems.\n• Use critical thinking to break down complex concepts.\n• Understand the broader objectives of your project or role and how your work fits into the overall strategy.\n• Develop a deeper understanding of the business context and how it is changing.\n• Use reflection to develop self awareness, enhance strengths and address development areas.\n• Interpret data to inform insights and recommendations.\n• Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nThe Opportunity\n\nAs part of the Data and Analytics Engineering team you will develop and implement AI solutions that enhance product offerings. As a Senior Associate, you will analyze complex problems, mentor junior team members, and maintain elevated professional standards while building meaningful client relationships.\n\nResponsibilities\n\n- Build and nurture meaningful relationships with clients\n\n- Utilize advanced analytical techniques to drive innovation\n\n- Work with cross-functional teams to achieve project goals\n\n- Uphold the firm's ethical standards and recommended practices\n\nWhat You Must Have\n\n- Bachelor's Degree in Computer Science, Data Processing/Analytics/Science, Artificial Intelligence and Robotics\n\n- 3 years of professional experience developing AI/ML systems or integrating AI into products\n\nWhat Sets You Apart\n\n- Master's Degree preferred\n\n- Possessing advanced proficiency in prompt engineering\n\n- Demonstrating experience deploying LLMs into production\n\n- Designing and optimizing RAG pipelines\n\n- Leading technical discovery in fast-paced environments\n\n- Collaborating effectively with cross-functional leaders\n\n- Advocating for responsible AI principles\n\n- Contributing to AI research or open-source communities\n\n- Demonstrating knowledge of orchestration tools such as LangChain, LlamaIndex, and experience with agent-based systems\n\nLearn more about how we work: https://pwc.to/how-we-work\n\nPwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.\n\nAs PwC is an equal opportunity employer, all qualified applicants will receive consideration for employment at PwC without regard to race; color; religion; national origin; sex (including pregnancy, sexual orientation, and gender identity); age; disability; genetic information (including family medical history); veteran, marital, or citizenship status; or, any other status protected by law. \n\nFor only those qualified applicants that are impacted by the Los Angeles County Fair Chance Ordinance for Employers, the Los Angeles' Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, San Diego County Fair Chance Ordinance, and the California Fair Chance Act, where applicable, arrest or conviction records will be considered for Employment in accordance with these laws. At PwC, we recognize that conviction records may have a direct, adverse, and negative relationship to responsibilities such as accessing sensitive company or customer information, handling proprietary assets, or collaborating closely with team members. We evaluate these factors thoughtfully to establish a secure and trusted workplace for all.\n\nApplications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines\n\nThe salary range for this position is: $55,000 - $151,470. For residents of Washington state the salary range for this position is: $55,000 - $187,000. Actual compensation within the range will be dependent upon the individual's skills, experience, qualifications and location, and applicable employment laws. All hired individuals are eligible for an annual discretionary bonus. PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more. To view our benefits at a glance, please visit the following link: https://pwc.to/benefits-at-a-glance",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Washington, DC",
    "job_city": "Washington",
    "job_state": "District of Columbia",
    "job_country": "US",
    "job_latitude": 38.9072873,
    "job_longitude": -77.0369274,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DxIy-w4lp_7eHIBzJAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's Degree in Computer Science, Data Processing/Analytics/Science, Artificial Intelligence and Robotics",
        "3 years of professional experience developing AI/ML systems or integrating AI into products",
        "Possessing advanced proficiency in prompt engineering",
        "Demonstrating experience deploying LLMs into production",
        "Designing and optimizing RAG pipelines",
        "Leading technical discovery in fast-paced environments",
        "Collaborating effectively with cross-functional leaders",
        "Advocating for responsible AI principles",
        "Contributing to AI research or open-source communities",
        "Demonstrating knowledge of orchestration tools such as LangChain, LlamaIndex, and experience with agent-based systems"
      ],
      "Benefits": [
        "The salary range for this position is: $55,000 - $151,470",
        "For residents of Washington state the salary range for this position is: $55,000 - $187,000",
        "All hired individuals are eligible for an annual discretionary bonus",
        "PwC offers a wide range of benefits, including medical, dental, vision, 401k, holiday pay, vacation, personal and family sick leave, and more"
      ],
      "Responsibilities": [
        "They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth",
        "Those in data science and machine learning engineering at PwC will focus on leveraging advanced analytics and machine learning techniques to extract insights from large datasets and drive data-driven decision making",
        "You will work on developing predictive models, conducting statistical analysis, and creating data visualisations to solve complex business problems",
        "Focused on relationships, you are building meaningful client connections, and learning how to manage and inspire others",
        "Navigating increasingly complex situations, you are growing your personal brand, deepening technical expertise and awareness of your strengths",
        "You are expected to anticipate the needs of your teams and clients, and to deliver quality",
        "Respond effectively to the diverse perspectives, needs, and feelings of others",
        "Use a broad range of tools, methodologies and techniques to generate new ideas and solve problems",
        "Use critical thinking to break down complex concepts",
        "Understand the broader objectives of your project or role and how your work fits into the overall strategy",
        "Develop a deeper understanding of the business context and how it is changing",
        "Use reflection to develop self awareness, enhance strengths and address development areas",
        "Interpret data to inform insights and recommendations",
        "Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements",
        "As part of the Data and Analytics Engineering team you will develop and implement AI solutions that enhance product offerings",
        "As a Senior Associate, you will analyze complex problems, mentor junior team members, and maintain elevated professional standards while building meaningful client relationships",
        "Build and nurture meaningful relationships with clients",
        "Utilize advanced analytical techniques to drive innovation",
        "Work with cross-functional teams to achieve project goals",
        "Uphold the firm's ethical standards and recommended practices"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "jobs-us-pwc-com-job-new-york-ctio-ai-engineer-sr-associate-932-91339725632",
    "_source": "new_jobs"
  },
  {
    "job_id": "V8L-NvQ_aen-CWbxAAAAAA==",
    "job_title": "Data Analytics Engineer - Marketing at MassMutual Springfield, MA",
    "employer_name": "MassMutual",
    "employer_logo": null,
    "employer_website": "https://www.massmutual.com",
    "job_publisher": "Moral Theologian",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://moraltheologian.com/career/job/data-analytics-engineer-marketing-at-massmutual-springfield-ma-VDNWWDdNSzNrNTN5WjlXL09ON3NBWUxlbXc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Moral Theologian",
        "apply_link": "https://moraltheologian.com/career/job/data-analytics-engineer-marketing-at-massmutual-springfield-ma-VDNWWDdNSzNrNTN5WjlXL09ON3NBWUxlbXc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Analytics Engineer - Marketing job at MassMutual. Springfield, MA.\n\nData Analytics Engineer, Data Foundation & Journey Analytics\n\nFull-Time, Hybrid, Boston or Springfield office\n\nThe Opportunity\n\nAs the Data Analytics Engineer , you will be prototyping and building data architecture that combines multiple marketing data sources, to enable a data foundation that supports marketing analytics, various marketing stakeholders to make data-driven decisions based on a high-quality and automated data environment. You will support and enable advanced sales campaigns by leveraging data to identify target audiences, optimize campaign strategies, and measure the effectiveness of marketing efforts. Additionally, you will join the workforce in building a future customer data mart, which will serve as a centralized repository for customer data, enabling more personalized and effective marketing initiatives. Furthermore, you will support AI initiatives by developing and implementing machine learning models that can predict customer behavior, optimize marketing spend, and enhance overall customer engagement.\n\nThe Team\n\nThis position is under the Data Foundation & Journey Analytics team, which acts as the liaison between marketing and enterprise data/IT teams. The team is part of the Marketing Analytics and Customer Insights group, which aims to drive growth and opportunities across the organization with brand analytics, marketing technology, insights and customer engagement.\n\nThe Impact (what your days & weeks will include):\n• Own and enhance advanced sales opportunity campaign\n• Collaborate with the Data Science team to deliver machine learning model deployment and enhancements\n• Support Marketing customer data vision and build data infrastructure to scale book of business analysis and advanced customer analytics.\n• Support and build sales analytics data strategy to scale advanced sales programs\n• Support and drive innovative AI capabilities to discover and implement marketing and sales opportunities to drive ROI and MassMutual's competitive advantage.\n• Deliver concise, clear and actionable presentations to decision-makers\n• Communicate data in a storytelling format, engage your audience and influence them to take actions\n\nThe Minimum Qualifications\n• Bachelor’s degree\n• 3+ years of experience in the areas of ETL/Data warehousing, analytics or analytical data engineering; familiar with multiple data warehouse environments\n• 5+ years of experience with SQL or SQL like languages\n• 3+ years of experience in data architecture, ideally in prototyping for campaign attribution model, marketing cross-sell model as well as customer journey data models with proven experience using data sources, tools, statistical principles/methodologies and techniques to improve business outcomes\n\nThe Ideal Qualifications\n• Master’s degree in Analytics, Applied Mathematics or Engineering with at least 5-year experience working in both insurance and financial industry.\n• 5+ years of experience in the areas of ETL/Data warehousing, analytics or analytical data engineering.\n• Strong communication and collaboration skills, capability to translate complex technical concepts to business stake holders.\n• Strong interpersonal skills and ability to work in a team environment as well as independently\n• Highly experienced with data integration design for marketing application in SFMC, email marketing tools and social media marketing tools\n• Proven data modeling experience, solid knowledge in different data modeling types such as Star Schema, Snowflake Schema and Galaxy schema to build marketing data assets.\n• Experience working in Jira environment, team sharing tools as well as code repositories such as Confluence, Bitbucket, Github\n• Proven experience with complex statistical predictive modeling, strong story telling skills to deliver actionable insights.\n• Ability to lead end to end vendor data ingestion process (sftp and API)\n• Ability to drive and manage project roadmaps in alignment with business stakeholders. Strong storytelling skills to convey complex data insights effectively.\n• Proven statistical/mathematical and analytical skills, familiarity with visualization tools such as Tableau or MicroStrategy, hands-on experience with R or Python.\n\nWhat to Expect as Part of MassMutual and the Team\n• Regular meetings with the Marketing Analytics Team\n• Focused one-on-one meetings with your manager\n• Access to mentorship opportunities\n• Networking opportunities including access to Asian, Hispanic/Latinx, African American, women, LGBTQ, veteran and disability-focused Business Resource Groups\n• Access to learning content on Degreed and other informational platforms\n• Your ethics and integrity will be valued by a company with a strong and stable ethical nosiness with industry leading pay and benefits\n\n#LI-RS1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\n\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1769990400,
    "job_posted_at_datetime_utc": "2026-02-02T00:00:00.000Z",
    "job_location": "Springfield, MA",
    "job_city": "Springfield",
    "job_state": "Massachusetts",
    "job_country": "US",
    "job_latitude": 42.101310399999996,
    "job_longitude": -72.58927419999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DV8L-NvQ_aen-CWbxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor’s degree",
        "3+ years of experience in the areas of ETL/Data warehousing, analytics or analytical data engineering; familiar with multiple data warehouse environments",
        "5+ years of experience with SQL or SQL like languages",
        "3+ years of experience in data architecture, ideally in prototyping for campaign attribution model, marketing cross-sell model as well as customer journey data models with proven experience using data sources, tools, statistical principles/methodologies and techniques to improve business outcomes",
        "Master’s degree in Analytics, Applied Mathematics or Engineering with at least 5-year experience working in both insurance and financial industry",
        "5+ years of experience in the areas of ETL/Data warehousing, analytics or analytical data engineering",
        "Strong communication and collaboration skills, capability to translate complex technical concepts to business stake holders",
        "Strong interpersonal skills and ability to work in a team environment as well as independently",
        "Highly experienced with data integration design for marketing application in SFMC, email marketing tools and social media marketing tools",
        "Proven data modeling experience, solid knowledge in different data modeling types such as Star Schema, Snowflake Schema and Galaxy schema to build marketing data assets",
        "Experience working in Jira environment, team sharing tools as well as code repositories such as Confluence, Bitbucket, Github",
        "Proven experience with complex statistical predictive modeling, strong story telling skills to deliver actionable insights",
        "Ability to lead end to end vendor data ingestion process (sftp and API)",
        "Ability to drive and manage project roadmaps in alignment with business stakeholders",
        "Strong storytelling skills to convey complex data insights effectively",
        "Proven statistical/mathematical and analytical skills, familiarity with visualization tools such as Tableau or MicroStrategy, hands-on experience with R or Python",
        "Networking opportunities including access to Asian, Hispanic/Latinx, African American, women, LGBTQ, veteran and disability-focused Business Resource Groups",
        "Access to learning content on Degreed and other informational platforms"
      ],
      "Benefits": [
        "Access to mentorship opportunities",
        "Your ethics and integrity will be valued by a company with a strong and stable ethical nosiness with industry leading pay and benefits"
      ],
      "Responsibilities": [
        "As the Data Analytics Engineer , you will be prototyping and building data architecture that combines multiple marketing data sources, to enable a data foundation that supports marketing analytics, various marketing stakeholders to make data-driven decisions based on a high-quality and automated data environment",
        "You will support and enable advanced sales campaigns by leveraging data to identify target audiences, optimize campaign strategies, and measure the effectiveness of marketing efforts",
        "Additionally, you will join the workforce in building a future customer data mart, which will serve as a centralized repository for customer data, enabling more personalized and effective marketing initiatives",
        "Furthermore, you will support AI initiatives by developing and implementing machine learning models that can predict customer behavior, optimize marketing spend, and enhance overall customer engagement",
        "The team is part of the Marketing Analytics and Customer Insights group, which aims to drive growth and opportunities across the organization with brand analytics, marketing technology, insights and customer engagement",
        "Own and enhance advanced sales opportunity campaign",
        "Collaborate with the Data Science team to deliver machine learning model deployment and enhancements",
        "Support Marketing customer data vision and build data infrastructure to scale book of business analysis and advanced customer analytics",
        "Support and build sales analytics data strategy to scale advanced sales programs",
        "Support and drive innovative AI capabilities to discover and implement marketing and sales opportunities to drive ROI and MassMutual's competitive advantage",
        "Deliver concise, clear and actionable presentations to decision-makers",
        "Communicate data in a storytelling format, engage your audience and influence them to take actions",
        "What to Expect as Part of MassMutual and the Team",
        "Regular meetings with the Marketing Analytics Team",
        "Focused one-on-one meetings with your manager"
      ]
    },
    "job_onet_soc": "11202100",
    "job_onet_job_zone": "4",
    "id": "moraltheologian-com-career-job-data-analytics-engineer-marketing-at-massmutual-springfield-ma-vdnwwddnsznrntn5wjlxl09on3nbwuxlbxc9pq",
    "_source": "new_jobs"
  }
]