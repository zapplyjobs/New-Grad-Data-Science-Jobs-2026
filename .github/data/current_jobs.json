[
  {
    "job_id": "UVVio6kevgI1MfEaAAAAAA==",
    "job_title": "Data Engineer/Flask Developer - 26410",
    "employer_name": "HII's Mission Technologies division",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Jobs.hii-Tsd.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.hii-tsd.com/job/Arlington%2C-VA-Data-EngineerFlask-Developer-26410-Remo/1351019200/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobs.hii-Tsd.com",
        "apply_link": "https://jobs.hii-tsd.com/job/Arlington%2C-VA-Data-EngineerFlask-Developer-26410-Remo/1351019200/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6944ed34a7227b3ece669c64?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-react-developer-26405-at-mission-technologies-a-division-of-hii-4344885344?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "| USNLX Veterans Jobs - National Labor Exchange",
        "apply_link": "https://veterans.usnlx.com/arlington-va/data-engineerreact-developer-26405/1954D06D256747DCBCAC949C2AF6BB34/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BillGoldenJobs.com",
        "apply_link": "https://jobzone.billgoldenjobs.com/jobs/260922662-data-engineer-flask-developer-26410?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/data-engineer-flask-developer-26410--arlington--e49f5cbc72919096a39cff20a15c991f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/ce2e19a2860c4cbbd1ec281458d1fdba?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Career.com",
        "apply_link": "https://www.career.com/job/mission-technologies-a-division-of-hii/data-engineer-flask-developer-26410/j202512190733387026148?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Requisition Number: 26410\n\nRequired Travel: 0 - 10%\n\nEmployment Type: Full Time/Salaried/Exempt\n\nAnticipated Salary Range: $98,717.00 - $130,000.00\n\nSecurity Clearance: Secret\n\nLevel of Experience: Mid\n\nThis opportunity resides with Warfare Systems (WS), a business group within HII’s Mission Technologies division. Warfare Systems comprises cyber and mission IT; electronic warfare; and C5ISR systems.\n\nHII works within our nation’s intelligence and cyber operations communities to defend our interests in cyberspace and anticipate emerging threats. Our capabilities in cybersecurity, network architecture, reverse engineering, software and hardware development uniquely enable us to support sensitive missions for the U.S. military and federal agency partners.\n\nMeet HII’s Mission Technologies Division\nOur team of more than 7,000 professionals worldwide delivers all-domain expertise and advanced technologies in service of mission partners across the globe. Mission Technologies is leading the next evolution of national defense – the data evolution - by accelerating a breadth of national security solutions for government and commercial customers. Our capabilities range from C5ISR, AI and Big Data, cyber operations and synthetic training environments to fleet sustainment, environmental remediation and the largest family of unmanned underwater vehicles in every class. Find the role that’s right for you. Apply today. We look forward to meeting you.\n\nTo learn more about Mission Technologies, click here for a short video: https://vimeo.com/732533072\nJob Description\n\nCome join our growing team today, supporting our Warfare Systems Business Group! HII-Mission Technologies is currently seeking a skilled Data Engineer/Flask Developer, who will support refining a centralized data environment (CDE) with impact across DoD! Key functionality for this position contributes toward auditable financial transaction data and building a common operating picture for leadership.\n\nA successful candidate will be well-versed in Flask and Python, knowledgeable about relational database (SQL), has strong written/oral communications, and can work as part of a team providing productive input toward solving challenging problems. Team members must be able to internalize and appreciate strategic requirements, to best align the most meaningful data to decision makers- getting the right data to the right people!\n\nEssential Job Responsibilities\n• Research, design, develop, and/or modify enterprise-wide systems and/or applications software.\n• Will be Involved in planning of system and development deployment as well as responsible for meeting software compliance standards.\n• Evaluate interface between hardware and software, operational requirements, and characteristics of overall system.\n• Document testing and maintenance of system corrections.\n• Design, develop, and maintain scalable backend services and RESTful APIs using Flask, Flask-RESTful, Flask-RESTX, or similar extensions, following best practices and clean architecture principles\n• Build and extend complex business logic in Python, implementing features from technical specifications and user stories while following SOLID principles and Flask best practices\n• Familiar with scripts for data ingest pipelines (Python).\n• Familiar with Relational Databases (SQL).\n• Utilize Agile development processes.\n• Guide development aligned to established business needs and/or policies.\n• Participate in code reviews, architectural discussions, and sprint planning, providing constructive feedback, suggesting improvements, and helping enforce coding standards\n• Additional duties as assigned or required.\n\nMinimum Qualifications\n• 5 years relevant experience with Bachelors in related field; 3 years relevant experience with Masters in related field; 0 years experience with PhD or Juris Doctorate in related field; or High School Diploma or equivalent and 9 years relevant experience.\n• Hands‑on experience building applications using Flask,\n• Proficiency in Python programming with solid understanding of core concepts,\n• Can operate independently and within a team.\n• Self-starter who takes initiative.\n• Clearance: Must possess and maintain a Secret clearance.\n\nPreferred Requirements\n• Ability to understand the transactional nature of the data being reviewed.\n• Experience with FMS, ADVANA.\n• Basic SQL skills for querying, managing, and optimizing relational databases.\n• Experience working in a Safe Agile environment.\n\nPhysical Requirements\n• Job performance requires adequate visual acuity and manual dexterity for meeting the requirements of the generic systems analyst discipline. Office work environment normally encountered.\n\nHII is more than a job - it’s an opportunity to build a new future. We offer competitive benefits such as best-in-class medical, dental and vision plan choices; wellness resources; employee assistance programs; Savings Plan Options (401(k)); financial planning tools, life insurance; employee discounts; paid holidays and paid time off; tuition reimbursement; as well as early childhood and post-secondary education scholarships. Bonus/other non-recurrent compensation is occasionally offered for qualified positions, and if applicable to this role will be addressed by the recruiter at the screening phase of application.\n\nWhy HII\nWe build the world’s most powerful, survivable naval ships and defense technology solutions that safeguard our seas, sky, land, space and cyber. Our workforce includes skilled tradespeople; artificial intelligence, machine learning (AI/ML) experts; engineers; technologists; scientists; logistics experts; and business administration professionals.\n\nRecognized as one of America’s top large company employers, we are a values and ethics driven organization that puts people’s safety and well-being first. Regardless of your role or where you serve, at HII, you’ll find a supportive and welcoming environment, competitive benefits, and valuable educational and training programs for continual career growth at every stage of your career.\n\nTogether we are working to ensure a future where everyone can be free and thrive.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.\n\nDo You Need Assistance?\nIf you need a reasonable accommodation for any part of the employment process, please send an e-mail to buildyourcareer@hii-co.com and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. Additionally, you may also call 1-844-849-8463 for assistance. Press #3 for HII Mission Technologies.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771113600,
    "job_posted_at_datetime_utc": "2026-02-15T00:00:00.000Z",
    "job_location": "Arlington, VA",
    "job_city": "Arlington",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.8816208,
    "job_longitude": -77.09098089999999,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DUVVio6kevgI1MfEaAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "A successful candidate will be well-versed in Flask and Python, knowledgeable about relational database (SQL), has strong written/oral communications, and can work as part of a team providing productive input toward solving challenging problems",
        "Team members must be able to internalize and appreciate strategic requirements, to best align the most meaningful data to decision makers- getting the right data to the right people!",
        "5 years relevant experience with Bachelors in related field; 3 years relevant experience with Masters in related field; 0 years experience with PhD or Juris Doctorate in related field; or High School Diploma or equivalent and 9 years relevant experience",
        "Hands‑on experience building applications using Flask,",
        "Proficiency in Python programming with solid understanding of core concepts,",
        "Can operate independently and within a team",
        "Self-starter who takes initiative",
        "Clearance: Must possess and maintain a Secret clearance",
        "Job performance requires adequate visual acuity and manual dexterity for meeting the requirements of the generic systems analyst discipline"
      ],
      "Benefits": [
        "Anticipated Salary Range: $98,717.00 - $130,000.00",
        "We offer competitive benefits such as best-in-class medical, dental and vision plan choices; wellness resources; employee assistance programs; Savings Plan Options (401(k)); financial planning tools, life insurance; employee discounts; paid holidays and paid time off; tuition reimbursement; as well as early childhood and post-secondary education scholarships"
      ],
      "Responsibilities": [
        "Required Travel: 0 - 10%",
        "Key functionality for this position contributes toward auditable financial transaction data and building a common operating picture for leadership",
        "Research, design, develop, and/or modify enterprise-wide systems and/or applications software",
        "Will be Involved in planning of system and development deployment as well as responsible for meeting software compliance standards",
        "Evaluate interface between hardware and software, operational requirements, and characteristics of overall system",
        "Document testing and maintenance of system corrections",
        "Design, develop, and maintain scalable backend services and RESTful APIs using Flask, Flask-RESTful, Flask-RESTX, or similar extensions, following best practices and clean architecture principles",
        "Build and extend complex business logic in Python, implementing features from technical specifications and user stories while following SOLID principles and Flask best practices",
        "Familiar with scripts for data ingest pipelines (Python)",
        "Familiar with Relational Databases (SQL)",
        "Utilize Agile development processes",
        "Guide development aligned to established business needs and/or policies",
        "Participate in code reviews, architectural discussions, and sprint planning, providing constructive feedback, suggesting improvements, and helping enforce coding standards",
        "Additional duties as assigned or required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-hii-tsd-com-job-arlington-2c-va-data-engineerflask-developer-26410-remo-1351019200",
    "_source": "new_jobs"
  },
  {
    "job_id": "DI_gmnxno-NVwm8wAAAAAA==",
    "job_title": "Senior Data Engineer (Informatica/Databricks)",
    "employer_name": "CACI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSEIAi6XHhq4EhegiHCD5u672hDATM_CMWOaqCb&s=0",
    "employer_website": null,
    "job_publisher": "CACI Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "CACI Careers",
        "apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9abc0f815d739892&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Infinitive-Inc/Job/Senior-Data-Engineer-(Python-PySpark-AWS)/-in-Ashburn,VA?jid=80f99d04bc706cc2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/senior-data-engineer-informaticadatabricks-ashburn-caci-international-inc-b8b74c37cb3bb6008869745852135bbe?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "TekSynap Job Openings - ICIMS",
        "apply_link": "https://careers-teksynap.icims.com/jobs/8146/senior-data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Tech Jobs Personalized",
        "apply_link": "https://builtin.com/job/senior-data-engineer-pythonpysparkaws/4210594?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/senior-data-engineer-informatica-databricks_7ea1ae6375c0867ceb8124f167b38d5087153?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-data-engineer-databricks-ashburn-va--5f5b01bf-92fb-430c-bb16-c97cfe25d721?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior Data Engineer (Informatica/Databricks)\n\nJob Category: Information Technology\n\nTime Type: Full time\n\nMinimum Clearance Required to Start: None\n\nEmployee Type: Regular\n\nPercentage of Travel Required: Up to 10%\n\nType of Travel: Local\n• * *\n\nThe Opportunity:\n\nCACI is currently looking for a highly skilled and experienced Senior Data Engineer (Informatica/Databricks) with agile methodology experience to join our BEAGLE (Border Enforcement Applications for Government Leading-Edge Information Technology) Agile Solution Factory (ASF) Team supporting Customs and Border Protection (CBP) client located in Northern Virginia! Join this passionate team of industry-leading individuals supporting the best practices in Agile Software Development for the Department of Homeland Security (DHS).\n\nAs a member of the BEAGLE ASF Team, you will support the men and women charged with safeguarding the American people and enhancing the Nation’s safety, security, and prosperity. CBP agents and officers are on the front lines, every day, protecting our national security by combining customs, immigration, border security, and agricultural protection into one coordinated and supportive activity.\n\nASF programs thrive in a culture of innovation and are constantly seeking individuals who can bring creative ideas to solve complex problems, both technical and procedural at the team and portfolio levels. The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial. You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints\n\nResponsibilities:\n• Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform.\n• Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing.\n• Work with database developers and administrators across multiple product teams.\n• Serve as a data and technology expert across a broad and diverse set of mission critical applications\n• Modernize the data warehouse environment by migrating the platform to Databricks\n• Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes.\n• Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components.\n• Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets.\n• Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases\n\nQualifications:\n\n· Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:\n\no 3 year check for felony convictions\n\no 1 year check for illegal drug use\n\no 1 year check for misconduct such as theft or fraud\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· Proven 7+ years of experience automating ELT data pipelines using Informatica.\n\n· Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS).\n\n· Experience building and optimizing data pipelines for batch and/or streaming data.\n\n· 3-5 years of Databricks experience. Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered.\n\n· Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks. Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design.\n\n· Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks\n\n· Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies.\n\n· Candidates with one or more of the above skillsets are encouraged to apply.\n\nDesired:\n\n· 5-10 years of DHS, DoD, or IC experience working in complex data environments, including the architecture and optimization of data schemas, terabyte-scale ETL, etc.\n\n· Exposure to implementing or migrating to Cloud environments like Amazon Web Services (AWS) or Microsoft Azure.\n\n· Previous experience as an Enterprise-level Data Architect, Data Engineer, Data Scientist, or Data Analyst.\n\n· Ability to apply advanced principles, theories, and concepts, and contribute to the development of innovative principles and ideas.\n\n-\n\n_________________________________________________________________________\n\nWhat You Can Expect:\n\nA culture of integrity.\n\nAt CACI, we place character and innovation at the center of everything we do. As a valued team member, you’ll be part of a high-performing group dedicated to our customer’s missions and driven by a higher purpose – to ensure the safety of our nation.\n\nAn environment of trust.\n\nCACI values the unique contributions that every employee brings to our company and our customers - every day. You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality.\n\nA focus on continuous growth.\n\nTogether, we will advance our nation's most critical missions, build on our lengthy track record of business success, and find opportunities to break new ground — in your career and in our legacy.\n\nYour potential is limitless. So is ours.\n\n_________________________________________________________________________\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits.\n\nThe proposed salary range for this position is:\n$113,200 - $237,800\n\nCACI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, age, national origin, disability, status as a protected veteran, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Ashburn, VA",
    "job_city": "Ashburn",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 39.043756699999996,
    "job_longitude": -77.4874416,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDI_gmnxno-NVwm8wAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial",
        "You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints",
        "Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:",
        "3 year check for felony convictions",
        "1 year check for illegal drug use",
        "1 year check for misconduct such as theft or fraud",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "Proven 7+ years of experience automating ELT data pipelines using Informatica",
        "Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS)",
        "Experience building and optimizing data pipelines for batch and/or streaming data",
        "3-5 years of Databricks experience",
        "Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered",
        "Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks",
        "Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design",
        "Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks",
        "Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies",
        "Candidates with one or more of the above skillsets are encouraged to apply"
      ],
      "Benefits": [
        "CACI values the unique contributions that every employee brings to our company and our customers - every day",
        "You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality",
        "A focus on continuous growth",
        "Pay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications",
        "Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives",
        "We offer competitive compensation, benefits and learning and development opportunities",
        "Our broad and competitive mix of benefits options is designed to support and protect employees and their families",
        "At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits",
        "$113,200 - $237,800"
      ],
      "Responsibilities": [
        "Percentage of Travel Required: Up to 10%",
        "Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform",
        "Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing",
        "Work with database developers and administrators across multiple product teams",
        "Serve as a data and technology expert across a broad and diverse set of mission critical applications",
        "Modernize the data warehouse environment by migrating the platform to Databricks",
        "Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes",
        "Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components",
        "Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets",
        "Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-caci-com-global-en-job-caciglobal322083externalenglobal",
    "_source": "new_jobs"
  },
  {
    "job_id": "aLEr5N2HS9ba0TU3AAAAAA==",
    "job_title": "Data Engineer - Remote at Staffing the Universe United States",
    "employer_name": "Staffing the Universe",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Ibfportal.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Ibfportal.com",
        "apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Remote job at Staffing the Universe. United States. Data Engineer\n\nData Engineer Hartford, CT or Remote Contract No third-party C2C\n\nJob Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team. On this team, you will be helping architect and deliver a wide variety of code artifacts. You will be working to build a scalable and secure ETL solutions for ope...",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "New Haven, CT",
    "job_city": "New Haven",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.308274,
    "job_longitude": -72.9278835,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaLEr5N2HS9ba0TU3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Data Engineer Hartford, CT or Remote Contract No third-party C2C"
      ],
      "Responsibilities": [
        "Job Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team",
        "On this team, you will be helping architect and deliver a wide variety of code artifacts",
        "You will be working to build a scalable and secure ETL solutions for ope.."
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "ibfportal-com-office-job-data-engineer-remote-at-staffing-the-universe-united-states-ym9lbu9gtu5hnlh5ede0nys1utrrk2vvzmc9pq",
    "_source": "new_jobs"
  },
  {
    "job_id": "A4Vxqpb6-fnOS8TTAAAAAA==",
    "job_title": "Walmart Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQV0kWbPp7sIFysOBbrsohip-f8-kSq7M6lcXJq&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Walmart",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://walmart.rightspotway.com/jobpage/obycpz8rgf19-ae0-382711d8bea237d360feb0-b3596fd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Walmart",
        "apply_link": "https://walmart.rightspotway.com/jobpage/obycpz8rgf19-ae0-382711d8bea237d360feb0-b3596fd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Walmart Data Engineer\n\nCompany Overview:\n\nWalmart Global Tech is a team of over 15,000 software engineers, data scientists, and service professionals delivering innovative technology solutions that enhance the retail experience for millions of customers and empower Walmart's 2.2 million associates. You will join a virtual-first, collaborative team focused on building the foundational intelligence through data, AI, and semantic modeling that drives autonomous decision-making across Walmart’s ecosystem.\n\nRole and Responsibilities of Walmart Data Engineer:\n\nYou will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously. You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning.\n• Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows.\n• Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state.\n• Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access.\n• Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations.\n• Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals.\n• Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources.\n• Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows.\n\nRequired Skills and Experience of Walmart Data Engineer:\n• 7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role.\n• Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem).\n• Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming).\n• Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI).\n• Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale.\n• Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning.\n• Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design.\n• Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience.\n• 4 years’ experience in data engineering, database engineering, business intelligence, or business analytics.\n• 1 year’s supervisory experience.\n\nPreferred Qualifications:\n• Master’s degree in Computer Science or related field with 5 years’ experience in software engineering or related field.\n• Experience with ETL tools and managing large datasets in cloud environments.\n• Demonstrated knowledge of inclusive digital experiences, Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, and assistive technologies.\n• Background in implementing accessibility best practices for inclusive product and service delivery.\n\nCompensation and Benefits of Walmart Data Engineer:\n• Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses.\n• Stock options and participation in Walmart’s stock purchase plan.\n• Health benefits including medical, vision, and dental coverage.\n• 401(k) plan with company match.\n• Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting.\n• Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement.\n• Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart.\n\nAbout Walmart:\n\nAt Walmart Global Tech, we innovate to simplify the retail experience, ensuring technology benefits millions of customers and supports associates worldwide. Our culture emphasizes inclusion, collaboration, and continuous learning. We empower teams to deliver meaningful solutions while fostering a flexible virtual-first work environment that encourages creativity, ownership, and professional growth. Being human-led is at the heart of every innovation we drive.\n\nFAQ:\n\nQ: What does a Walmart Data Engineer do?\n\nA: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making.\n\nQ: What skills are needed for a Data Engineer?\n\nA: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities.\n\nQ: What level is a Data Engineer?\n\nA: This is typically a mid-level technical role requiring experience in data engineering and analytics.\n\nQ: How can someone be successful as a Data Engineer?\n\nA: Success comes from building efficient data pipelines, ensuring data quality, collaborating with teams, and supporting business insights.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771200000,
    "job_posted_at_datetime_utc": "2026-02-16T00:00:00.000Z",
    "job_location": "Cedar Hill, TX",
    "job_city": "Cedar Hill",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.588468899999995,
    "job_longitude": -96.9561152,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DA4Vxqpb6-fnOS8TTAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Required Skills and Experience of Walmart Data Engineer:",
        "7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role",
        "Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem)",
        "Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming)",
        "Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI)",
        "Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale",
        "Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning",
        "Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design",
        "Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience",
        "4 years’ experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "A: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities",
        "A: This is typically a mid-level technical role requiring experience in data engineering and analytics"
      ],
      "Benefits": [
        "Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses",
        "Stock options and participation in Walmart’s stock purchase plan",
        "Health benefits including medical, vision, and dental coverage",
        "401(k) plan with company match",
        "Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting",
        "Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement",
        "Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart"
      ],
      "Responsibilities": [
        "You will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously",
        "You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning",
        "Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows",
        "Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state",
        "Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access",
        "Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations",
        "Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals",
        "Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources",
        "Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows",
        "A: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "walmart-rightspotway-com-jobpage-obycpz8rgf19-ae0-382711d8bea237d360feb0-b3596fd",
    "_source": "new_jobs"
  },
  {
    "job_id": "pqN0Jk8pV2qn3GybAAAAAA==",
    "job_title": "ML/AI Data Engineer Remote 6",
    "employer_name": "FEI Systems",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQbt_MPchQI_pacSjmp85Z_7HTs3DdqsjS5fR5D&s=0",
    "employer_website": "https://feisystems.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-co-all-cities-ml-ai-data-engineer-remote-6-fei-systems-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-co-all-cities-ml-ai-data-engineer-remote-6-fei-systems-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At FEI Systems, we create innovative technology solutions to improve the delivery of health and human services because we know when cumbersome administrative processes stand in the way, those who need it most are often left without access to proper care and support. From comprehensive case management software to disaster recovery services and content management information systems used in delivering foreign aid, our solutions are improving the lives of millions of people. We're looking for a data engineer who shares our commitment to leveraging technology to make a real impact in the world - a professional who knows, beyond all else, that the quality of our products and services is only as good as the company we keep.\n\nRole Overview\n\nWe are seeking a Data Engineer to support and execute enterprise Machine Learning and Artificial Intelligence initiatives. This role is a hands-on, tactical execution position focused on building, operating, and maintaining the data pipelines and data foundations required for ML/AI solutions.\n\nWorking closely with the ML/AI Architect, data scientists, and application engineering teams, this role is responsible for ensuring that data within our AWS and Snowflake-based data lake is high quality, well-governed, feature-ready, and production-grade to support model training, deployment, and ongoing operations.\n\nPrimary ResponsibilitiesData Pipeline Engineering\n• Design, build, and maintain scalable data pipelines to support ML/AI workloads\n• Ingest data from multiple sources into the Snowflake data lake using batch and streaming patterns\n• Develop and maintain ELT pipelines leveraging Snowflake-native capabilities\n• Ensure pipelines are reliable, performant, and production-ready\nSnowflake Data Engineering & Transformation\n• Perform data transformations directly in Snowflake using SQL and Snowflake features\n• Design and optimize schemas, tables, views, and materialized views for ML/AI consumption\n• Implement transformation logic supporting analytics, feature engineering, and model training\n• Optimize Snowflake usage for performance and cost efficiency\nData Quality, Governance & Management\n• Implement data quality checks, validation rules, and monitoring within pipelines and Snowflake\n• Support data governance initiatives including metadata management, lineage, and access controls\n• Ensure datasets adhere to enterprise standards for security, privacy, and compliance\n• Identify, troubleshoot, and remediate data quality issues impacting ML/AI workflows\nFeature Engineering & Data Preparation\n• Perform data cleansing, normalization, and enrichment to support ML model development\n• Design and implement feature engineering pipelines, including feature aggregation and transformation\n• Ensure consistency, reuse, and versioning of features across models and use cases\n• Collaborate with ML engineers and data scientists to operationalize features from Snowflake into training pipelines\nModel Training & Execution Support\n• Support and execute model training workflows, including dataset preparation and refreshes\n• Automate data preparation steps for experimentation, retraining, and scheduled runs\n• Ensure training datasets and features are reproducible, traceable, and auditable\nMLOps & SDLC Integration\n• Integrate data pipelines and Snowflake transformations into CI/CD workflows\n• Support version control, testing, and deployment of data assets\n• Monitor pipeline health, data freshness, and downstream impacts on ML/AI systems\n• Partner with platform, ML, and DevOps teams to improve operational maturity\nRequired Technical SkillsData Engineering & Snowflake\n• Strong proficiency in Python for data processing and pipeline development\n• Advanced SQL skills, with hands-on experience transforming data in Snowflake\n• Experience designing ELT pipelines using Snowflake as the central data lake\n• Understanding of Snowflake performance tuning and cost optimization concepts\nCloud & AWS\n• Experience working within the AWS ecosystem, including services such as:\n• S3, Glue, Athena\n• Lambda, Step Functions\n• Kinesis, Snowpipe or MSK (preferred)\n• Experience integrating Snowflake with AWS-based ingestion and processing pipelines\n• Exposure to Amazon SageMaker data preparation and training workflows\nML/AI Data Foundations\n• Understanding of data requirements for machine learning and AI workloads\n• Experience preparing training datasets and features from enterprise data lakes\n• Familiarity with reproducibility, dataset versioning, and data lineage concepts\nDevOps & Engineering Practices\n• Experience operating within a structured SDLC\n• Familiarity with CI/CD pipelines for data and ML workflows\n• Understanding of API-based and event-driven data integration patterns\n• Experience supporting distributed data processing environments\nRequired Education/Certification\n• Bachelor's degree in Computer Science, Machine Learning, Artificial Intelligence, or related field\nPreferred Qualifications\n• Experience supporting ML/AI platforms or products in production\n• Familiarity with feature stores and ML data management tools\n• Exposure to data observability, quality, and monitoring solutions\n• Experience working in governance-heavy or regulated environments\n• Snowflake or AWS certifications (preferred, not required)\n• Experience leveraging ML/AI in a highly regulated healthcare environment (Understanding of HIPAA, 42CFR Part 2 and other privacy regulations)\nWhat Success Looks Like\n• Reliable, high-quality Snowflake datasets powering ML/AI use cases\n• Well-governed, trusted data foundations for feature engineering and model training\n• Efficient, repeatable data preparation and transformation workflows\n• Reduced friction between data engineering, ML, and application teams\nLocation: Remote\n\nStatus: Full-time position with full company benefits\n\nNOTICE: EO/AA/VEVRAA/Disabled Employer - Federal Contractor. FEI Systems participates in E-Verify, a federal program that enables employers to verify the identity and employment eligibility of all persons hired to work in the United States by providing the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee's Form I-9 to confirm work authorization. For more information on E-Verify, please contact DHS at (888) #.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771286400,
    "job_posted_at_datetime_utc": "2026-02-17T00:00:00.000Z",
    "job_location": "Colorado",
    "job_city": null,
    "job_state": "Colorado",
    "job_country": "US",
    "job_latitude": 39.5500507,
    "job_longitude": -105.78206739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DpqN0Jk8pV2qn3GybAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Required Technical SkillsData Engineering & Snowflake",
        "Strong proficiency in Python for data processing and pipeline development",
        "Advanced SQL skills, with hands-on experience transforming data in Snowflake",
        "Experience designing ELT pipelines using Snowflake as the central data lake",
        "Understanding of Snowflake performance tuning and cost optimization concepts",
        "Cloud & AWS",
        "Experience working within the AWS ecosystem, including services such as:",
        "S3, Glue, Athena",
        "Experience integrating Snowflake with AWS-based ingestion and processing pipelines",
        "Exposure to Amazon SageMaker data preparation and training workflows",
        "ML/AI Data Foundations",
        "Understanding of data requirements for machine learning and AI workloads",
        "Experience preparing training datasets and features from enterprise data lakes",
        "Familiarity with reproducibility, dataset versioning, and data lineage concepts",
        "DevOps & Engineering Practices",
        "Experience operating within a structured SDLC",
        "Familiarity with CI/CD pipelines for data and ML workflows",
        "Required Education/Certification",
        "Bachelor's degree in Computer Science, Machine Learning, Artificial Intelligence, or related field",
        "Reliable, high-quality Snowflake datasets powering ML/AI use cases",
        "Well-governed, trusted data foundations for feature engineering and model training",
        "Efficient, repeatable data preparation and transformation workflows",
        "Reduced friction between data engineering, ML, and application teams"
      ],
      "Benefits": [
        "Status: Full-time position with full company benefits"
      ],
      "Responsibilities": [
        "We are seeking a Data Engineer to support and execute enterprise Machine Learning and Artificial Intelligence initiatives",
        "This role is a hands-on, tactical execution position focused on building, operating, and maintaining the data pipelines and data foundations required for ML/AI solutions",
        "Working closely with the ML/AI Architect, data scientists, and application engineering teams, this role is responsible for ensuring that data within our AWS and Snowflake-based data lake is high quality, well-governed, feature-ready, and production-grade to support model training, deployment, and ongoing operations",
        "Primary ResponsibilitiesData Pipeline Engineering",
        "Design, build, and maintain scalable data pipelines to support ML/AI workloads",
        "Ingest data from multiple sources into the Snowflake data lake using batch and streaming patterns",
        "Develop and maintain ELT pipelines leveraging Snowflake-native capabilities",
        "Ensure pipelines are reliable, performant, and production-ready",
        "Snowflake Data Engineering & Transformation",
        "Perform data transformations directly in Snowflake using SQL and Snowflake features",
        "Design and optimize schemas, tables, views, and materialized views for ML/AI consumption",
        "Implement transformation logic supporting analytics, feature engineering, and model training",
        "Optimize Snowflake usage for performance and cost efficiency",
        "Data Quality, Governance & Management",
        "Implement data quality checks, validation rules, and monitoring within pipelines and Snowflake",
        "Support data governance initiatives including metadata management, lineage, and access controls",
        "Ensure datasets adhere to enterprise standards for security, privacy, and compliance",
        "Identify, troubleshoot, and remediate data quality issues impacting ML/AI workflows",
        "Feature Engineering & Data Preparation",
        "Perform data cleansing, normalization, and enrichment to support ML model development",
        "Design and implement feature engineering pipelines, including feature aggregation and transformation",
        "Ensure consistency, reuse, and versioning of features across models and use cases",
        "Collaborate with ML engineers and data scientists to operationalize features from Snowflake into training pipelines",
        "Model Training & Execution Support",
        "Support and execute model training workflows, including dataset preparation and refreshes",
        "Automate data preparation steps for experimentation, retraining, and scheduled runs",
        "Ensure training datasets and features are reproducible, traceable, and auditable",
        "MLOps & SDLC Integration",
        "Integrate data pipelines and Snowflake transformations into CI/CD workflows",
        "Support version control, testing, and deployment of data assets",
        "Monitor pipeline health, data freshness, and downstream impacts on ML/AI systems",
        "Partner with platform, ML, and DevOps teams to improve operational maturity",
        "Lambda, Step Functions",
        "Understanding of API-based and event-driven data integration patterns",
        "Experience supporting distributed data processing environments"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "www-jobilize-com-job-us-co-all-cities-ml-ai-data-engineer-remote-6-fei-systems-hiring-now",
    "_source": "new_jobs"
  },
  {
    "job_id": "1Eb8GME5F6h5wzAiAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Rural King",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSI99A-RNW3mNS-GYEPIGd0U4Qdfm6662lvJ7uw&s=0",
    "employer_website": "https://www.ruralking.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Rural-King-Supply/Job/Data-Engineer/-in-Mattoon,IL?jid=4c03fb6a091c6a1e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-rural-king-4371246007?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/98942441a7db69bf5fff859d1f9da2f3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/data-engineer_7ea1a64fa7b5bb1f8b19872d37b34c2da2d7e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/data-engineer-mattoon-il--e8d0d5a5-70ea-4eed-87dd-58b13c57a77d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Job Listings - ICIMS",
        "apply_link": "https://careers-ruralking.icims.com/jobs/35283/data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Nexxt",
        "apply_link": "https://www.nexxt.com/jobs/data-engineer-mattoon-il-3161318692-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About us\n\nRural King is America's Farm and Home Store, providing essentials to the communities we serve. With a wide array of necessities ranging from food and feed to farm and home products, Rural King serves over 130 locations across 13 states and is constantly expanding. Our annual sales exceed $2.5 Billion, and our heart beats in Mattoon, IL, home to our corporate office, distribution center, and flagship store.\n\nOne thing our customers appreciate is our unique shopping experience, complete with complimentary popcorn and coffee. It's just one way we show our appreciation for their support.\n\nAt Rural King, we value our associates and strive to create a positive, rewarding workplace. We offer growth opportunities, competitive benefits, and a people-first environment where dedicated individuals come together to serve rural communities passionately. Join us, and you'll find not just a job but a chance to grow professionally, contribute meaningfully, and make a difference in the lives of those we serve.\n\nHow we reward you\n\n401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%\n\nHealthcare plans to support your needs\n\nVirtual doctor visits\n\nAccess to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program\n\n15% Associate Discount\n\nDave Ramsey's SmartDollar Program\n\nAssociate Assistance Program\n\nRK Cares Associate Hardship Program\n\n24/7 Chaplaincy Services\n\nCompany paid YMCA Family Membership\n\nWhat You'll do\n\nAs a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization.\n• Design, build, maintain, and manage the systems that move data efficiently within the organization.\n• Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake.\n• Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs\n• Monitor the performance of data pipelines that are efficient and secure\n• Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism.\n• Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments.\n• Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively.\n• Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement.\n• Perform other duties as assigned.\n\nSupervisory Responsibilities\n\nNone\n\nEssential Qualities for Success\n• At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education.\n• Information Technology Engineering experience preferred.\n• Experience as a Data Engineer or similar software engineering role.\n• Proficient with git.\n• Good knowledge of SQL and PHP or similar languages.\n• Good knowledge of big data technologies.\n• Good knowledge of data modeling.\n• Familiar with Looker, Power BI or other BI tools.\n• Working knowledge of databases, MySQL/MariaDB, and SQL.\n• Strong understanding of retail business practices.\n• Excellent negotiation and conflict resolution skills.\n• Demonstrated ability to adapt in a fast-paced environment.\n• Strong analytical and problem-solving skills.\n• Excellent organizational skills and attention to detail.\n• Demonstrated behaviors must reflect integrity, professionalism, and confidentiality.\n\nPhysical Requirements\n• Ability to maintain a seated or standing position for extended durations.\n• Capability to lift 15 pounds periodically.\n• Able to navigate and access all facilities.\n• Skill to effectively communicate verbally with others, both in-person and via electronic devices.\n• Close vision for computer-related tasks.\n\nReasonable accommodations may be made to enable individuals with disabilities to perform essential job functions.\n\nThe pay range for this position is $55,000 - $65,000 annualized and is bonus eligible. Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs. To learn more about our benefits, review here https://acrobat.adobe.com/id/urn:aaid:sc:VA6C2:14539c15-191a-4b77-9c13-f6ccfce10094.\n\nResponsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization. - Design, build, maintain, and manage the systems that move data efficiently within the organization. - Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake. - Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism. - Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments. - Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively. - Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement. - Perform other duties as assigned. Supervisory Responsibilities None",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Mattoon, IL",
    "job_city": "Mattoon",
    "job_state": "Illinois",
    "job_country": "US",
    "job_latitude": 39.4830897,
    "job_longitude": -88.37282549999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D1Eb8GME5F6h5wzAiAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55000,
    "job_max_salary": 65000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education",
        "Experience as a Data Engineer or similar software engineering role",
        "Proficient with git",
        "Good knowledge of SQL and PHP or similar languages",
        "Good knowledge of big data technologies",
        "Good knowledge of data modeling",
        "Familiar with Looker, Power BI or other BI tools",
        "Working knowledge of databases, MySQL/MariaDB, and SQL",
        "Strong understanding of retail business practices",
        "Excellent negotiation and conflict resolution skills",
        "Demonstrated ability to adapt in a fast-paced environment",
        "Strong analytical and problem-solving skills",
        "Excellent organizational skills and attention to detail",
        "Demonstrated behaviors must reflect integrity, professionalism, and confidentiality",
        "Ability to maintain a seated or standing position for extended durations",
        "Capability to lift 15 pounds periodically",
        "Able to navigate and access all facilities",
        "Skill to effectively communicate verbally with others, both in-person and via electronic devices",
        "Close vision for computer-related tasks"
      ],
      "Benefits": [
        "401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%",
        "Healthcare plans to support your needs",
        "Access to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program",
        "15% Associate Discount",
        "Dave Ramsey's SmartDollar Program",
        "Associate Assistance Program",
        "RK Cares Associate Hardship Program",
        "24/7 Chaplaincy Services",
        "The pay range for this position is $55,000 - $65,000 annualized and is bonus eligible",
        "Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs"
      ],
      "Responsibilities": [
        "As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs",
        "Monitor the performance of data pipelines that are efficient and secure",
        "Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned",
        "Responsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d",
    "_source": "new_jobs"
  },
  {
    "job_id": "fKq613YVZoGaO3ZhAAAAAA==",
    "job_title": "Temporary  Student Data Engineer",
    "employer_name": "University of Notre Dame",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB_bk0V1eXoFSLkXbpRAL9w5OPDz55ACjhxHof&s=0",
    "employer_website": null,
    "job_publisher": "BMES Career Center",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BMES Career Center",
        "apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/5e385fd85cc450ceae1fd6846215958e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/university-of-notre-dame/notre-dame-in/student-data-engineer-temporary/b56cc5f7b3f351a3edb50ec87faf8e52?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/temporary-student-data-engineer--notre-dame--e272bff6f756c6e91c32e5a9ffa44cbfd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido",
        "apply_link": "https://us.jobrapido.com/jobpreview/3091304044883345408?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/indiana/business/4868026961/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-notre-dame-temporary-student-data-engineer-university-part_time?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temporary Student Data Engineer\n\nNotre Dame, IN, United States\nContract\nProvost\nTemporary\n\nCompany Description\n\nJob Description\nWe are seeking a Student Data Engineer (SDE) to join a team of students creating a Roblox game as a data collection tool, gauging students' interest in STEM and health care careers. The Lucy Family Institute for Data & Society (LFIDS) leverages data science, AI & ML toward social good. LFIDS engages with the Notre Dame community & beyond through funded research projects & collaborations, educational workshops, and special events. SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana. In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed.\n\nKey Responsibilities:\n\nWithin assigned projects, this role requires completion of data processing and programming tasks related to:\n• Data collection, management, harvesting, processing, transformation, and visualization;\n• Prototype data processing solutions;\n• Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms.\n\nThe successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners. Other responsibilities may include data analysis and giving presentations to diverse audiences.\n\nAdditional Requirements\n\nIn addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested. For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience.\n\nAdditional Opportunities\n\nStudent data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects. Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately.\n\nThe Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests.\n\nCore Qualities & Expectations\n• Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving\n• Confidentiality: Maintaining confidentiality is required.\n• Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings).\n• Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly. Perfection is not expected-it's okay to take time to learn. What matters is trying your best in each unique circumstance. We're committed to supporting your growth and confidence in the role.\n• Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities. We aim to provide a structure that supports your success.\n• Attention to detail or the ability to follow a set of instructions that we'll co-create and adjust based on your preferred learning and working style.\n• Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer.\n\nQualifications\nWe are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:\n• Data science methods and tools,\n• Software design\n• User experience principles\n\nExperience in an LFIDS area of expertise, like:\n• R and/or Python\n• Events and communications support\n• Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites).\n\nAdditional Information\nCompensation: $17.00/hour\n\nApplications for this position will close on February 13, 2026.\n\nThe University of Notre Dame seeks to attract, develop, and retain the highest quality faculty, staff and administration. The University is an Equal Opportunity Employer, and does not discriminate on the basis of race, color, national or ethnic origin, sex, disability, veteran status, genetic information, or age in employment. Moreover, Notre Dame prohibits discrimination against veterans or disabled qualified individuals, and complies with 41 CFR 60-741.5(a) and 41 CFR 60-300.5(a). We strongly encourage applications from candidates attracted to a university with a Catholic identity.\n\nTo apply, visit https://jobs.smartrecruiters.com/UniversityOfNotreDame/3743990011597785-temporary-student-data-engineer\n\nCopyright 2025 Jobelephant.com Inc. All rights reserved.\n\nPosted by the FREE value-added recruitment advertising agency jeid-5691f5d28abbde4e8b71884761a96763",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Notre Dame, IN",
    "job_city": "Notre Dame",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.7001908,
    "job_longitude": -86.2379328,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DfKq613YVZoGaO3ZhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Student data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects",
        "Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving",
        "Confidentiality: Maintaining confidentiality is required",
        "Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings)",
        "Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly",
        "Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities",
        "We aim to provide a structure that supports your success",
        "Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer",
        "We are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:",
        "Data science methods and tools,",
        "Software design",
        "User experience principles",
        "Experience in an LFIDS area of expertise, like:",
        "R and/or Python",
        "Events and communications support",
        "Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites)"
      ],
      "Benefits": [
        "Compensation: $17.00/hour"
      ],
      "Responsibilities": [
        "SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana",
        "In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed",
        "Within assigned projects, this role requires completion of data processing and programming tasks related to:",
        "Data collection, management, harvesting, processing, transformation, and visualization;",
        "Prototype data processing solutions;",
        "Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms",
        "The successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners",
        "Other responsibilities may include data analysis and giving presentations to diverse audiences",
        "In addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested",
        "For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience",
        "Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately",
        "The Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobboard-bmes-org-jobs-22037211-temporary-student-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "TS4IgSLzbYSiKuL6AAAAAA==",
    "job_title": "Lead Data Product Engineer",
    "employer_name": "Disney Experiences",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFU7L5wyZ-tHqOMi8SFhH7dXkKGqRCp8KvYWMN&s=0",
    "employer_website": "https://disneyexperiences.com",
    "job_publisher": "Disney Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.disneycareers.com/en/job/glendale/lead-data-product-engineer/391/87077257008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Disney Careers",
        "apply_link": "https://www.disneycareers.com/en/job/glendale/lead-data-product-engineer/391/87077257008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/be06c1592ceaecdc13ab0eda3d1dad63?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talents By Vaia",
        "apply_link": "https://talents.vaia.com/companies/disneyland-hong-kong/lead-data-engineer-38583087/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Swooped",
        "apply_link": "https://swooped.co/job-postings/lead-data-engineer-glendale-disney-media-entertainment-distribution-e47e7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GetHiredToday - Women For Hire",
        "apply_link": "https://jobs.womenforhire.com/job/usa/glendale-ca/lead-data-product-engineer-600741/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5630484438?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobMonkey Jobs",
        "apply_link": "https://www.jobmonkeyjobs.com/career/27443459/Lead-Data-Product-Engineer-California-Glendale-7308?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Bandana.com",
        "apply_link": "https://bandana.com/jobs/e1d2fbef-0671-4ce1-8d17-98815dd76276?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Disney Experiences Technology, our team creates world-class immersive digital experiences for the Company’s premier vacation brands including Disney’s Parks & Resorts worldwide, Disney Cruise Line, Aulani, A Disney Resort & Spa, and Disney Vacation Club. The Disney Experiences Technology team is responsible for the end-to-end digital and physical Guest experience for all technology & digital-led initiatives across the Attractions & Entertainment, Food & Beverage, Resorts & Transportation, and Merchandise lines of business as well as other initiatives including the MyDisneyExperience app and Hey, Disney!\n\nIf you’re passionate about designing and engineering complex, large-scale data products, come join us! We’re seeking a Lead Data Product Engineer to architect, build, and optimize enterprise-grade data solutions. You’ll own the full lifecycle of data products, from ingestion and transformation to architecture and enablement, while ensuring reliability and scalability.\n\nAs a technical lead, you’ll shape data product design, define contracts and schemas, and partner closely with data source owners, engineers, architects, and end-user teams. This is not a pure coding role, instead, you will engineer the product layer of data, designing assets, writing queries, and translating complex source data into consumable products that data engineers implement and scale.\n\nYou will sit within Data Products & Platforms for Disney Experiences, collaborating with internal teams and technical leaders across the company.\n\nYou Will\n• Architect and deliver enterprise data models, schemas, and contracts to support multi-source ingestion, transformation, and consumption at scale.\n• Own the full product lifecycle for data domains, including design, versioning, governance, and deprecation.\n• Define and evolve technical strategies for high-volume operational and analytical workloads.\n• Act as the data and technical subject matter expert, covering not only your domain and products but also the enablement capabilities of our modern data ecosystem with partners.\n• Partner with architects and data engineers to design and implement scalable, secure, high-throughput pipelines and consumption-ready data products.\n• Translate business requirements into technical specifications and engineering deliverables.\n• Perform deep reviews and gap assessments of existing data flows, modernizing legacy pipelines into modern cloud-native ecosystems.\n• Manage technical acceptance criteria, SLAs, timelines, and cost controls for data products.\n• Represent the data product engineering function in executive forums, advocating for technical excellence and data-driven strategy.\n\nYou Have\n• 7+ years designing and delivering large-scale data products, with proven experience leading data solutions and/or engineering teams.\n• Hands-on expertise in data product management or data engineering, especially in complex business and operational domains.\n• Advanced skills in data modeling and documentation (conceptual, logical, physical), driving solutions end-to-end: discovery to deployed pipelines supporting BI, analytics, and AI/ML.\n• Deep technical fluency with data engineering concepts and platforms (AWS: S3, Lambda, Step Functions, Glue), data platforms (Snowflake), governance (data contracts), transformation and orchestration (dbt, Airflow), and streaming/event technologies (Kinesis, Pub/Sub patterns, Kafka).\n• Strong understanding of batch and real-time architectures, including event-driven and streaming solutions.\n• Proven ability to partner and influence across technical and business teams; clear, concise communicator who can translate complexity.\n• Recognized technical authority in data product engineering practices, defining best-in-class methods and coaching others on process and execution.\n\nRequired Education\n• Bachelor’s degree in Computer Science, Information Systems, or related field—or equivalent professional experience.\n\n#DISNEYTECH\n\nThe hiring range for this position in California is $166,800 - $223,600 per year based on a 40 hour work week. The amount of hours scheduled per week may vary based on business needs. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Glendale, CA",
    "job_city": "Glendale",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.146367399999995,
    "job_longitude": -118.2488703,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DTS4IgSLzbYSiKuL6AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "7+ years designing and delivering large-scale data products, with proven experience leading data solutions and/or engineering teams",
        "Hands-on expertise in data product management or data engineering, especially in complex business and operational domains",
        "Advanced skills in data modeling and documentation (conceptual, logical, physical), driving solutions end-to-end: discovery to deployed pipelines supporting BI, analytics, and AI/ML",
        "Deep technical fluency with data engineering concepts and platforms (AWS: S3, Lambda, Step Functions, Glue), data platforms (Snowflake), governance (data contracts), transformation and orchestration (dbt, Airflow), and streaming/event technologies (Kinesis, Pub/Sub patterns, Kafka)",
        "Strong understanding of batch and real-time architectures, including event-driven and streaming solutions",
        "Proven ability to partner and influence across technical and business teams; clear, concise communicator who can translate complexity",
        "Recognized technical authority in data product engineering practices, defining best-in-class methods and coaching others on process and execution",
        "Bachelor’s degree in Computer Science, Information Systems, or related field—or equivalent professional experience"
      ],
      "Benefits": [
        "The hiring range for this position in California is $166,800 - $223,600 per year based on a 40 hour work week",
        "The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors",
        "A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered"
      ],
      "Responsibilities": [
        "You’ll own the full lifecycle of data products, from ingestion and transformation to architecture and enablement, while ensuring reliability and scalability",
        "As a technical lead, you’ll shape data product design, define contracts and schemas, and partner closely with data source owners, engineers, architects, and end-user teams",
        "This is not a pure coding role, instead, you will engineer the product layer of data, designing assets, writing queries, and translating complex source data into consumable products that data engineers implement and scale",
        "You will sit within Data Products & Platforms for Disney Experiences, collaborating with internal teams and technical leaders across the company",
        "Architect and deliver enterprise data models, schemas, and contracts to support multi-source ingestion, transformation, and consumption at scale",
        "Own the full product lifecycle for data domains, including design, versioning, governance, and deprecation",
        "Define and evolve technical strategies for high-volume operational and analytical workloads",
        "Act as the data and technical subject matter expert, covering not only your domain and products but also the enablement capabilities of our modern data ecosystem with partners",
        "Partner with architects and data engineers to design and implement scalable, secure, high-throughput pipelines and consumption-ready data products",
        "Translate business requirements into technical specifications and engineering deliverables",
        "Perform deep reviews and gap assessments of existing data flows, modernizing legacy pipelines into modern cloud-native ecosystems",
        "Manage technical acceptance criteria, SLAs, timelines, and cost controls for data products",
        "Represent the data product engineering function in executive forums, advocating for technical excellence and data-driven strategy"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-disneycareers-com-en-job-glendale-lead-data-product-engineer-391-87077257008",
    "_source": "new_jobs"
  },
  {
    "job_id": "8ojyM0IL2Dgweb6mAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Guardianlife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEHyPUSx8in9lbGwtt3n-urbaK-a93llGwRZYg&s=0",
    "employer_website": "https://www.guardianlife.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6984c41c0f6f7e7a2cdf36a2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/guardian-life/holmdel-nj/lead-data-engineer/448a3771cda355807800f3a245f659f2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5618263952?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/lead-data-engineer-holmdel-new-jersey-us-guardian-life-r000108510?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/lead-data-engineer--holmdel-township--e33d7f694afe11a60b979debb2c4bd89a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/40aefed101891767dbcd5f364e470e55?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/holmdel-township/new-jersey/software_development/4858586430/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Guardian is seeking a highly skilled and motivated Lead Data Engineer to join the FPRS&CSWM Data Engineering team. In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases. Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient. The ideal candidate will have a passion for data engineering, thrive in a collaborative environment and are excited about leveraging cutting-edge technologies to drive business success.\n\nYour contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers. You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs. We value curiosity, creativity, and continuous learning. If you're passionate about solving meaningful problems and creating value through data-driven innovation, we look forward to welcoming you to our team.\n\nYou will\n• Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables.\n• Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth.\n• Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products\n• Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems.\n• Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.\n• Construct meaningful data assets sourced from structured, semi structured, and unstructured data.\n• Develop real-time data solutions by creating new API endpoints or streaming frameworks.\n• Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle.\n• Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams.\n• Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication.\n• Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions.\n• Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives.\n• Stay up-to-date with the latest trends in modern data engineering, machine learning & AI.\n\nYou have\n• Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field.\n• 5+ years of experience working with Python, SQL, PySpark, and bash scripts. Proficient in software development lifecycle and software engineering practices.\n• 4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases.\n• 3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark.\n• 2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality.\n• Solid understanding of data modeling and warehousing techniques. Experience working in a data warehouse is a plus.\n• Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities.\n• Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries).\n• Proficient in understanding and incorporating software engineering principles in design & development process.\n• Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent).\n• Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business.\n\nLocation\n• Three days a week at a Guardian office in Bethlehem, PA, New York, NY. Pittsfield, MA or Holmdel, NJ.\n\nSalary Range:\n\n$99,150.00 - $162,885.00\n\nThe salary range reflected above is a good faith estimate of base pay for the primary location of the position. The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate. In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation.\n\nOur Promise\n\nAt Guardian, you'll have the support and flexibility to achieve your professional and personal goals. Through skill-building, leadership development and philanthropic opportunities, we provide opportunities to build communities and grow your career, surrounded by diverse colleagues with high ethical standards.\n\nInspire Well-Being\n\nAs part of Guardian's Purpose - to inspire well-being - we are committed to offering contemporary, supportive, flexible, and inclusive benefits and resources to our colleagues. Explore our company benefits at www.guardianlife.com/careers/corporate/benefits.Benefits apply to full-time eligible employees. Interns are not eligible for most Company benefits.\n\nEqual Employment Opportunity\n\nGuardian is an equal opportunity employer. All qualified applicants will be considered for employment without regard to age, race, color, creed, religion, sex, affectional or sexual orientation, national origin, ancestry, marital status, disability, military or veteran status, or any other classification protected by applicable law.\n\nAccommodations\n\nGuardian is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities.Guardian also provides reasonable accommodations to qualified job applicants (and employees) to accommodate the individual's known limitations related to pregnancy, childbirth, or related medical conditions, unless doing so would create an undue hardship. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact MyHR@glic.com. Please note: this resource is for accommodation requests only. For all other inquires related to your application and careers at Guardian, refer to the Guardian Careers site.\n\nVisa Sponsorship\n\nGuardian is not currently or in the foreseeable future sponsoring employment visas. In order to be a successful applicant. you must be legally authorized to work in the United States, without the need for employer sponsorship.\n\nCurrent Guardian Colleagues: Please apply through the internal Jobs Hub in Workday.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Holmdel, NJ",
    "job_city": "Holmdel",
    "job_state": "New Jersey",
    "job_country": "US",
    "job_latitude": 40.3848944,
    "job_longitude": -74.18900599999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D8ojyM0IL2Dgweb6mAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 99150,
    "job_max_salary": 162885,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "We value curiosity, creativity, and continuous learning",
        "Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field",
        "5+ years of experience working with Python, SQL, PySpark, and bash scripts",
        "Proficient in software development lifecycle and software engineering practices",
        "4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases",
        "3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark",
        "2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality",
        "Solid understanding of data modeling and warehousing techniques",
        "Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities",
        "Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries)",
        "Proficient in understanding and incorporating software engineering principles in design & development process",
        "Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent)",
        "Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business",
        "you must be legally authorized to work in the United States, without the need for employer sponsorship"
      ],
      "Benefits": [
        "$99,150.00 - $162,885.00",
        "The salary range reflected above is a good faith estimate of base pay for the primary location of the position",
        "The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate",
        "In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation",
        "Interns are not eligible for most Company benefits"
      ],
      "Responsibilities": [
        "In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases",
        "Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient",
        "Your contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers",
        "You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs",
        "Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables",
        "Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth",
        "Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products",
        "Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems",
        "Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues",
        "Construct meaningful data assets sourced from structured, semi structured, and unstructured data",
        "Develop real-time data solutions by creating new API endpoints or streaming frameworks",
        "Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle",
        "Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams",
        "Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication",
        "Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions",
        "Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives",
        "Stay up-to-date with the latest trends in modern data engineering, machine learning & AI"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-guardian-life-job-lead-data-engineer-in-holmdel-nj",
    "_source": "new_jobs"
  },
  {
    "job_id": "XQf3XW1gRIsW1UiuAAAAAA==",
    "job_title": "AWS Redshift Data Engineer Data Architect",
    "employer_name": "Numentica LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTootVXWXzkNN4CMEZIpwL0zphlLjGSX6fP41El&s=0",
    "employer_website": "https://numentica.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This is a remote position.\n\nRole : AWS Redshift Data Engineer / Data Architect - Remote\n\nOverview\n\nWe are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture.\n\nKey Responsibilities\n\nOptimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS).\n\nLead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime.\n\nAnalyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads.\n\nReview and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis.\n\nDesign data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency.\n\nCollaborate with stakeholders on architecture recommendations and best practices.\n\nRequired Skills\n\nStrong hands-on experience with Amazon Redshift (Provisioned Serverless RMS).\n\nDeep SQL performance tuning and query optimization expertise.\n\nSolid AWS data engineering background (RDS DynamoDB Kinesis).\n\nExperience with modern lakehouse concepts especially Apache Iceberg.\n\nData architecture and analytical platform design experience.\n\nNice to Have\n\nExposure to AI-assisted query optimization or analytics platforms.\n\nExperience building or supporting BI and analytics use cases at scale.\n\nRequired Skills :\n\nAWS Redshift Data Engineer / Data Architect\n\nKey Skills\n\nFund Management,Drafting,End User Support,Infrastructure,Airlines,Catia\n\nEmployment Type : Full Time\n\nExperience : years\n\nVacancy : 1",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXQf3XW1gRIsW1UiuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture",
        "Strong hands-on experience with Amazon Redshift (Provisioned Serverless RMS)",
        "Deep SQL performance tuning and query optimization expertise",
        "Solid AWS data engineering background (RDS DynamoDB Kinesis)",
        "Experience with modern lakehouse concepts especially Apache Iceberg",
        "Data architecture and analytical platform design experience",
        "Exposure to AI-assisted query optimization or analytics platforms",
        "Experience building or supporting BI and analytics use cases at scale",
        "AWS Redshift Data Engineer / Data Architect"
      ],
      "Responsibilities": [
        "Optimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS)",
        "Lead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime",
        "Analyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads",
        "Review and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis",
        "Design data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency",
        "Collaborate with stakeholders on architecture recommendations and best practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "zXyI7pf5emLDB5aTAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Deloitte",
    "employer_logo": null,
    "employer_website": "https://www.deloitte.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/engineer?id=2451775113&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary\n\nLead Data Engineer\n\nRole Overview: As a Lead Engineer (Data Lifecycle Management), you will take a hands-on role in designing, building, and operating data platform capabilities that govern how data is created, classified, used/shared, retained, archived, and deleted within scope. You will lead delivery of lifecycle controls-such as metadata capture and lineage, data classification and policy enforcement, retention/legal hold workflows, and audit-ready evidence and reporting-embedded directly into modern data stacks (cloud storage, lakehouse/warehouse, and data pipelines). Your expertise will be pivotal in turning governance, privacy, and regulatory requirements into scalable engineering patterns and automation, improving data trust, reducing operational risk, and enabling compliant reuse of data products. The ideal candidate is a hands-on technical leader and mentor who partners closely with data governance, security, privacy, legal, and platform teams to deliver reliable, outcome-driven lifecycle solutions.\n\nRecruiting for this role ends on 5/29/26.\n\nKey Responsibilities:\n\nOutcome-Driven Accountability: Embrace and drive a culture of accountability for customer and business outcomes. Develop engineering solutions that solve complex problems with valuable outcomes, ensuring high-quality, lean designs and implementations.\n\nTechnical Leadership and Advocacy: Serve as the technical advocate for products, ensuring code integrity, feasibility, and alignment with business and customer goals. Lead requirement analysis, contributing to low-level architecture and component design, development, unit testing, integrations, and support.\n\nEngineering Craftsmanship: Maintain accountability for the integrity of code design, implementation, quality, data, and ongoing maintenance and operations. Stay hands-on, self-driven, and continuously learn new approaches, languages, and frameworks. Create technical specifications, and write high-quality, supportable, scalable code and review code of other engineers, mentoring them, to ensure all quality KPIs are met or exceeded. Demonstrate collaborative skills to work effectively with diverse teams.\n\nCustomer-Centric Engineering: Develop lean engineering solutions through rapid, inexpensive experimentation to solve customer needs. Engage with customers and product teams before, during, and after delivery to ensure the right solution is delivered at the right time.\n\nIncremental and Iterative Delivery: Adopt a mindset that favors action and evidence over extensive planning. Utilize a leaning-forward approach to navigate complexity and uncertainty, delivering lean, supportable, and maintainable solutions.\n\nCross-Functional Collaboration and Integration: Work collaboratively with empowered, cross-functional teams including product management, experience, and delivery. Integrate diverse perspectives to make well-informed decisions that balance feasibility, viability, usability, and value. Foster a collaborative environment that enhances team synergy and innovation.\n\nAdvanced Technical Proficiency: Possess deep expertise in modern software engineering practices and principles, including Agile methodologies and DevSecOps to deliver daily product deployments using full automation from code check-in to production with all quality checks through SDLC lifecycle. Strive to be a role model, leveraging these techniques to optimize solutioning and product delivery. Demonstrate strong understanding of the full lifecycle product development, focusing on continuous improvement and learning.\n\nDomain Expertise: Quickly acquire domain-specific knowledge relevant to the business or product. Translate business/user needs, architectures, and UX/UI designs into technical specifications and code. Be a valuable, flexible, and dedicated team member, supportive of teammates, and focused on quality and tech debt payoff.\n\nEffective Communication and Influence: Exhibit exceptional communication skills, capable of articulating complex technical concepts clearly and compellingly. Inspire and influence teammates and product teams through well-structured arguments and trade-offs supported by evidence. Create coherent narratives that align technical solutions with business objectives.\n\nEngagement and Collaborative Co-Creation: Engage and collaborate with product engineering teams at all organizational levels, including customers as needed. Build and maintain constructive relationships, fostering a culture of co-creation and shared momentum towards achieving product goals. Align diverse perspectives and drive consensus to create feasible solutions.\n\nThe team: US Deloitte Technology Product Engineering has modernized software and product delivery, creating a scalable, cost-effective model that focuses on value/outcomes that leverages a progressive and responsive talent structure. As Deloitte's primary internal development team, Product Engineering delivers innovative digital solutions to businesses, service lines, and internal operations with proven bottom-line results and outcomes. It helps power Deloitte's success. It is the engine that drives Deloitte, serving many of the world's largest, most respected companies. We develop and deploy cutting-edge internal and go-to-market solutions that help Deloitte operate effectively and lead in the market. Our reputation is built on a tradition of delivering with excellence.\n\nKey Qualifications:\n• A bachelor's degree in computer science, software engineering, or a related discipline. An advanced degree (e.g., MS) is preferred but not required. Experience is the most relevant factor.\n• Excellent software engineering foundation with deep understanding of OOP/OOD, sequence/activity/state/ER/DFD diagrams, data-structure, algorithms, code instrumentations, etc.\n• 8+ years proven experience with Python, SQL/NoSQL.\n• 8+ years of experience with cloud-native engineering and understanding of Azure Data Pipelines/the Azure Portal Environment.\n• 8+ years delivering governed data platforms.\n• 2+ years of experience with AI/ML and GenAI.\n• Strong understanding of methodologies & tools like, XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc. to deliver high quality products rapidly.\n• Deep experience with at least one modern data platform and its governance controls (e.g., Databricks, Snowflake, BigQuery, Redshift, Synapse).\n• Experience with implementing data governance and lifecycle management controls across Microsoft 365 (M365) applications including Teams, OneDrive, SharePoint, CoPilot, etc.\n• Experience with governance tools like Microsoft Purview.\n• Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care.\n• Limited immigration sponsorship may be available\n\nThe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $107,700 to $221,200.\n\nYou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.\n\nInformation for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html\n\nEA_ITS_ExpHire\n\nPXE_JOBS\n\nDeloitte is committed to providing reasonable accommodations for people with disabilities. If you require a reasonable accommodation to participate in the recruiting process, please direct your inquiries to the Global Call Center (GCC) at USTalentCICInbox@deloitte.com.\n\nRecruiting tips\n\nFrom developing a stand out resume to putting your best foot forward in the interview, we want you to feel prepared and confident as you explore opportunities at Deloitte. Check out recruiting tips from Deloitte recruiters.\n\nBenefits\n\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\n\nOur people and culture\n\nOur inclusive culture empowers our people to be who they are, contribute their unique perspectives, and make a difference individually and collectively. It enables us to leverage different ways of thinking, ideas, and perspectives, and bring more creativity and innovation to help solve our clients' most complex challenges. This makes Deloitte one of the most rewarding places to work.\n\nOur purpose\n\nDeloitte's purpose is to make an impact that matters for our people, clients, and communities. At Deloitte, purpose is synonymous with how we work every day. It defines who we are. Our purpose comes through in our work with clients that enables impact and value in their organizations, as well as through our own investments, commitments, and actions across areas that help drive positive outcomes for our communities. Learn more.\n\nProfessional development\n\nFrom entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to build new skills, take on leadership opportunities and connect and grow through mentorship. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.\n\nAs used in this posting, \"Deloitte\" means Deloitte Services LP, a subsidiary of Deloitte LLP. Please see www.deloitte.com/us/about for a detailed description of the legal structure of Deloitte LLP and its subsidiaries.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.\n\nQualified applicants with criminal histories, including arrest or conviction records, will be considered for employment in accordance with the requirements of applicable state and local laws, including the Los Angeles County Fair Chance Ordinance for Employers, City of Los Angeles's Fair Chance Initiative for Hiring Ordinance, San Francisco Fair Chance Ordinance, and the California Fair Chance Act. See notices of various fair chance hiring and ban-the-box laws where available. Fair Chance Hiring and Ban-the-Box Notices | Deloitte US Careers\n\nRequisition code: 323572\n\nJob ID 323572",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771113600,
    "job_posted_at_datetime_utc": "2026-02-15T00:00:00.000Z",
    "job_location": "Williamsville, NY",
    "job_city": "Williamsville",
    "job_state": "New York",
    "job_country": "US",
    "job_latitude": 42.963947,
    "job_longitude": -78.73780909999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzXyI7pf5emLDB5aTAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 108000,
    "job_max_salary": 221000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate is a hands-on technical leader and mentor who partners closely with data governance, security, privacy, legal, and platform teams to deliver reliable, outcome-driven lifecycle solutions",
        "Domain Expertise: Quickly acquire domain-specific knowledge relevant to the business or product",
        "Translate business/user needs, architectures, and UX/UI designs into technical specifications and code",
        "Be a valuable, flexible, and dedicated team member, supportive of teammates, and focused on quality and tech debt payoff",
        "Effective Communication and Influence: Exhibit exceptional communication skills, capable of articulating complex technical concepts clearly and compellingly",
        "A bachelor's degree in computer science, software engineering, or a related discipline",
        "Experience is the most relevant factor",
        "Excellent software engineering foundation with deep understanding of OOP/OOD, sequence/activity/state/ER/DFD diagrams, data-structure, algorithms, code instrumentations, etc",
        "8+ years proven experience with Python, SQL/NoSQL",
        "8+ years of experience with cloud-native engineering and understanding of Azure Data Pipelines/the Azure Portal Environment",
        "8+ years delivering governed data platforms",
        "2+ years of experience with AI/ML and GenAI",
        "Strong understanding of methodologies & tools like, XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc",
        "Deep experience with at least one modern data platform and its governance controls (e.g., Databricks, Snowflake, BigQuery, Redshift, Synapse)",
        "Experience with implementing data governance and lifecycle management controls across Microsoft 365 (M365) applications including Teams, OneDrive, SharePoint, CoPilot, etc",
        "Experience with governance tools like Microsoft Purview",
        "Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care",
        "Limited immigration sponsorship may be available"
      ],
      "Benefits": [
        "The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs",
        "A reasonable estimate of the current range is $107,700 to $221,200",
        "You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance"
      ],
      "Responsibilities": [
        "Role Overview: As a Lead Engineer (Data Lifecycle Management), you will take a hands-on role in designing, building, and operating data platform capabilities that govern how data is created, classified, used/shared, retained, archived, and deleted within scope",
        "You will lead delivery of lifecycle controls-such as metadata capture and lineage, data classification and policy enforcement, retention/legal hold workflows, and audit-ready evidence and reporting-embedded directly into modern data stacks (cloud storage, lakehouse/warehouse, and data pipelines)",
        "Your expertise will be pivotal in turning governance, privacy, and regulatory requirements into scalable engineering patterns and automation, improving data trust, reducing operational risk, and enabling compliant reuse of data products",
        "Outcome-Driven Accountability: Embrace and drive a culture of accountability for customer and business outcomes",
        "Develop engineering solutions that solve complex problems with valuable outcomes, ensuring high-quality, lean designs and implementations",
        "Technical Leadership and Advocacy: Serve as the technical advocate for products, ensuring code integrity, feasibility, and alignment with business and customer goals",
        "Lead requirement analysis, contributing to low-level architecture and component design, development, unit testing, integrations, and support",
        "Engineering Craftsmanship: Maintain accountability for the integrity of code design, implementation, quality, data, and ongoing maintenance and operations",
        "Stay hands-on, self-driven, and continuously learn new approaches, languages, and frameworks",
        "Create technical specifications, and write high-quality, supportable, scalable code and review code of other engineers, mentoring them, to ensure all quality KPIs are met or exceeded",
        "Demonstrate collaborative skills to work effectively with diverse teams",
        "Customer-Centric Engineering: Develop lean engineering solutions through rapid, inexpensive experimentation to solve customer needs",
        "Engage with customers and product teams before, during, and after delivery to ensure the right solution is delivered at the right time",
        "Incremental and Iterative Delivery: Adopt a mindset that favors action and evidence over extensive planning",
        "Utilize a leaning-forward approach to navigate complexity and uncertainty, delivering lean, supportable, and maintainable solutions",
        "Cross-Functional Collaboration and Integration: Work collaboratively with empowered, cross-functional teams including product management, experience, and delivery",
        "Integrate diverse perspectives to make well-informed decisions that balance feasibility, viability, usability, and value",
        "Foster a collaborative environment that enhances team synergy and innovation",
        "Advanced Technical Proficiency: Possess deep expertise in modern software engineering practices and principles, including Agile methodologies and DevSecOps to deliver daily product deployments using full automation from code check-in to production with all quality checks through SDLC lifecycle",
        "Strive to be a role model, leveraging these techniques to optimize solutioning and product delivery",
        "Demonstrate strong understanding of the full lifecycle product development, focusing on continuous improvement and learning",
        "Inspire and influence teammates and product teams through well-structured arguments and trade-offs supported by evidence",
        "Create coherent narratives that align technical solutions with business objectives",
        "Engagement and Collaborative Co-Creation: Engage and collaborate with product engineering teams at all organizational levels, including customers as needed",
        "Build and maintain constructive relationships, fostering a culture of co-creation and shared momentum towards achieving product goals",
        "Align diverse perspectives and drive consensus to create feasible solutions",
        "to deliver high quality products rapidly"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02",
    "_source": "new_jobs"
  },
  {
    "job_id": "J3qD7yLhFTV7L6odAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Slalom",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTz83P7TkZM86wEdE65TUA9A-yqu267uumzStA3&s=0",
    "employer_website": "https://www.slalom.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-slalom-4369144916?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-slalom-4369144916?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-engineer?id=2439287257&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Web Hosting Services",
        "apply_link": "https://perris.id.au/.career/job/data-engineer-at-procore-technologies-oregon-T2VtelRkR25vSlpRc1c4b1hhMVIxdz09?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Role: Data Engineer\n\nWho You’ll Work With\n\nAt Slalom we co-create custom software, data and cloud products with clients who are ready to accelerate their digital transformation. We're passionate about technology, compelled by its potential as we help create the digital products, experiences, and technology-driven organizations that drive true change. We’re thrilled by the opportunity to build the future we want to see, with anyone willing to join us.\n\nSlalom's Data Engineering Discipline Is Focused On Injecting Intelligence Into Products, Engineering Systems That Support Learning And Insight And Creating Innovative Data Products. Within Data Engineering We Help Customers Build World-class Products Through Effective Use Of\n• Data engineering consisting of streaming / real-time data solutions, modern data platforms and data systems within products (i.e., database systems, graph databases, key-value stores, document databases and transactional systems)\n• Enhancing Machine Learning and Artificial Intelligence capabilities\n• You will work collaboratively with teams in a hybrid environment, with expectation to be in-person with Slalom team members and clients as needed.\n• You also must be within commutable distance to one of the listed Slalom office locations for this role.\n\nWhat You’ll Do\n\nSlalom Data Engineering discipline is comprised of passionate, flexible technologists who love to practice and hone their craft. As tools evolve and technologies emerge, we work to stay in front of innovations in data platform development and delivery.\n\nAs a Data Engineer for Slalom, you will work in collaborative teams to deliver innovative solutions on Amazon Web Services, Microsoft Azure, and Google Cloud Platform using core cloud data warehouse tools, distributed processing engines, event streaming platforms, and other modern data technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.\n\nYou will be engaged to participate in design sessions and be responsible for the timely completion of development items assigned to you in a project backlog.\n\nWhat You’ll Bring\n\nYou will have an interest to become the best at what you do and will have many opportunities to gain hands-on experience with new data platforms and programming languages as you explore the range of technologies that we help our clients with including:\n• Big Data Platforms (Apache Spark, Presto, Amazon EMR)\n• Cloud Data Warehouses (Amazon Redshift, Snowflake, Google BigQuery)\n• Object Oriented Coding (Java, Python)\n• NoSQL Databases (DynamoDB, Cosmos DB, MongoDB)\n• Container Management Systems (Kubernetes, Amazon ECS)\n• Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)\n• Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)\n• Visual Analytics (Tableau, PowerBI)\n• Modern Data Workflows (Apache Airflow, dbt, Dagster)\n\nAbout Us\n\nSlalom is a fiercely human business and technology consulting company that leads with outcomes to bring more value, in all ways, always. From strategy through delivery, our agile teams across 52 offices in 12 countries collaborate with clients to bring powerful customer experiences, innovative ways of working, and new products and services to life. We are trusted by leaders across the Global 1000, many successful enterprise and mid-market companies, and 500+ public sector organizations to improve operations, drive growth, and create value. At Slalom, we believe that together, we can move faster, dream bigger, and build better tomorrows for all.\n\nCompensation And Benefits\n\nSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses, as well as discounted home, auto, and pet insurance.\n\nSlalom is committed to fair and equitable compensation practices. For this role, we are hiring at the following levels and targeted base pay salary ranges outlined below. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.\n• Denver, Portland\n• Consultant: $91,000 - $122,000\n• Seattle\n• Consultant: $99,000 - $133,000\n\nWe will accept applicants until April 5th, 2026, or until the position is filled.\n\nWe are committed to pay transparency and compliance with applicable laws. If you have questions or concerns about the pay range or other compensation information in this posting, please contact us at: peopleone@slalom.com.\n\nEEO and Accommodations\n\nSlalom is an equal opportunity employer and is committed to attracting, developing and retaining highly qualified talent who empower our innovative teams through unique perspectives and experiences. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team or contact accomodationrequest@slalom.com if you require accommodations during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "Oregon City, OR",
    "job_city": "Oregon City",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.3556099,
    "job_longitude": -122.605853,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DJ3qD7yLhFTV7L6odAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Big Data Platforms (Apache Spark, Presto, Amazon EMR)",
        "NoSQL Databases (DynamoDB, Cosmos DB, MongoDB)",
        "Modern Data Workflows (Apache Airflow, dbt, Dagster)",
        "Slalom will also consider qualified applications with criminal histories, consistent with legal requirements"
      ],
      "Benefits": [
        "Compensation And Benefits",
        "Slalom prides itself on helping team members thrive in their work and life",
        "As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability",
        "We also offer yearly $350 reimbursement account for any well-being-related expenses, as well as discounted home, auto, and pet insurance",
        "Slalom is committed to fair and equitable compensation practices",
        "For this role, we are hiring at the following levels and targeted base pay salary ranges outlined below",
        "In addition, individuals may be eligible for an annual discretionary bonus",
        "Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors",
        "The salary pay range is subject to change and may be modified at any time",
        "Consultant: $99,000 - $133,000"
      ],
      "Responsibilities": [
        "Data engineering consisting of streaming / real-time data solutions, modern data platforms and data systems within products (i.e., database systems, graph databases, key-value stores, document databases and transactional systems)",
        "Enhancing Machine Learning and Artificial Intelligence capabilities",
        "You will work collaboratively with teams in a hybrid environment, with expectation to be in-person with Slalom team members and clients as needed",
        "You also must be within commutable distance to one of the listed Slalom office locations for this role",
        "Slalom Data Engineering discipline is comprised of passionate, flexible technologists who love to practice and hone their craft",
        "As a Data Engineer for Slalom, you will work in collaborative teams to deliver innovative solutions on Amazon Web Services, Microsoft Azure, and Google Cloud Platform using core cloud data warehouse tools, distributed processing engines, event streaming platforms, and other modern data technologies",
        "In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics",
        "You will be engaged to participate in design sessions and be responsible for the timely completion of development items assigned to you in a project backlog",
        "You will have an interest to become the best at what you do and will have many opportunities to gain hands-on experience with new data platforms and programming languages as you explore the range of technologies that we help our clients with including:",
        "Cloud Data Warehouses (Amazon Redshift, Snowflake, Google BigQuery)",
        "Object Oriented Coding (Java, Python)",
        "Container Management Systems (Kubernetes, Amazon ECS)",
        "Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)",
        "Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)",
        "Visual Analytics (Tableau, PowerBI)"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-data-engineer-at-slalom-4369144916",
    "_source": "new_jobs"
  },
  {
    "job_id": "_fJ4esw2CSBo0axNAAAAAA==",
    "job_title": "Google Cloud Platform Data Engineer (Locals Only)",
    "employer_name": "Jobs via Dice",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/4RAiIOB4Wvohn4qafdPor?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/4RAiIOB4Wvohn4qafdPor?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Overview\n\nDice is the leading career destination for tech experts. Our client, VeridianTech, is seeking a Google Cloud Platform Data Engineer to join their team in Mountain View, CA (Onsite) for a duration of 12+ months. In this role, you’ll develop and enhance Python frameworks and design robust data pipelines that power advanced data processing, quality, and machine learning operations. Apply via Dice today!\n\nKey Responsibilities\n• Develop and enhance Python frameworks and libraries to support data processing, quality, lineage, governance, analysis, and machine learning operations.\n• Design, build, and maintain scalable and efficient data pipelines on Google Cloud Platform.\n• Implement robust monitoring, logging, and alerting systems to ensure the reliability and stability of data infrastructure.\n• Build scalable batch pipelines leveraging BigQuery, Dataflow and Airflow/Composer scheduler/executor framework on Google Cloud Platform.\n• Build data pipelines leveraging Scala, Pub/Sub, Akka, and Dataflow on Google Cloud Platform.\n• Design data models for optimal storage and retrieval to support machine learning modeling using technologies like Bigtable and Vertex Feature Store.\n• Contribute to shared Data Engineering tooling and standards to improve productivity and quality for the team.\n\nRequired Qualifications\n• Python Expertise: Write and maintain Python frameworks and libraries to support data processing and integration tasks.\n• Code Management: Use Git and GitHub for source control, code reviews, and version management.\n• Google Cloud Platform Proficiency: Extensive experience working with GCP services (e.g., BigQuery, Cloud Dataflow, Pub/Sub, Cloud Storage).\n• Python Mastery: Proficient in Python with experience in optimizing data processing frameworks and libraries.\n• Software Engineering: Strong understanding of best practices including version control, collaborative development, code reviews, and CI/CD.\n• Data Management: Deep knowledge of data modeling, ETL/ELT, and data warehousing concepts.\n• Problem-Solving: Excellent problem-solving skills with the ability to tackle complex data engineering challenges.\n• Communication: Ability to explain complex technical details to non-technical stakeholders.\n• Data Science Stack: Proficiency in data analysis with tools such as Jupyter Notebook, pandas, and NumPy.\n• Frameworks/Tools: Familiarity with machine learning and data processing tools such as TensorFlow, Apache Spark, and scikit-learn.\n• Education: Bachelor’s or master’s degree in Computer Science, Engineering, Computer Information Systems, Mathematics, Physics, or a related field, or equivalent software development training.",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771286400,
    "job_posted_at_datetime_utc": "2026-02-17T00:00:00.000Z",
    "job_location": "Mountain View, CA",
    "job_city": "Mountain View",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.390026399999996,
    "job_longitude": -122.0812304,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_fJ4esw2CSBo0axNAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Python Expertise: Write and maintain Python frameworks and libraries to support data processing and integration tasks",
        "Code Management: Use Git and GitHub for source control, code reviews, and version management",
        "Google Cloud Platform Proficiency: Extensive experience working with GCP services (e.g., BigQuery, Cloud Dataflow, Pub/Sub, Cloud Storage)",
        "Python Mastery: Proficient in Python with experience in optimizing data processing frameworks and libraries",
        "Software Engineering: Strong understanding of best practices including version control, collaborative development, code reviews, and CI/CD",
        "Data Management: Deep knowledge of data modeling, ETL/ELT, and data warehousing concepts",
        "Problem-Solving: Excellent problem-solving skills with the ability to tackle complex data engineering challenges",
        "Communication: Ability to explain complex technical details to non-technical stakeholders",
        "Data Science Stack: Proficiency in data analysis with tools such as Jupyter Notebook, pandas, and NumPy",
        "Frameworks/Tools: Familiarity with machine learning and data processing tools such as TensorFlow, Apache Spark, and scikit-learn",
        "Education: Bachelor’s or master’s degree in Computer Science, Engineering, Computer Information Systems, Mathematics, Physics, or a related field, or equivalent software development training"
      ],
      "Responsibilities": [
        "In this role, you’ll develop and enhance Python frameworks and design robust data pipelines that power advanced data processing, quality, and machine learning operations",
        "Develop and enhance Python frameworks and libraries to support data processing, quality, lineage, governance, analysis, and machine learning operations",
        "Design, build, and maintain scalable and efficient data pipelines on Google Cloud Platform",
        "Implement robust monitoring, logging, and alerting systems to ensure the reliability and stability of data infrastructure",
        "Build scalable batch pipelines leveraging BigQuery, Dataflow and Airflow/Composer scheduler/executor framework on Google Cloud Platform",
        "Build data pipelines leveraging Scala, Pub/Sub, Akka, and Dataflow on Google Cloud Platform",
        "Design data models for optimal storage and retrieval to support machine learning modeling using technologies like Bigtable and Vertex Feature Store",
        "Contribute to shared Data Engineering tooling and standards to improve productivity and quality for the team"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-4raiiob4wvohn4qafdpor",
    "_source": "new_jobs"
  },
  {
    "job_id": "ybwkukVOvvN7CZ4IAAAAAA==",
    "job_title": "Data Engineer Prin",
    "employer_name": "American Electric Power",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0RrFTOdrhzngcyQL-2zVSyVH1kAYBiD2Gj2L_&s=0",
    "employer_website": null,
    "job_publisher": "Tallo",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Tallo",
        "apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This job listing in Franklin - OH has been recently added. Tallo will add a summary here for this job shortly.",
    "job_is_remote": false,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Franklin, OH",
    "job_city": "Franklin",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.5589474,
    "job_longitude": -84.30410739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DybwkukVOvvN7CZ4IAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "tallo-com-jobs-technology-data-engineer-oh-franklin-data-engineer-prin-5d6c21303973",
    "_source": "new_jobs"
  },
  {
    "job_id": "Y4E5Ry_jMXZhJqZ0AAAAAA==",
    "job_title": "Senior Data Engineer (Hybrid: New York, NY - US)",
    "employer_name": "Energy Solutions - USA",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRpgxcxiBAtVhfhrhaRjnyyT-NmzJHIzfHTuZx_&s=0",
    "employer_website": null,
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-JV_IC1132348_KO0,42_KE43,63.htm?jl=1009998197539&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-JV_IC1132348_KO0,42_KE43,63.htm?jl=1009998197539&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/senior-data-engineer_7ea1a5828cd50166b000f5270394bdcb6b252?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Climatebase",
        "apply_link": "https://www.climatebase.org/job/68869193/senior-data-engineer-hybrid-new-york-ny---us?source=job_directory&queryID=8d9ee16f49c678cf06453d64a901b03e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/senior-pre-sales-solutions-engineer-av-it-new-york-ny-at-shure-incorporated-4364456775?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-pre-sales-solutions-engineer-av-it-new-york-ny-new-york-ny--a7599309-30a9-4b3c-aa9d-c8d4d1d024d5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remotejob",
        "apply_link": "https://remotejob.bsebexam.org.in/senior-data-engineer-hybrid-new-york-ny-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5629776722?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/pre-sales-solutions-engineer-av-jobs/E511D8BEFAF8E875?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. Since 1995, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.\n\nWe are currently seeking a Senior Data Engineer to join our Information Systems team to design, develop, and maintain data platforms that support the data needs across Energy Solutions. In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies. They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights. This unique position is perfect for individuals with technical prowess in data field who want to have an impact on energy efficiency markets and greenhouse gas reductions through our work for major North American utilities and other clients around the country.\n\nThis is a hybrid work opportunity. ES has offices in Oakland, CA, Orange, CA, Portland, OR, Chicago, IL, and Boston, MA.\n\nResponsibilities include but are not limited to:\n• Build, automate, and manage near-real-time scalable data ingestion pipelines for master data management, deep-learning, and predictive analytics.\n• Build and maintain cloud native big data environments on AWS, that are highly secure, scalable, flexible, and highly performant using appropriate SQL, NoSQL and NewSQL technologies.\n• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.\n• Provide technical input into build/buy/partner decisions for all components of the data infrastructure.\n• Partner closely with Data Scientists, BI developers, and Product Managers to design and implement data models, database schemas, data structures, and processing logic to support various data science, analytics, machine learning, and BI initiatives.\n• Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics, and model features, populate data models etc., using Spark, Python, SQL, and other technologies in the AWS.\n• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.\n• Lead by example, demonstrating best practices for code development and optimization, unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response to ensure data availability, data quality, and usability.\n• Define SLAs for data availability and correctness. Automate data availability and quality monitoring and respond to alerts when data delivery SLAs are not being met.\n• Communicate progress across organizations and levels from individual contributor to executive. Identify and clarify the critical few issues that need action and drive appropriate decisions and actions. Communicate results clearly.\n\nMinimum Qualifications:\n• A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience.\n• High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python. Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows.\n• Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena. Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions.\n• Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server. Knowledge of database design, optimization techniques, and advanced querying capabilities.\n• Experience in performance tuning and optimizing database operations.\n• Familiarity with data governance frameworks and data security best practices.\n• Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development.\n\nThe salary range for this role is $140,000 – $165,000/Annually with a target compensation of $156,750 based on experience and qualifications\n\nCompensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).\n\nAI Use\n\nAt Energy Solutions we believe in the importance of authentic interactions and equitable opportunities. We base our candidate selection on one's own skills, knowledge, and experience. To ensure the integrity and fairness of our interview process, the use of artificial intelligence (AI) tools (including Generative AI) or other means to generate or assist with responses during interviews is strictly prohibited. This practice supports our commitment to create a transparent and equitable space where skills, knowledge and experience skills can truly shine.\n\nEqual Opportunity Employer\n\nEnergy Solutions is an affirmative action-equal opportunity employer and prohibits discrimination and harassment of any type. We afford equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristics protected by law. Energy Solutions conforms to the spirit as well as to the letter of all applicable laws and regulations.\n\nOffice Locations and a Remote Workforce\n\nEnergy Solutions operates as a predominantly remote workforce with offices in six different locations. Employees who reside within 40 miles of an office (except New York) will be assigned to that location, though in-office attendance requirements may vary by team. At this time, we are not accepting applications from candidates residing in the following states: Delaware, Kentucky, Mississippi, Montana, Nebraska, North Dakota, and Wyoming.\n\nBackground Check Information\n\nInformation will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.\n\nReasonable Accommodations\n\nEnergy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require accommodations in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.\n\nPrivacy Notice for Job Applicants",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DY4E5Ry_jMXZhJqZ0AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 140000,
    "job_max_salary": 165000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies",
        "They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights",
        "A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience",
        "High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python",
        "Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows",
        "Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena",
        "Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions",
        "Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server",
        "Knowledge of database design, optimization techniques, and advanced querying capabilities",
        "Experience in performance tuning and optimizing database operations",
        "Familiarity with data governance frameworks and data security best practices",
        "Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development",
        "Information will be requested to perform the compulsory background check",
        "A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment"
      ],
      "Benefits": [
        "The salary range for this role is $140,000 – $165,000/Annually with a target compensation of $156,750 based on experience and qualifications",
        "Compensation is commensurate with experience and includes a generous retirement package",
        "Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP)"
      ],
      "Responsibilities": [
        "Build, automate, and manage near-real-time scalable data ingestion pipelines for master data management, deep-learning, and predictive analytics",
        "Build and maintain cloud native big data environments on AWS, that are highly secure, scalable, flexible, and highly performant using appropriate SQL, NoSQL and NewSQL technologies",
        "Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage",
        "Provide technical input into build/buy/partner decisions for all components of the data infrastructure",
        "Partner closely with Data Scientists, BI developers, and Product Managers to design and implement data models, database schemas, data structures, and processing logic to support various data science, analytics, machine learning, and BI initiatives",
        "Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics, and model features, populate data models etc., using Spark, Python, SQL, and other technologies in the AWS",
        "Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage",
        "Lead by example, demonstrating best practices for code development and optimization, unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response to ensure data availability, data quality, and usability",
        "Define SLAs for data availability and correctness",
        "Automate data availability and quality monitoring and respond to alerts when data delivery SLAs are not being met",
        "Communicate progress across organizations and levels from individual contributor to executive",
        "Identify and clarify the critical few issues that need action and drive appropriate decisions and actions",
        "Communicate results clearly"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-glassdoor-com-job-listing-senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-jv_ic1132348_ko0-42_ke43-63-htm",
    "_source": "new_jobs"
  },
  {
    "job_id": "VpHFbpEQe5weQK_1AAAAAA==",
    "job_title": "Walmart Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQV0kWbPp7sIFysOBbrsohip-f8-kSq7M6lcXJq&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Walmart",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://walmart.rightspotway.com/jobpage/rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Walmart",
        "apply_link": "https://walmart.rightspotway.com/jobpage/rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Walmart Data Engineer\n\nCompany Overview:\n\nWalmart Global Tech is a team of over 15,000 software engineers, data scientists, and service professionals delivering innovative technology solutions that enhance the retail experience for millions of customers and empower Walmart's 2.2 million associates. You will join a virtual-first, collaborative team focused on building the foundational intelligence through data, AI, and semantic modeling that drives autonomous decision-making across Walmart’s ecosystem.\n\nRole and Responsibilities of Walmart Data Engineer:\n\nYou will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously. You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning.\n• Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows.\n• Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state.\n• Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access.\n• Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations.\n• Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals.\n• Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources.\n• Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows.\n\nRequired Skills and Experience of Walmart Data Engineer:\n• 7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role.\n• Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem).\n• Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming).\n• Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI).\n• Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale.\n• Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning.\n• Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design.\n• Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience.\n• 4 years’ experience in data engineering, database engineering, business intelligence, or business analytics.\n• 1 year’s supervisory experience.\n\nPreferred Qualifications:\n• Master’s degree in Computer Science or related field with 5 years’ experience in software engineering or related field.\n• Experience with ETL tools and managing large datasets in cloud environments.\n• Demonstrated knowledge of inclusive digital experiences, Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, and assistive technologies.\n• Background in implementing accessibility best practices for inclusive product and service delivery.\n\nCompensation and Benefits of Walmart Data Engineer:\n• Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses.\n• Stock options and participation in Walmart’s stock purchase plan.\n• Health benefits including medical, vision, and dental coverage.\n• 401(k) plan with company match.\n• Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting.\n• Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement.\n• Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart.\n\nAbout Walmart:\n\nAt Walmart Global Tech, we innovate to simplify the retail experience, ensuring technology benefits millions of customers and supports associates worldwide. Our culture emphasizes inclusion, collaboration, and continuous learning. We empower teams to deliver meaningful solutions while fostering a flexible virtual-first work environment that encourages creativity, ownership, and professional growth. Being human-led is at the heart of every innovation we drive.\n\nFAQ:\n\nQ: What does a Walmart Data Engineer do?\n\nA: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making.\n\nQ: What skills are needed for a Data Engineer?\n\nA: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities.\n\nQ: What level is a Data Engineer?\n\nA: This is typically a mid-level technical role requiring experience in data engineering and analytics.\n\nQ: How can someone be successful as a Data Engineer?\n\nA: Success comes from building efficient data pipelines, ensuring data quality, collaborating with teams, and supporting business insights.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771200000,
    "job_posted_at_datetime_utc": "2026-02-16T00:00:00.000Z",
    "job_location": "Glendale, CO",
    "job_city": "Glendale",
    "job_state": "Colorado",
    "job_country": "US",
    "job_latitude": 39.7049873,
    "job_longitude": -104.9335904,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVpHFbpEQe5weQK_1AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Required Skills and Experience of Walmart Data Engineer:",
        "7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role",
        "Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem)",
        "Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming)",
        "Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI)",
        "Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale",
        "Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning",
        "Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design",
        "Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience",
        "4 years’ experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "A: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities",
        "A: This is typically a mid-level technical role requiring experience in data engineering and analytics"
      ],
      "Benefits": [
        "Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses",
        "Stock options and participation in Walmart’s stock purchase plan",
        "Health benefits including medical, vision, and dental coverage",
        "401(k) plan with company match",
        "Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting",
        "Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement",
        "Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart"
      ],
      "Responsibilities": [
        "You will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously",
        "You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning",
        "Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows",
        "Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state",
        "Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access",
        "Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations",
        "Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals",
        "Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources",
        "Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows",
        "A: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "walmart-rightspotway-com-jobpage-rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df",
    "_source": "new_jobs"
  },
  {
    "job_id": "kZoFvH-I-kdByj7fAAAAAA==",
    "job_title": "Data Engineer with Databricks Focus",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=9bbd03b4359f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=9bbd03b4359f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Data Engineer with a focus on Databricks.\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines using Databricks technologies\n\nLead the migration and rehydration of approximately 500 PowerBI reports, optimizing data sources\n\nImplement and maintain CI / CD pipelines for data assets using modern DevOps practices\n\nRequired Qualifications\n\n4+ years of hands-on data engineering experience\n\nStrong proficiency in Python and SQL\n\nDeep experience with Databricks and its associated technologies\n\nProven track record of implementing CI / CD for data workloads\n\nRelevant certifications in Databricks or Azure Data Engineering preferred",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Sunnyvale, CA",
    "job_city": "Sunnyvale",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.368829999999996,
    "job_longitude": -122.0363496,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkZoFvH-I-kdByj7fAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "4+ years of hands-on data engineering experience",
        "Strong proficiency in Python and SQL",
        "Deep experience with Databricks and its associated technologies",
        "Proven track record of implementing CI / CD for data workloads"
      ],
      "Responsibilities": [
        "Design, build, and maintain scalable data pipelines using Databricks technologies",
        "Lead the migration and rehydration of approximately 500 PowerBI reports, optimizing data sources",
        "Implement and maintain CI / CD pipelines for data assets using modern DevOps practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  }
]