[
  {
    "job_id": "UVVio6kevgI1MfEaAAAAAA==",
    "job_title": "Data Engineer/Flask Developer - 26410",
    "employer_name": "HII's Mission Technologies division",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Jobs.hii-Tsd.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.hii-tsd.com/job/Arlington%2C-VA-Data-EngineerFlask-Developer-26410-Remo/1351019200/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobs.hii-Tsd.com",
        "apply_link": "https://jobs.hii-tsd.com/job/Arlington%2C-VA-Data-EngineerFlask-Developer-26410-Remo/1351019200/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6944ed34a7227b3ece669c64?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-react-developer-26405-at-mission-technologies-a-division-of-hii-4344885344?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "| USNLX Veterans Jobs - National Labor Exchange",
        "apply_link": "https://veterans.usnlx.com/arlington-va/data-engineerreact-developer-26405/1954D06D256747DCBCAC949C2AF6BB34/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BillGoldenJobs.com",
        "apply_link": "https://jobzone.billgoldenjobs.com/jobs/260922662-data-engineer-flask-developer-26410?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/data-engineer-flask-developer-26410--arlington--e49f5cbc72919096a39cff20a15c991f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/ce2e19a2860c4cbbd1ec281458d1fdba?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Career.com",
        "apply_link": "https://www.career.com/job/mission-technologies-a-division-of-hii/data-engineer-flask-developer-26410/j202512190733387026148?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Requisition Number: 26410\n\nRequired Travel: 0 - 10%\n\nEmployment Type: Full Time/Salaried/Exempt\n\nAnticipated Salary Range: $98,717.00 - $130,000.00\n\nSecurity Clearance: Secret\n\nLevel of Experience: Mid\n\nThis opportunity resides with Warfare Systems (WS), a business group within HII’s Mission Technologies division. Warfare Systems comprises cyber and mission IT; electronic warfare; and C5ISR systems.\n\nHII works within our nation’s intelligence and cyber operations communities to defend our interests in cyberspace and anticipate emerging threats. Our capabilities in cybersecurity, network architecture, reverse engineering, software and hardware development uniquely enable us to support sensitive missions for the U.S. military and federal agency partners.\n\nMeet HII’s Mission Technologies Division\nOur team of more than 7,000 professionals worldwide delivers all-domain expertise and advanced technologies in service of mission partners across the globe. Mission Technologies is leading the next evolution of national defense – the data evolution - by accelerating a breadth of national security solutions for government and commercial customers. Our capabilities range from C5ISR, AI and Big Data, cyber operations and synthetic training environments to fleet sustainment, environmental remediation and the largest family of unmanned underwater vehicles in every class. Find the role that’s right for you. Apply today. We look forward to meeting you.\n\nTo learn more about Mission Technologies, click here for a short video: https://vimeo.com/732533072\nJob Description\n\nCome join our growing team today, supporting our Warfare Systems Business Group! HII-Mission Technologies is currently seeking a skilled Data Engineer/Flask Developer, who will support refining a centralized data environment (CDE) with impact across DoD! Key functionality for this position contributes toward auditable financial transaction data and building a common operating picture for leadership.\n\nA successful candidate will be well-versed in Flask and Python, knowledgeable about relational database (SQL), has strong written/oral communications, and can work as part of a team providing productive input toward solving challenging problems. Team members must be able to internalize and appreciate strategic requirements, to best align the most meaningful data to decision makers- getting the right data to the right people!\n\nEssential Job Responsibilities\n• Research, design, develop, and/or modify enterprise-wide systems and/or applications software.\n• Will be Involved in planning of system and development deployment as well as responsible for meeting software compliance standards.\n• Evaluate interface between hardware and software, operational requirements, and characteristics of overall system.\n• Document testing and maintenance of system corrections.\n• Design, develop, and maintain scalable backend services and RESTful APIs using Flask, Flask-RESTful, Flask-RESTX, or similar extensions, following best practices and clean architecture principles\n• Build and extend complex business logic in Python, implementing features from technical specifications and user stories while following SOLID principles and Flask best practices\n• Familiar with scripts for data ingest pipelines (Python).\n• Familiar with Relational Databases (SQL).\n• Utilize Agile development processes.\n• Guide development aligned to established business needs and/or policies.\n• Participate in code reviews, architectural discussions, and sprint planning, providing constructive feedback, suggesting improvements, and helping enforce coding standards\n• Additional duties as assigned or required.\n\nMinimum Qualifications\n• 5 years relevant experience with Bachelors in related field; 3 years relevant experience with Masters in related field; 0 years experience with PhD or Juris Doctorate in related field; or High School Diploma or equivalent and 9 years relevant experience.\n• Hands‑on experience building applications using Flask,\n• Proficiency in Python programming with solid understanding of core concepts,\n• Can operate independently and within a team.\n• Self-starter who takes initiative.\n• Clearance: Must possess and maintain a Secret clearance.\n\nPreferred Requirements\n• Ability to understand the transactional nature of the data being reviewed.\n• Experience with FMS, ADVANA.\n• Basic SQL skills for querying, managing, and optimizing relational databases.\n• Experience working in a Safe Agile environment.\n\nPhysical Requirements\n• Job performance requires adequate visual acuity and manual dexterity for meeting the requirements of the generic systems analyst discipline. Office work environment normally encountered.\n\nHII is more than a job - it’s an opportunity to build a new future. We offer competitive benefits such as best-in-class medical, dental and vision plan choices; wellness resources; employee assistance programs; Savings Plan Options (401(k)); financial planning tools, life insurance; employee discounts; paid holidays and paid time off; tuition reimbursement; as well as early childhood and post-secondary education scholarships. Bonus/other non-recurrent compensation is occasionally offered for qualified positions, and if applicable to this role will be addressed by the recruiter at the screening phase of application.\n\nWhy HII\nWe build the world’s most powerful, survivable naval ships and defense technology solutions that safeguard our seas, sky, land, space and cyber. Our workforce includes skilled tradespeople; artificial intelligence, machine learning (AI/ML) experts; engineers; technologists; scientists; logistics experts; and business administration professionals.\n\nRecognized as one of America’s top large company employers, we are a values and ethics driven organization that puts people’s safety and well-being first. Regardless of your role or where you serve, at HII, you’ll find a supportive and welcoming environment, competitive benefits, and valuable educational and training programs for continual career growth at every stage of your career.\n\nTogether we are working to ensure a future where everyone can be free and thrive.\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.\n\nDo You Need Assistance?\nIf you need a reasonable accommodation for any part of the employment process, please send an e-mail to buildyourcareer@hii-co.com and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. Additionally, you may also call 1-844-849-8463 for assistance. Press #3 for HII Mission Technologies.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771113600,
    "job_posted_at_datetime_utc": "2026-02-15T00:00:00.000Z",
    "job_location": "Arlington, VA",
    "job_city": "Arlington",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.8816208,
    "job_longitude": -77.09098089999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DUVVio6kevgI1MfEaAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "A successful candidate will be well-versed in Flask and Python, knowledgeable about relational database (SQL), has strong written/oral communications, and can work as part of a team providing productive input toward solving challenging problems",
        "Team members must be able to internalize and appreciate strategic requirements, to best align the most meaningful data to decision makers- getting the right data to the right people!",
        "5 years relevant experience with Bachelors in related field; 3 years relevant experience with Masters in related field; 0 years experience with PhD or Juris Doctorate in related field; or High School Diploma or equivalent and 9 years relevant experience",
        "Hands‑on experience building applications using Flask,",
        "Proficiency in Python programming with solid understanding of core concepts,",
        "Can operate independently and within a team",
        "Self-starter who takes initiative",
        "Clearance: Must possess and maintain a Secret clearance",
        "Job performance requires adequate visual acuity and manual dexterity for meeting the requirements of the generic systems analyst discipline"
      ],
      "Benefits": [
        "Anticipated Salary Range: $98,717.00 - $130,000.00",
        "We offer competitive benefits such as best-in-class medical, dental and vision plan choices; wellness resources; employee assistance programs; Savings Plan Options (401(k)); financial planning tools, life insurance; employee discounts; paid holidays and paid time off; tuition reimbursement; as well as early childhood and post-secondary education scholarships"
      ],
      "Responsibilities": [
        "Required Travel: 0 - 10%",
        "Key functionality for this position contributes toward auditable financial transaction data and building a common operating picture for leadership",
        "Research, design, develop, and/or modify enterprise-wide systems and/or applications software",
        "Will be Involved in planning of system and development deployment as well as responsible for meeting software compliance standards",
        "Evaluate interface between hardware and software, operational requirements, and characteristics of overall system",
        "Document testing and maintenance of system corrections",
        "Design, develop, and maintain scalable backend services and RESTful APIs using Flask, Flask-RESTful, Flask-RESTX, or similar extensions, following best practices and clean architecture principles",
        "Build and extend complex business logic in Python, implementing features from technical specifications and user stories while following SOLID principles and Flask best practices",
        "Familiar with scripts for data ingest pipelines (Python)",
        "Familiar with Relational Databases (SQL)",
        "Utilize Agile development processes",
        "Guide development aligned to established business needs and/or policies",
        "Participate in code reviews, architectural discussions, and sprint planning, providing constructive feedback, suggesting improvements, and helping enforce coding standards",
        "Additional duties as assigned or required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-hii-tsd-com-job-arlington-2c-va-data-engineerflask-developer-26410-remo-1351019200",
    "_source": "new_jobs"
  },
  {
    "job_id": "DI_gmnxno-NVwm8wAAAAAA==",
    "job_title": "Senior Data Engineer (Informatica/Databricks)",
    "employer_name": "CACI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSEIAi6XHhq4EhegiHCD5u672hDATM_CMWOaqCb&s=0",
    "employer_website": null,
    "job_publisher": "CACI Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "CACI Careers",
        "apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9abc0f815d739892&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Infinitive-Inc/Job/Senior-Data-Engineer-(Python-PySpark-AWS)/-in-Ashburn,VA?jid=80f99d04bc706cc2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/senior-data-engineer-informaticadatabricks-ashburn-caci-international-inc-b8b74c37cb3bb6008869745852135bbe?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "TekSynap Job Openings - ICIMS",
        "apply_link": "https://careers-teksynap.icims.com/jobs/8146/senior-data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Tech Jobs Personalized",
        "apply_link": "https://builtin.com/job/senior-data-engineer-pythonpysparkaws/4210594?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/senior-data-engineer-informatica-databricks_7ea1ae6375c0867ceb8124f167b38d5087153?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-data-engineer-databricks-ashburn-va--5f5b01bf-92fb-430c-bb16-c97cfe25d721?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Title: Senior Data Engineer (Informatica/Databricks)\n\nJob Category: Information Technology\n\nTime Type: Full time\n\nMinimum Clearance Required to Start: None\n\nEmployee Type: Regular\n\nPercentage of Travel Required: Up to 10%\n\nType of Travel: Local\n• * *\n\nThe Opportunity:\n\nCACI is currently looking for a highly skilled and experienced Senior Data Engineer (Informatica/Databricks) with agile methodology experience to join our BEAGLE (Border Enforcement Applications for Government Leading-Edge Information Technology) Agile Solution Factory (ASF) Team supporting Customs and Border Protection (CBP) client located in Northern Virginia! Join this passionate team of industry-leading individuals supporting the best practices in Agile Software Development for the Department of Homeland Security (DHS).\n\nAs a member of the BEAGLE ASF Team, you will support the men and women charged with safeguarding the American people and enhancing the Nation’s safety, security, and prosperity. CBP agents and officers are on the front lines, every day, protecting our national security by combining customs, immigration, border security, and agricultural protection into one coordinated and supportive activity.\n\nASF programs thrive in a culture of innovation and are constantly seeking individuals who can bring creative ideas to solve complex problems, both technical and procedural at the team and portfolio levels. The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial. You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints\n\nResponsibilities:\n• Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform.\n• Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing.\n• Work with database developers and administrators across multiple product teams.\n• Serve as a data and technology expert across a broad and diverse set of mission critical applications\n• Modernize the data warehouse environment by migrating the platform to Databricks\n• Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes.\n• Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components.\n• Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets.\n• Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases\n\nQualifications:\n\n· Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:\n\no 3 year check for felony convictions\n\no 1 year check for illegal drug use\n\no 1 year check for misconduct such as theft or fraud\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· Proven 7+ years of experience automating ELT data pipelines using Informatica.\n\n· Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS).\n\n· Experience building and optimizing data pipelines for batch and/or streaming data.\n\n· 3-5 years of Databricks experience. Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered.\n\n· Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks. Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design.\n\n· Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks\n\n· Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies.\n\n· Candidates with one or more of the above skillsets are encouraged to apply.\n\nDesired:\n\n· 5-10 years of DHS, DoD, or IC experience working in complex data environments, including the architecture and optimization of data schemas, terabyte-scale ETL, etc.\n\n· Exposure to implementing or migrating to Cloud environments like Amazon Web Services (AWS) or Microsoft Azure.\n\n· Previous experience as an Enterprise-level Data Architect, Data Engineer, Data Scientist, or Data Analyst.\n\n· Ability to apply advanced principles, theories, and concepts, and contribute to the development of innovative principles and ideas.\n\n-\n\n_________________________________________________________________________\n\nWhat You Can Expect:\n\nA culture of integrity.\n\nAt CACI, we place character and innovation at the center of everything we do. As a valued team member, you’ll be part of a high-performing group dedicated to our customer’s missions and driven by a higher purpose – to ensure the safety of our nation.\n\nAn environment of trust.\n\nCACI values the unique contributions that every employee brings to our company and our customers - every day. You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality.\n\nA focus on continuous growth.\n\nTogether, we will advance our nation's most critical missions, build on our lengthy track record of business success, and find opportunities to break new ground — in your career and in our legacy.\n\nYour potential is limitless. So is ours.\n\n_________________________________________________________________________\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits.\n\nThe proposed salary range for this position is:\n$113,200 - $237,800\n\nCACI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, age, national origin, disability, status as a protected veteran, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Ashburn, VA",
    "job_city": "Ashburn",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 39.043756699999996,
    "job_longitude": -77.4874416,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDI_gmnxno-NVwm8wAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial",
        "You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints",
        "Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:",
        "3 year check for felony convictions",
        "1 year check for illegal drug use",
        "1 year check for misconduct such as theft or fraud",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "Proven 7+ years of experience automating ELT data pipelines using Informatica",
        "Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS)",
        "Experience building and optimizing data pipelines for batch and/or streaming data",
        "3-5 years of Databricks experience",
        "Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered",
        "Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks",
        "Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design",
        "Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks",
        "Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies",
        "Candidates with one or more of the above skillsets are encouraged to apply"
      ],
      "Benefits": [
        "CACI values the unique contributions that every employee brings to our company and our customers - every day",
        "You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality",
        "A focus on continuous growth",
        "Pay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications",
        "Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives",
        "We offer competitive compensation, benefits and learning and development opportunities",
        "Our broad and competitive mix of benefits options is designed to support and protect employees and their families",
        "At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits",
        "$113,200 - $237,800"
      ],
      "Responsibilities": [
        "Percentage of Travel Required: Up to 10%",
        "Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform",
        "Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing",
        "Work with database developers and administrators across multiple product teams",
        "Serve as a data and technology expert across a broad and diverse set of mission critical applications",
        "Modernize the data warehouse environment by migrating the platform to Databricks",
        "Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes",
        "Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components",
        "Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets",
        "Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-caci-com-global-en-job-caciglobal322083externalenglobal",
    "_source": "new_jobs"
  },
  {
    "job_id": "U94cMktvMFyMZQbNAAAAAA==",
    "job_title": "Senior Data Engineer (Remote)",
    "employer_name": "CareFirst BlueCross BlueShield",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRCyGfghEeyHonJaWEop5fDwDnNOuCfGxIzCuYm&s=0",
    "employer_website": "https://www.carefirst.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-carefirst-bluecross-blueshield-4360799112?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-carefirst-bluecross-blueshield-4360799112?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-warehousing?id=2449244773&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "International Association Of Women",
        "apply_link": "https://careers.iawomen.com/job/senior-data-engineer-remote-7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Purpose\n\nResp & Qualifications\n\nThe primary purpose of a Senior Data Engineer in Production Support is to maintain the seamless operation of data pipelines, databases, and analytics platforms in live environments. This includes monitoring and troubleshooting data workflows, resolving incidents, and proactively addressing performance bottlenecks to minimize downtime and ensure data availability for end users and business stakeholders.\n\nEssential Functions\n• Incident Management: Quickly diagnose and resolve data-related issues in production, minimizing impact on business operations.\n• Monitoring & Alerting: Implement and maintain monitoring solutions to detect anomalies, failures, or performance degradation in data systems.\n• Root Cause Analysis: Investigate recurring problems, identify their root causes, and implement long-term solutions to prevent future incidents.\n• Performance Optimization: Analyze data workflows and infrastructure for inefficiencies, tuning systems for optimal performance and scalability.\n• Collaboration: Work closely with data engineers, analysts, software developers, and IT teams to ensure seamless integration and deployment of data solutions.\n• Documentation & Best Practices: Maintain clear documentation of production environments, issue resolutions, and standard operating procedures.\n• Change Management: Participate in the planning and execution of changes to data systems, including upgrades, patches, and configuration updates, while minimizing disruption to ongoing operations.\n• Security & Compliance: Ensure data systems adhere to organizational security policies and regulatory requirements, identifying and mitigating potential vulnerabilities.\n• Capacity Planning: Forecast future data storage and processing needs, recommending infrastructure enhancements to support growth and evolving business requirements.\n• User Support: Provide technical assistance to end users and business teams, addressing queries and enabling effective utilization of data resources.\n• Automation: Develop and maintain scripts and tools to automate repetitive support tasks, streamlining processes and reducing manual intervention.\n\nSupervisory Responsibility\n\nPosition does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.\n\nQualifications\n\nEducation Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.\n\nExperience\n• 5 years Experience with database design and developing modeling tools.\n• Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.\n\nKnowledge, Skills And Abilities (KSAs)\n• Experience with Informatica IICS: Practical knowledge in using Informatica Intelligent Cloud Services (IICS) for building, managing, and optimizing cloud-based data integration workflows.\n• Knowledge of Kafka: Familiarity with Apache Kafka architecture and able to trouble shoot issues in a timely manner.\n• MDM SaaS Experience: Understanding and hands-on experience with Master Data Management (MDM) Software as a Service solution for ensuring data consistency, quality, and governance across enterprise systems.\n• Proficiency in GitHub: Demonstrated expertise in utilizing GitHub for version control, code collaboration, and managing development workflows within data engineering projects.\n• Extensive Experience with SQL: Advanced proficiency in writing complex queries, optimizing performance, and managing large datasets in Snowflake, Oracle, and SQL Server Databases.\n• Expertise in Microsoft Azure: Hands-on experience with Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Blob Storage, Fabrics, and other relevant Azure cloud services.\n• Power BI Experience: Hands-on experience in developing, deploying, and maintaining data visualizations and dashboards using Power BI, including advanced DAX queries, and integrating with diverse data sources for actionable insights.\n• Control-M Knowledge: Familiarity with Control-M workload automation, including designing, scheduling, and monitoring data pipeline jobs to ensure reliable and efficient batch processing within enterprise environments.\n• Proficiency in Scripting Languages: Demonstrated ability to automate data workflows using Python, PowerShell, or similar scripting languages.\n• Experience with Version Control Tools: Strong understanding and practical use of Git, GitHub, Azure DevOps for collaborative development and CI/CD pipelines.\n• Strong Data Engineering Fundamentals: Solid understanding of ETL/ELT processes, data warehousing concepts, and best practices in data architecture and governance.\n• Flexible working long hours and on demand: Willingness and ability to adapt to varying work schedules, including evenings and weekends, to meet project deadlines and production support requirements. Able to respond promptly to urgent issues and provide support whenever needed to ensure business continuity and system reliability.\n\nSalary Range: $96,048 - $190,762\n\nSalary Range Disclaimer\n\nThe disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilities of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).\n\nDepartment\n\nData Platforms and Services\n\nEqual Employment Opportunity\n\nCareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.\n\nWhere To Apply\n\nPlease visit our website to apply: www.carefirst.com/careers\n\nFederal Disc/Physical Demand\n\nNote: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.\n\nPhysical Demands\n\nThe associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.\n\nSponsorship in US\n\nMust be eligible to work in the U.S. without Sponsorship",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Baltimore, MD",
    "job_city": "Baltimore",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.2905023,
    "job_longitude": -76.6104072,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DU94cMktvMFyMZQbNAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 96048,
    "job_max_salary": 190762,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience",
        "5 years Experience with database design and developing modeling tools",
        "Experience developing and updating ETL/ELT scripts",
        "Hands-on experience with application development, relational database layout, development, data modeling",
        "Knowledge, Skills And Abilities (KSAs)",
        "Experience with Informatica IICS: Practical knowledge in using Informatica Intelligent Cloud Services (IICS) for building, managing, and optimizing cloud-based data integration workflows",
        "Knowledge of Kafka: Familiarity with Apache Kafka architecture and able to trouble shoot issues in a timely manner",
        "MDM SaaS Experience: Understanding and hands-on experience with Master Data Management (MDM) Software as a Service solution for ensuring data consistency, quality, and governance across enterprise systems",
        "Proficiency in GitHub: Demonstrated expertise in utilizing GitHub for version control, code collaboration, and managing development workflows within data engineering projects",
        "Extensive Experience with SQL: Advanced proficiency in writing complex queries, optimizing performance, and managing large datasets in Snowflake, Oracle, and SQL Server Databases",
        "Expertise in Microsoft Azure: Hands-on experience with Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Blob Storage, Fabrics, and other relevant Azure cloud services",
        "Power BI Experience: Hands-on experience in developing, deploying, and maintaining data visualizations and dashboards using Power BI, including advanced DAX queries, and integrating with diverse data sources for actionable insights",
        "Control-M Knowledge: Familiarity with Control-M workload automation, including designing, scheduling, and monitoring data pipeline jobs to ensure reliable and efficient batch processing within enterprise environments",
        "Proficiency in Scripting Languages: Demonstrated ability to automate data workflows using Python, PowerShell, or similar scripting languages",
        "Experience with Version Control Tools: Strong understanding and practical use of Git, GitHub, Azure DevOps for collaborative development and CI/CD pipelines",
        "Strong Data Engineering Fundamentals: Solid understanding of ETL/ELT processes, data warehousing concepts, and best practices in data architecture and governance",
        "Flexible working long hours and on demand: Willingness and ability to adapt to varying work schedules, including evenings and weekends, to meet project deadlines and production support requirements",
        "The hands are regularly used to write, type, key and handle or feel small controls and objects",
        "The associate must frequently talk and hear",
        "Weights up to 25 pounds are occasionally lifted",
        "Sponsorship in US",
        "Must be eligible to work in the U.S. without Sponsorship"
      ],
      "Benefits": [
        "Salary Range: $96,048 - $190,762",
        "Salary Range Disclaimer",
        "In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements)"
      ],
      "Responsibilities": [
        "The primary purpose of a Senior Data Engineer in Production Support is to maintain the seamless operation of data pipelines, databases, and analytics platforms in live environments",
        "This includes monitoring and troubleshooting data workflows, resolving incidents, and proactively addressing performance bottlenecks to minimize downtime and ensure data availability for end users and business stakeholders",
        "Incident Management: Quickly diagnose and resolve data-related issues in production, minimizing impact on business operations",
        "Monitoring & Alerting: Implement and maintain monitoring solutions to detect anomalies, failures, or performance degradation in data systems",
        "Root Cause Analysis: Investigate recurring problems, identify their root causes, and implement long-term solutions to prevent future incidents",
        "Performance Optimization: Analyze data workflows and infrastructure for inefficiencies, tuning systems for optimal performance and scalability",
        "Collaboration: Work closely with data engineers, analysts, software developers, and IT teams to ensure seamless integration and deployment of data solutions",
        "Documentation & Best Practices: Maintain clear documentation of production environments, issue resolutions, and standard operating procedures",
        "Change Management: Participate in the planning and execution of changes to data systems, including upgrades, patches, and configuration updates, while minimizing disruption to ongoing operations",
        "Security & Compliance: Ensure data systems adhere to organizational security policies and regulatory requirements, identifying and mitigating potential vulnerabilities",
        "Capacity Planning: Forecast future data storage and processing needs, recommending infrastructure enhancements to support growth and evolving business requirements",
        "User Support: Provide technical assistance to end users and business teams, addressing queries and enabling effective utilization of data resources",
        "Automation: Develop and maintain scripts and tools to automate repetitive support tasks, streamlining processes and reducing manual intervention",
        "Supervisory Responsibility",
        "Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff",
        "May lead a team of matrixed resources",
        "Able to respond promptly to urgent issues and provide support whenever needed to ensure business continuity and system reliability",
        "The associate is primarily seated while performing the duties of the position",
        "Occasional walking or standing is required"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-senior-data-engineer-remote-at-carefirst-bluecross-blueshield-4360799112",
    "_source": "new_jobs"
  },
  {
    "job_id": "Dcu1fLjPsIMWWyv9AAAAAA==",
    "job_title": "Associate Data Engineer",
    "employer_name": "THE DOW CHEMICAL COMPANY",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHE6Qv_UOqARLxrnwfpr-ucKbs6GEppbjL9zYs&s=0",
    "employer_website": null,
    "job_publisher": "THE DOW CHEMICAL COMPANY",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "THE DOW CHEMICAL COMPANY",
        "apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Dow, we believe in putting people first and we’re passionate about delivering integrity, respect and safety to our customers, our employees and the planet.\n\nOur people are at the heart of our solutions. They reflect the communities we live in and the world where we do business. Their diversity is our strength. We’re a community of relentless problem solvers that offers the daily opportunity to contribute with your perspective, transform industries and shape the future. Our purpose is simple - to deliver a sustainable future for the world through science and collaboration.If you’re looking for a challenge and meaningful role, you’re in the right place.\n\nAbout you and the role\n\nDow has an exciting and challenging opportunity for an Associate Data Engineer located in Midland, MI or Houston, TX. which will work on the Enterprise Data & Analytics - Data Analytics Platform team.\n\nAs an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow. TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects. They will work closely with amulti-disciplinary team to:\n• Createdata pipelines forprojectsin the Azure environment.\n• Write notebooks in Databricks\n• Develop Infrastructure as Code (IaC) code\n• Build logic apps\n• Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation.\n\nAssociate Data Engineer Responsibilities / Duties:\n• Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)\n• Develop and deploy data pipelines using Azure data services\n• Deployment of Azure services using Infrastructure as Code and Azure DevOps.\n• This entry level position is for an Independent Contributor and is not expected to be a people leader\n\nKnowledge, skills and abilities include:\n• Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives\n• Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’\n• Concepts of Data architecting - concepts\n• General understanding of digital industry trends\n\nOther Critical Skills:\n• Ability to thrive in challenging situations and solve complex problems\n• Ability to manage own work effort across multiple projects with little supervision\n• Analytical and problem-solving skills\n• Customer centricity\n• Good communication\n\nYour Skills\n• Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs). This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms.\n• Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems.\n• Business Processes: Understanding how business functions operate and how technology enables or improves them. This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs.\n• Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments.\n• Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards.\n\nRequired qualifications:\n• Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree.\n• A minimum requirement for this U.S. based position is the ability to work legally in the United States. No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process.\n\nYour preferred qualifications include:\n• A degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines\n• Experiencewith Azuredata services andPython (other programming language)\n• Experience developing intheAzureenvironment\n• Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling\n• Multi-application and cross-platform design experience\n\nNote: Relocation assistance is not available with this position.\n\nBenefits – What Dow offers you\n\nWe invest in you.\n\nDow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career. You bring your background, talent, and perspective to work every day. Dow rewards that commitment by investing in your total wellbeing.\n\nHere are just a few highlights of what you would be offered as a Dow employee:\n• Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives.\n• Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it.\n• Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals.\n• Employee stock purchase programs (availability varies depending on location).\n• Student Debt Retirement Savings Match Program (U.S. only).\n• Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match.\n• Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs. Travel insurance is also available in certain countries/locations.\n• Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building.\n• Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs.\n• Competitive yearly vacation allowance.\n• Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents).\n• Paid time off to care for family members who are sick or injured.\n• Paid time off to support volunteering and Employee Resource Group’s (ERG) participation.\n• Wellbeing Portal for all Dow employees, our one-stop shop to promote wellbeing, empowering employees to take ownership of their entire wellbeing journey.\n• On-site fitness facilities to help stay healthy and active (availability varies depending on location).\n• Employee discounts for online shopping, cinema tickets, gym memberships and more.\n• Additionally, some of our locations might offer:\n• Transportation allowance (availability varies depending on location)\n• Meal subsidiaries/vouchers (availability varies depending on location)\n• Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)\n\nJoin our team, we can make a difference together.\n\nAbout Dow\nDow (NYSE: DOW) is one of the world’s leading materials science companies, serving customers in high-growth markets such as packaging, infrastructure, mobility and consumer applications.Our global breadth, asset integration and scale, focused innovation, leading business positions and commitment to sustainability enable us to achieve profitable growth and help deliver a sustainable future. We operate manufacturing sites in 30countries and employ approximately36,000 people. Dow delivered sales of approximately$43 billionin 2024. References to Dow or the Company mean Dow Inc. and its subsidiaries. Learn more about us and our ambition to be the most innovative, customer-centric, inclusive and sustainable materials science company in the world by visitingwww.dow.comopens in a new tab.\n\nAs part of our dedication to inclusion, Dow is committed to equal opportunities in employment. We encourage every employee to bring their whole self to work each day to not only deliver more value, but also have a more fulfilling career. Further information regarding Dow's equal opportunities is available on www.dow.comopens in a new tab.\nDow is an Equal Employment Opportunity employer and is committed to providing opportunities without regard for race, color, religion, sex, including pregnancy, sexual orientation, or gender identity, national origin, age, disability and genetic information, including family medical history. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may call us at 1-833-My Dow HR (833-693-6947) and select option 8.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "La Marque, TX",
    "job_city": "La Marque",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 29.3685674,
    "job_longitude": -94.9713134,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDcu1fLjPsIMWWyv9AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This entry level position is for an Independent Contributor and is not expected to be a people leader",
        "Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives",
        "Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’",
        "Concepts of Data architecting - concepts",
        "General understanding of digital industry trends",
        "Ability to thrive in challenging situations and solve complex problems",
        "Ability to manage own work effort across multiple projects with little supervision",
        "Analytical and problem-solving skills",
        "Customer centricity",
        "Good communication",
        "Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs)",
        "Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree",
        "A minimum requirement for this U.S. based position is the ability to work legally in the United States",
        "No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process",
        "Python (other programming language)",
        "Experience developing intheAzureenvironment",
        "Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling",
        "Multi-application and cross-platform design experience"
      ],
      "Benefits": [
        "We invest in you",
        "Dow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career",
        "You bring your background, talent, and perspective to work every day",
        "Dow rewards that commitment by investing in your total wellbeing",
        "Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives",
        "Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it",
        "Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals",
        "Employee stock purchase programs (availability varies depending on location)",
        "Student Debt Retirement Savings Match Program (U.S. only)",
        "Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match",
        "Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs",
        "Travel insurance is also available in certain countries/locations",
        "Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building",
        "Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs",
        "Competitive yearly vacation allowance",
        "Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents)",
        "Paid time off to care for family members who are sick or injured",
        "Paid time off to support volunteering and Employee Resource Group’s (ERG) participation",
        "On-site fitness facilities to help stay healthy and active (availability varies depending on location)",
        "Employee discounts for online shopping, cinema tickets, gym memberships and more",
        "Additionally, some of our locations might offer:",
        "Transportation allowance (availability varies depending on location)",
        "Meal subsidiaries/vouchers (availability varies depending on location)",
        "Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)"
      ],
      "Responsibilities": [
        "which will work on the Enterprise Data & Analytics - Data Analytics Platform team",
        "As an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow",
        "TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects",
        "They will work closely with amulti-disciplinary team to:",
        "Createdata pipelines forprojectsin the Azure environment",
        "Write notebooks in Databricks",
        "Develop Infrastructure as Code (IaC) code",
        "Build logic apps",
        "Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation",
        "Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)",
        "Develop and deploy data pipelines using Azure data services",
        "Deployment of Azure services using Infrastructure as Code and Azure DevOps",
        "This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms",
        "Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems",
        "Business Processes: Understanding how business functions operate and how technology enables or improves them",
        "This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs",
        "Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments",
        "Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-dow-com-hiring-associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c",
    "_source": "new_jobs"
  },
  {
    "job_id": "P2ieg-hGIVfZYRVjAAAAAA==",
    "job_title": "Sr. Data Engineer",
    "employer_name": "CVS Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQudUQ7bAJjtUW_FO_Jjs6PIFXc_7l7lfcD4fbb&s=0",
    "employer_website": "https://www.cvshealth.com",
    "job_publisher": "Occupational Health & Safety",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.ohsonline.com/job/sr-data-engineer/82494548/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Occupational Health & Safety",
        "apply_link": "https://careers.ohsonline.com/job/sr-data-engineer/82494548/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/qA3XmNTnOXQJK8k5m22eEzhiKUAbpwqNxCkmSyxt87lJpUt9lQKgKw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We're building a world of health around every individual - shaping a more connected, convenient and compassionate health experience. At CVS Health®, you'll be surrounded by passionate colleagues who care deeply, innovate with purpose, hold ourselves accountable and prioritize safety and quality in everything we do. Join us and be part of something bigger - helping to simplify health care one person, one family and one community at a time.\n\nPosition Summary\n\nWe're seeking a Sr. Data Engineer to design and implement data pipelines that power analytical capabilities. This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.\n\nYou will be part of a dedicated team creating datasets for financial and analytic workloads.\n\nKey Responsibilities:\n• Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources.\n• Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs.\n• Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access.\n• Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements.\n• Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models.\n• Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers.\n• Design and architect data infrastructure analytical workloads.\n\nRequired Qualifications\n• 5+ years of applicable work experience\n• Proficiency in Python, specifically with ETL pipelines.\n• Strong proficiency in SQL and experience in developing complex queries.\n• Familiarity with pySpark, DBT, or other similar frameworks.\n• Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP).\n• Understanding of data warehousing concepts, dimensional modeling, and building data marts.\n• Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners.\n\nPreferred Qualifications\n• Knowledge of data governance best practices in a cloud environment.\n• Experience with data design in BigQuery\n• Experience working with the Epic data model.\n• Experience working with healthcare data (Claims and Admissions)\n\nEducation and Experience\n• College degree or certification in related fields\n\nAnticipated Weekly Hours\n40\n\nTime Type\nFull time\n\nPay Range\n\nThe typical pay range for this role is:\n\n$83,430.00 - $222,480.00\n\nThis pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.\n\nOur people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\nGreat benefits for great people\n\nWe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:\n• Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan.\n• No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n• Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\n\nFor more information, visit https://jobs.cvshealth.com/us/en/benefits\n\nWe anticipate the application window for this opening will close on: 02/20/2026\n\nQualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Wisconsin",
    "job_city": null,
    "job_state": "Wisconsin",
    "job_country": "US",
    "job_latitude": 43.7844397,
    "job_longitude": -88.7878678,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DP2ieg-hGIVfZYRVjAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions",
        "5+ years of applicable work experience",
        "Proficiency in Python, specifically with ETL pipelines",
        "Strong proficiency in SQL and experience in developing complex queries",
        "Familiarity with pySpark, DBT, or other similar frameworks",
        "Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP)",
        "Understanding of data warehousing concepts, dimensional modeling, and building data marts",
        "Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners",
        "College degree or certification in related fields"
      ],
      "Benefits": [
        "Pay Range",
        "$83,430.00 - $222,480.00",
        "This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls",
        "The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors",
        "This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above",
        "We take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be",
        "In addition to our competitive wages, our great benefits include:",
        "Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan",
        "No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching",
        "Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility"
      ],
      "Responsibilities": [
        "Data Engineer to design and implement data pipelines that power analytical capabilities",
        "You will be part of a dedicated team creating datasets for financial and analytic workloads",
        "Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources",
        "Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs",
        "Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access",
        "Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements",
        "Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models",
        "Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes",
        "Share best practices and mentor junior data engineers",
        "Design and architect data infrastructure analytical workloads"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-ohsonline-com-job-sr-data-engineer-82494548",
    "_source": "new_jobs"
  },
  {
    "job_id": "aLEr5N2HS9ba0TU3AAAAAA==",
    "job_title": "Data Engineer - Remote at Staffing the Universe United States",
    "employer_name": "Staffing the Universe",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Ibfportal.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Ibfportal.com",
        "apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Remote job at Staffing the Universe. United States. Data Engineer\n\nData Engineer Hartford, CT or Remote Contract No third-party C2C\n\nJob Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team. On this team, you will be helping architect and deliver a wide variety of code artifacts. You will be working to build a scalable and secure ETL solutions for ope...",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "New Haven, CT",
    "job_city": "New Haven",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.308274,
    "job_longitude": -72.9278835,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaLEr5N2HS9ba0TU3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Data Engineer Hartford, CT or Remote Contract No third-party C2C"
      ],
      "Responsibilities": [
        "Job Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team",
        "On this team, you will be helping architect and deliver a wide variety of code artifacts",
        "You will be working to build a scalable and secure ETL solutions for ope.."
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "ibfportal-com-office-job-data-engineer-remote-at-staffing-the-universe-united-states-ym9lbu9gtu5hnlh5ede0nys1utrrk2vvzmc9pq",
    "_source": "new_jobs"
  },
  {
    "job_id": "zXyI7pf5emLDB5aTAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Deloitte",
    "employer_logo": null,
    "employer_website": "https://www.deloitte.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/engineer?id=2451775113&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary\n\nLead Data Engineer\n\nRole Overview: As a Lead Engineer (Data Lifecycle Management), you will take a hands-on role in designing, building, and operating data platform capabilities that govern how data is created, classified, used/shared, retained, archived, and deleted within scope. You will lead delivery of lifecycle controls-such as metadata capture and lineage, data classification and policy enforcement, retention/legal hold workflows, and audit-ready evidence and reporting-embedded directly into modern data stacks (cloud storage, lakehouse/warehouse, and data pipelines). Your expertise will be pivotal in turning governance, privacy, and regulatory requirements into scalable engineering patterns and automation, improving data trust, reducing operational risk, and enabling compliant reuse of data products. The ideal candidate is a hands-on technical leader and mentor who partners closely with data governance, security, privacy, legal, and platform teams to deliver reliable, outcome-driven lifecycle solutions.\n\nRecruiting for this role ends on 5/29/26.\n\nKey Responsibilities:\n\nOutcome-Driven Accountability: Embrace and drive a culture of accountability for customer and business outcomes. Develop engineering solutions that solve complex problems with valuable outcomes, ensuring high-quality, lean designs and implementations.\n\nTechnical Leadership and Advocacy: Serve as the technical advocate for products, ensuring code integrity, feasibility, and alignment with business and customer goals. Lead requirement analysis, contributing to low-level architecture and component design, development, unit testing, integrations, and support.\n\nEngineering Craftsmanship: Maintain accountability for the integrity of code design, implementation, quality, data, and ongoing maintenance and operations. Stay hands-on, self-driven, and continuously learn new approaches, languages, and frameworks. Create technical specifications, and write high-quality, supportable, scalable code and review code of other engineers, mentoring them, to ensure all quality KPIs are met or exceeded. Demonstrate collaborative skills to work effectively with diverse teams.\n\nCustomer-Centric Engineering: Develop lean engineering solutions through rapid, inexpensive experimentation to solve customer needs. Engage with customers and product teams before, during, and after delivery to ensure the right solution is delivered at the right time.\n\nIncremental and Iterative Delivery: Adopt a mindset that favors action and evidence over extensive planning. Utilize a leaning-forward approach to navigate complexity and uncertainty, delivering lean, supportable, and maintainable solutions.\n\nCross-Functional Collaboration and Integration: Work collaboratively with empowered, cross-functional teams including product management, experience, and delivery. Integrate diverse perspectives to make well-informed decisions that balance feasibility, viability, usability, and value. Foster a collaborative environment that enhances team synergy and innovation.\n\nAdvanced Technical Proficiency: Possess deep expertise in modern software engineering practices and principles, including Agile methodologies and DevSecOps to deliver daily product deployments using full automation from code check-in to production with all quality checks through SDLC lifecycle. Strive to be a role model, leveraging these techniques to optimize solutioning and product delivery. Demonstrate strong understanding of the full lifecycle product development, focusing on continuous improvement and learning.\n\nDomain Expertise: Quickly acquire domain-specific knowledge relevant to the business or product. Translate business/user needs, architectures, and UX/UI designs into technical specifications and code. Be a valuable, flexible, and dedicated team member, supportive of teammates, and focused on quality and tech debt payoff.\n\nEffective Communication and Influence: Exhibit exceptional communication skills, capable of articulating complex technical concepts clearly and compellingly. Inspire and influence teammates and product teams through well-structured arguments and trade-offs supported by evidence. Create coherent narratives that align technical solutions with business objectives.\n\nEngagement and Collaborative Co-Creation: Engage and collaborate with product engineering teams at all organizational levels, including customers as needed. Build and maintain constructive relationships, fostering a culture of co-creation and shared momentum towards achieving product goals. Align diverse perspectives and drive consensus to create feasible solutions.\n\nThe team: US Deloitte Technology Product Engineering has modernized software and product delivery, creating a scalable, cost-effective model that focuses on value/outcomes that leverages a progressive and responsive talent structure. As Deloitte's primary internal development team, Product Engineering delivers innovative digital solutions to businesses, service lines, and internal operations with proven bottom-line results and outcomes. It helps power Deloitte's success. It is the engine that drives Deloitte, serving many of the world's largest, most respected companies. We develop and deploy cutting-edge internal and go-to-market solutions that help Deloitte operate effectively and lead in the market. Our reputation is built on a tradition of delivering with excellence.\n\nKey Qualifications:\n• A bachelor's degree in computer science, software engineering, or a related discipline. An advanced degree (e.g., MS) is preferred but not required. Experience is the most relevant factor.\n• Excellent software engineering foundation with deep understanding of OOP/OOD, sequence/activity/state/ER/DFD diagrams, data-structure, algorithms, code instrumentations, etc.\n• 8+ years proven experience with Python, SQL/NoSQL.\n• 8+ years of experience with cloud-native engineering and understanding of Azure Data Pipelines/the Azure Portal Environment.\n• 8+ years delivering governed data platforms.\n• 2+ years of experience with AI/ML and GenAI.\n• Strong understanding of methodologies & tools like, XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc. to deliver high quality products rapidly.\n• Deep experience with at least one modern data platform and its governance controls (e.g., Databricks, Snowflake, BigQuery, Redshift, Synapse).\n• Experience with implementing data governance and lifecycle management controls across Microsoft 365 (M365) applications including Teams, OneDrive, SharePoint, CoPilot, etc.\n• Experience with governance tools like Microsoft Purview.\n• Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care.\n• Limited immigration sponsorship may be available\n\nThe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $107,700 to $221,200.\n\nYou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.\n\nInformation for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html\n\nEA_ITS_ExpHire\n\nPXE_JOBS\n\nDeloitte is committed to providing reasonable accommodations for people with disabilities. If you require a reasonable accommodation to participate in the recruiting process, please direct your inquiries to the Global Call Center (GCC) at USTalentCICInbox@deloitte.com.\n\nRecruiting tips\n\nFrom developing a stand out resume to putting your best foot forward in the interview, we want you to feel prepared and confident as you explore opportunities at Deloitte. Check out recruiting tips from Deloitte recruiters.\n\nBenefits\n\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\n\nOur people and culture\n\nOur inclusive culture empowers our people to be who they are, contribute their unique perspectives, and make a difference individually and collectively. It enables us to leverage different ways of thinking, ideas, and perspectives, and bring more creativity and innovation to help solve our clients' most complex challenges. This makes Deloitte one of the most rewarding places to work.\n\nOur purpose\n\nDeloitte's purpose is to make an impact that matters for our people, clients, and communities. At Deloitte, purpose is synonymous with how we work every day. It defines who we are. Our purpose comes through in our work with clients that enables impact and value in their organizations, as well as through our own investments, commitments, and actions across areas that help drive positive outcomes for our communities. Learn more.\n\nProfessional development\n\nFrom entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to build new skills, take on leadership opportunities and connect and grow through mentorship. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.\n\nAs used in this posting, \"Deloitte\" means Deloitte Services LP, a subsidiary of Deloitte LLP. Please see www.deloitte.com/us/about for a detailed description of the legal structure of Deloitte LLP and its subsidiaries.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.\n\nQualified applicants with criminal histories, including arrest or conviction records, will be considered for employment in accordance with the requirements of applicable state and local laws, including the Los Angeles County Fair Chance Ordinance for Employers, City of Los Angeles's Fair Chance Initiative for Hiring Ordinance, San Francisco Fair Chance Ordinance, and the California Fair Chance Act. See notices of various fair chance hiring and ban-the-box laws where available. Fair Chance Hiring and Ban-the-Box Notices | Deloitte US Careers\n\nRequisition code: 323572\n\nJob ID 323572",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771113600,
    "job_posted_at_datetime_utc": "2026-02-15T00:00:00.000Z",
    "job_location": "Williamsville, NY",
    "job_city": "Williamsville",
    "job_state": "New York",
    "job_country": "US",
    "job_latitude": 42.963947,
    "job_longitude": -78.73780909999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzXyI7pf5emLDB5aTAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 108000,
    "job_max_salary": 221000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate is a hands-on technical leader and mentor who partners closely with data governance, security, privacy, legal, and platform teams to deliver reliable, outcome-driven lifecycle solutions",
        "Domain Expertise: Quickly acquire domain-specific knowledge relevant to the business or product",
        "Translate business/user needs, architectures, and UX/UI designs into technical specifications and code",
        "Be a valuable, flexible, and dedicated team member, supportive of teammates, and focused on quality and tech debt payoff",
        "Effective Communication and Influence: Exhibit exceptional communication skills, capable of articulating complex technical concepts clearly and compellingly",
        "A bachelor's degree in computer science, software engineering, or a related discipline",
        "Experience is the most relevant factor",
        "Excellent software engineering foundation with deep understanding of OOP/OOD, sequence/activity/state/ER/DFD diagrams, data-structure, algorithms, code instrumentations, etc",
        "8+ years proven experience with Python, SQL/NoSQL",
        "8+ years of experience with cloud-native engineering and understanding of Azure Data Pipelines/the Azure Portal Environment",
        "8+ years delivering governed data platforms",
        "2+ years of experience with AI/ML and GenAI",
        "Strong understanding of methodologies & tools like, XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc",
        "Deep experience with at least one modern data platform and its governance controls (e.g., Databricks, Snowflake, BigQuery, Redshift, Synapse)",
        "Experience with implementing data governance and lifecycle management controls across Microsoft 365 (M365) applications including Teams, OneDrive, SharePoint, CoPilot, etc",
        "Experience with governance tools like Microsoft Purview",
        "Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care",
        "Limited immigration sponsorship may be available"
      ],
      "Benefits": [
        "The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs",
        "A reasonable estimate of the current range is $107,700 to $221,200",
        "You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance"
      ],
      "Responsibilities": [
        "Role Overview: As a Lead Engineer (Data Lifecycle Management), you will take a hands-on role in designing, building, and operating data platform capabilities that govern how data is created, classified, used/shared, retained, archived, and deleted within scope",
        "You will lead delivery of lifecycle controls-such as metadata capture and lineage, data classification and policy enforcement, retention/legal hold workflows, and audit-ready evidence and reporting-embedded directly into modern data stacks (cloud storage, lakehouse/warehouse, and data pipelines)",
        "Your expertise will be pivotal in turning governance, privacy, and regulatory requirements into scalable engineering patterns and automation, improving data trust, reducing operational risk, and enabling compliant reuse of data products",
        "Outcome-Driven Accountability: Embrace and drive a culture of accountability for customer and business outcomes",
        "Develop engineering solutions that solve complex problems with valuable outcomes, ensuring high-quality, lean designs and implementations",
        "Technical Leadership and Advocacy: Serve as the technical advocate for products, ensuring code integrity, feasibility, and alignment with business and customer goals",
        "Lead requirement analysis, contributing to low-level architecture and component design, development, unit testing, integrations, and support",
        "Engineering Craftsmanship: Maintain accountability for the integrity of code design, implementation, quality, data, and ongoing maintenance and operations",
        "Stay hands-on, self-driven, and continuously learn new approaches, languages, and frameworks",
        "Create technical specifications, and write high-quality, supportable, scalable code and review code of other engineers, mentoring them, to ensure all quality KPIs are met or exceeded",
        "Demonstrate collaborative skills to work effectively with diverse teams",
        "Customer-Centric Engineering: Develop lean engineering solutions through rapid, inexpensive experimentation to solve customer needs",
        "Engage with customers and product teams before, during, and after delivery to ensure the right solution is delivered at the right time",
        "Incremental and Iterative Delivery: Adopt a mindset that favors action and evidence over extensive planning",
        "Utilize a leaning-forward approach to navigate complexity and uncertainty, delivering lean, supportable, and maintainable solutions",
        "Cross-Functional Collaboration and Integration: Work collaboratively with empowered, cross-functional teams including product management, experience, and delivery",
        "Integrate diverse perspectives to make well-informed decisions that balance feasibility, viability, usability, and value",
        "Foster a collaborative environment that enhances team synergy and innovation",
        "Advanced Technical Proficiency: Possess deep expertise in modern software engineering practices and principles, including Agile methodologies and DevSecOps to deliver daily product deployments using full automation from code check-in to production with all quality checks through SDLC lifecycle",
        "Strive to be a role model, leveraging these techniques to optimize solutioning and product delivery",
        "Demonstrate strong understanding of the full lifecycle product development, focusing on continuous improvement and learning",
        "Inspire and influence teammates and product teams through well-structured arguments and trade-offs supported by evidence",
        "Create coherent narratives that align technical solutions with business objectives",
        "Engagement and Collaborative Co-Creation: Engage and collaborate with product engineering teams at all organizational levels, including customers as needed",
        "Build and maintain constructive relationships, fostering a culture of co-creation and shared momentum towards achieving product goals",
        "Align diverse perspectives and drive consensus to create feasible solutions",
        "to deliver high quality products rapidly"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-lead-data-engineer-williamsville-deloitte-47032ddc0c5490a771c870be10facd02",
    "_source": "new_jobs"
  },
  {
    "job_id": "vmP5DjcyLh4_B94fAAAAAA==",
    "job_title": "Senior Data Engineer (Remote)",
    "employer_name": "Parsons Corporation",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWoeHM7rt-U37K1_ab3N5lIK6QuMuDbepnStZr&s=0",
    "employer_website": "https://www.parsons.com",
    "job_publisher": "Parsons Careers - Parsons Corporation",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Parsons Careers - Parsons Corporation",
        "apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "In a world of possibilities, pursue one with endless opportunities. Imagine Next!\n\nAt Parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. Guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. Unleash your talent and redefine what’s possible.\n\nJob Description:\n\nParsons is looking for an amazingly talented Senior Data Engineer to join our team! In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization.\n\nWhat You'll Be Doing:\n• Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture.\n• Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake.\n• Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing.\n• Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing.\n• Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation.\n• Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms.\n• Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data.\n• Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement.\n\nWhat Required Skills You'll Bring:\n• Strong hands-on experience with T-SQL and Python.\n• Experience with comprehensive data conversion projects is preferred (ERP systems including Oracle Cloud ERP and/or SAP S4/HANA)\n• Experience with Relational Database systems\n• Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)\n• Familiar with multi-dimensional and tabular models\n• 5+ years of experience in data engineering, data architecture, or data platform development.\n• Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar).\n• Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines.\n• Deep understanding of lakehouse architecture and medallion design patterns.\n• Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines.\n• Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats.\n• Strong problem-solving skills and ability to work independently in a fast-paced environment.\n• US person\n\nWhat Desired Skills You'll Bring:\n• Experience with data governance, security, and compliance (e.g., SOX, HIPAA).\n• Snowflake, Azure Data Engineer, dbt, and/or Databricks certifications\n• Exposure to real-time data processing and streaming technologies (e.g., Kafka, Spark Streaming).\n• Familiarity with data observability tools and automated testing frameworks for pipelines.\n• Bachelor's or Master’s degree in Computer Science, Information Systems, or a related field\n\nSecurity Clearance Requirement:\nNone\n\nThis position is part of our Corporate team.\n\nFor over 80 years, Parsons Corporation, has shaped the future of the defense, intelligence, and critical infrastructure markets. Our employees work in a close-knit team environment to find new, innovative ways to deliver smart solutions that are used and valued by customers around the world. By combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, transportation, smart cities, and more, we're providing tomorrow's solutions today.\n\nSalary Range: $100,900.00 - $176,600.00\n\nWe value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!\n\nThis position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled.\n\nParsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status.\n\nWe truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! Imagine next and join the Parsons quest—APPLY TODAY!\n\nParsons is aware of fraudulent recruitment practices. To learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Virginia",
    "job_city": null,
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 37.4315734,
    "job_longitude": -78.6568942,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvmP5DjcyLh4_B94fAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 101000,
    "job_max_salary": 177000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Strong hands-on experience with T-SQL and Python",
        "Experience with Relational Database systems",
        "Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)",
        "Familiar with multi-dimensional and tabular models",
        "5+ years of experience in data engineering, data architecture, or data platform development",
        "Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar)",
        "Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines",
        "Deep understanding of lakehouse architecture and medallion design patterns",
        "Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines",
        "Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats",
        "Strong problem-solving skills and ability to work independently in a fast-paced environment",
        "US person"
      ],
      "Benefits": [
        "Salary Range: $100,900.00 - $176,600.00",
        "We value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!"
      ],
      "Responsibilities": [
        "In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization",
        "Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture",
        "Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake",
        "Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing",
        "Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing",
        "Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation",
        "Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms",
        "Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data",
        "Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-parsons-com-jobs-senior-data-engineer-remote-virtual-r-174702-jobs-information-technology",
    "_source": "new_jobs"
  },
  {
    "job_id": "fKq613YVZoGaO3ZhAAAAAA==",
    "job_title": "Temporary  Student Data Engineer",
    "employer_name": "University of Notre Dame",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB_bk0V1eXoFSLkXbpRAL9w5OPDz55ACjhxHof&s=0",
    "employer_website": null,
    "job_publisher": "BMES Career Center",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BMES Career Center",
        "apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/5e385fd85cc450ceae1fd6846215958e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/698af9934db8972cec006c83?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/university-of-notre-dame/notre-dame-in/student-data-engineer-temporary/b56cc5f7b3f351a3edb50ec87faf8e52?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/temporary-student-data-engineer--notre-dame--e272bff6f756c6e91c32e5a9ffa44cbfd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido",
        "apply_link": "https://us.jobrapido.com/jobpreview/3091304044883345408?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/indiana/business/4868026961/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-notre-dame-temporary-student-data-engineer-university-part_time?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temporary Student Data Engineer\n\nNotre Dame, IN, United States\nContract\nProvost\nTemporary\n\nCompany Description\n\nJob Description\nWe are seeking a Student Data Engineer (SDE) to join a team of students creating a Roblox game as a data collection tool, gauging students' interest in STEM and health care careers. The Lucy Family Institute for Data & Society (LFIDS) leverages data science, AI & ML toward social good. LFIDS engages with the Notre Dame community & beyond through funded research projects & collaborations, educational workshops, and special events. SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana. In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed.\n\nKey Responsibilities:\n\nWithin assigned projects, this role requires completion of data processing and programming tasks related to:\n• Data collection, management, harvesting, processing, transformation, and visualization;\n• Prototype data processing solutions;\n• Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms.\n\nThe successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners. Other responsibilities may include data analysis and giving presentations to diverse audiences.\n\nAdditional Requirements\n\nIn addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested. For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience.\n\nAdditional Opportunities\n\nStudent data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects. Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately.\n\nThe Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests.\n\nCore Qualities & Expectations\n• Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving\n• Confidentiality: Maintaining confidentiality is required.\n• Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings).\n• Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly. Perfection is not expected-it's okay to take time to learn. What matters is trying your best in each unique circumstance. We're committed to supporting your growth and confidence in the role.\n• Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities. We aim to provide a structure that supports your success.\n• Attention to detail or the ability to follow a set of instructions that we'll co-create and adjust based on your preferred learning and working style.\n• Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer.\n\nQualifications\nWe are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:\n• Data science methods and tools,\n• Software design\n• User experience principles\n\nExperience in an LFIDS area of expertise, like:\n• R and/or Python\n• Events and communications support\n• Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites).\n\nAdditional Information\nCompensation: $17.00/hour\n\nApplications for this position will close on February 13, 2026.\n\nThe University of Notre Dame seeks to attract, develop, and retain the highest quality faculty, staff and administration. The University is an Equal Opportunity Employer, and does not discriminate on the basis of race, color, national or ethnic origin, sex, disability, veteran status, genetic information, or age in employment. Moreover, Notre Dame prohibits discrimination against veterans or disabled qualified individuals, and complies with 41 CFR 60-741.5(a) and 41 CFR 60-300.5(a). We strongly encourage applications from candidates attracted to a university with a Catholic identity.\n\nTo apply, visit https://jobs.smartrecruiters.com/UniversityOfNotreDame/3743990011597785-temporary-student-data-engineer\n\nCopyright 2025 Jobelephant.com Inc. All rights reserved.\n\nPosted by the FREE value-added recruitment advertising agency jeid-5691f5d28abbde4e8b71884761a96763",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Notre Dame, IN",
    "job_city": "Notre Dame",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.7001908,
    "job_longitude": -86.2379328,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DfKq613YVZoGaO3ZhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Student data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects",
        "Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving",
        "Confidentiality: Maintaining confidentiality is required",
        "Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings)",
        "Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly",
        "Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities",
        "We aim to provide a structure that supports your success",
        "Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer",
        "We are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:",
        "Data science methods and tools,",
        "Software design",
        "User experience principles",
        "Experience in an LFIDS area of expertise, like:",
        "R and/or Python",
        "Events and communications support",
        "Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites)"
      ],
      "Benefits": [
        "Compensation: $17.00/hour"
      ],
      "Responsibilities": [
        "SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana",
        "In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed",
        "Within assigned projects, this role requires completion of data processing and programming tasks related to:",
        "Data collection, management, harvesting, processing, transformation, and visualization;",
        "Prototype data processing solutions;",
        "Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms",
        "The successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners",
        "Other responsibilities may include data analysis and giving presentations to diverse audiences",
        "In addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested",
        "For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience",
        "Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately",
        "The Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobboard-bmes-org-jobs-22037211-temporary-student-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "8ojyM0IL2Dgweb6mAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Guardianlife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEHyPUSx8in9lbGwtt3n-urbaK-a93llGwRZYg&s=0",
    "employer_website": "https://www.guardianlife.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6984c41c0f6f7e7a2cdf36a2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/guardian-life/holmdel-nj/lead-data-engineer/448a3771cda355807800f3a245f659f2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5618263952?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/lead-data-engineer-holmdel-new-jersey-us-guardian-life-r000108510?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/lead-data-engineer--holmdel-township--e33d7f694afe11a60b979debb2c4bd89a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/40aefed101891767dbcd5f364e470e55?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/holmdel-township/new-jersey/software_development/4858586430/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Guardian is seeking a highly skilled and motivated Lead Data Engineer to join the FPRS&CSWM Data Engineering team. In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases. Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient. The ideal candidate will have a passion for data engineering, thrive in a collaborative environment and are excited about leveraging cutting-edge technologies to drive business success.\n\nYour contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers. You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs. We value curiosity, creativity, and continuous learning. If you're passionate about solving meaningful problems and creating value through data-driven innovation, we look forward to welcoming you to our team.\n\nYou will\n• Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables.\n• Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth.\n• Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products\n• Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems.\n• Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.\n• Construct meaningful data assets sourced from structured, semi structured, and unstructured data.\n• Develop real-time data solutions by creating new API endpoints or streaming frameworks.\n• Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle.\n• Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams.\n• Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication.\n• Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions.\n• Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives.\n• Stay up-to-date with the latest trends in modern data engineering, machine learning & AI.\n\nYou have\n• Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field.\n• 5+ years of experience working with Python, SQL, PySpark, and bash scripts. Proficient in software development lifecycle and software engineering practices.\n• 4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases.\n• 3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark.\n• 2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality.\n• Solid understanding of data modeling and warehousing techniques. Experience working in a data warehouse is a plus.\n• Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities.\n• Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries).\n• Proficient in understanding and incorporating software engineering principles in design & development process.\n• Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent).\n• Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business.\n\nLocation\n• Three days a week at a Guardian office in Bethlehem, PA, New York, NY. Pittsfield, MA or Holmdel, NJ.\n\nSalary Range:\n\n$99,150.00 - $162,885.00\n\nThe salary range reflected above is a good faith estimate of base pay for the primary location of the position. The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate. In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation.\n\nOur Promise\n\nAt Guardian, you'll have the support and flexibility to achieve your professional and personal goals. Through skill-building, leadership development and philanthropic opportunities, we provide opportunities to build communities and grow your career, surrounded by diverse colleagues with high ethical standards.\n\nInspire Well-Being\n\nAs part of Guardian's Purpose - to inspire well-being - we are committed to offering contemporary, supportive, flexible, and inclusive benefits and resources to our colleagues. Explore our company benefits at www.guardianlife.com/careers/corporate/benefits.Benefits apply to full-time eligible employees. Interns are not eligible for most Company benefits.\n\nEqual Employment Opportunity\n\nGuardian is an equal opportunity employer. All qualified applicants will be considered for employment without regard to age, race, color, creed, religion, sex, affectional or sexual orientation, national origin, ancestry, marital status, disability, military or veteran status, or any other classification protected by applicable law.\n\nAccommodations\n\nGuardian is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities.Guardian also provides reasonable accommodations to qualified job applicants (and employees) to accommodate the individual's known limitations related to pregnancy, childbirth, or related medical conditions, unless doing so would create an undue hardship. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact MyHR@glic.com. Please note: this resource is for accommodation requests only. For all other inquires related to your application and careers at Guardian, refer to the Guardian Careers site.\n\nVisa Sponsorship\n\nGuardian is not currently or in the foreseeable future sponsoring employment visas. In order to be a successful applicant. you must be legally authorized to work in the United States, without the need for employer sponsorship.\n\nCurrent Guardian Colleagues: Please apply through the internal Jobs Hub in Workday.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Holmdel, NJ",
    "job_city": "Holmdel",
    "job_state": "New Jersey",
    "job_country": "US",
    "job_latitude": 40.3848944,
    "job_longitude": -74.18900599999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D8ojyM0IL2Dgweb6mAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 99150,
    "job_max_salary": 162885,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "We value curiosity, creativity, and continuous learning",
        "Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field",
        "5+ years of experience working with Python, SQL, PySpark, and bash scripts",
        "Proficient in software development lifecycle and software engineering practices",
        "4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases",
        "3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark",
        "2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality",
        "Solid understanding of data modeling and warehousing techniques",
        "Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities",
        "Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries)",
        "Proficient in understanding and incorporating software engineering principles in design & development process",
        "Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent)",
        "Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business",
        "you must be legally authorized to work in the United States, without the need for employer sponsorship"
      ],
      "Benefits": [
        "$99,150.00 - $162,885.00",
        "The salary range reflected above is a good faith estimate of base pay for the primary location of the position",
        "The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate",
        "In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation",
        "Interns are not eligible for most Company benefits"
      ],
      "Responsibilities": [
        "In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases",
        "Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient",
        "Your contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers",
        "You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs",
        "Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables",
        "Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth",
        "Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products",
        "Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems",
        "Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues",
        "Construct meaningful data assets sourced from structured, semi structured, and unstructured data",
        "Develop real-time data solutions by creating new API endpoints or streaming frameworks",
        "Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle",
        "Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams",
        "Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication",
        "Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions",
        "Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives",
        "Stay up-to-date with the latest trends in modern data engineering, machine learning & AI"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-guardian-life-job-lead-data-engineer-in-holmdel-nj",
    "_source": "new_jobs"
  },
  {
    "job_id": "J3qD7yLhFTV7L6odAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Slalom",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTz83P7TkZM86wEdE65TUA9A-yqu267uumzStA3&s=0",
    "employer_website": "https://www.slalom.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-slalom-4369144916?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-slalom-4369144916?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-engineer?id=2439287257&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Web Hosting Services",
        "apply_link": "https://perris.id.au/.career/job/data-engineer-at-procore-technologies-oregon-T2VtelRkR25vSlpRc1c4b1hhMVIxdz09?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Role: Data Engineer\n\nWho You’ll Work With\n\nAt Slalom we co-create custom software, data and cloud products with clients who are ready to accelerate their digital transformation. We're passionate about technology, compelled by its potential as we help create the digital products, experiences, and technology-driven organizations that drive true change. We’re thrilled by the opportunity to build the future we want to see, with anyone willing to join us.\n\nSlalom's Data Engineering Discipline Is Focused On Injecting Intelligence Into Products, Engineering Systems That Support Learning And Insight And Creating Innovative Data Products. Within Data Engineering We Help Customers Build World-class Products Through Effective Use Of\n• Data engineering consisting of streaming / real-time data solutions, modern data platforms and data systems within products (i.e., database systems, graph databases, key-value stores, document databases and transactional systems)\n• Enhancing Machine Learning and Artificial Intelligence capabilities\n• You will work collaboratively with teams in a hybrid environment, with expectation to be in-person with Slalom team members and clients as needed.\n• You also must be within commutable distance to one of the listed Slalom office locations for this role.\n\nWhat You’ll Do\n\nSlalom Data Engineering discipline is comprised of passionate, flexible technologists who love to practice and hone their craft. As tools evolve and technologies emerge, we work to stay in front of innovations in data platform development and delivery.\n\nAs a Data Engineer for Slalom, you will work in collaborative teams to deliver innovative solutions on Amazon Web Services, Microsoft Azure, and Google Cloud Platform using core cloud data warehouse tools, distributed processing engines, event streaming platforms, and other modern data technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.\n\nYou will be engaged to participate in design sessions and be responsible for the timely completion of development items assigned to you in a project backlog.\n\nWhat You’ll Bring\n\nYou will have an interest to become the best at what you do and will have many opportunities to gain hands-on experience with new data platforms and programming languages as you explore the range of technologies that we help our clients with including:\n• Big Data Platforms (Apache Spark, Presto, Amazon EMR)\n• Cloud Data Warehouses (Amazon Redshift, Snowflake, Google BigQuery)\n• Object Oriented Coding (Java, Python)\n• NoSQL Databases (DynamoDB, Cosmos DB, MongoDB)\n• Container Management Systems (Kubernetes, Amazon ECS)\n• Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)\n• Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)\n• Visual Analytics (Tableau, PowerBI)\n• Modern Data Workflows (Apache Airflow, dbt, Dagster)\n\nAbout Us\n\nSlalom is a fiercely human business and technology consulting company that leads with outcomes to bring more value, in all ways, always. From strategy through delivery, our agile teams across 52 offices in 12 countries collaborate with clients to bring powerful customer experiences, innovative ways of working, and new products and services to life. We are trusted by leaders across the Global 1000, many successful enterprise and mid-market companies, and 500+ public sector organizations to improve operations, drive growth, and create value. At Slalom, we believe that together, we can move faster, dream bigger, and build better tomorrows for all.\n\nCompensation And Benefits\n\nSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses, as well as discounted home, auto, and pet insurance.\n\nSlalom is committed to fair and equitable compensation practices. For this role, we are hiring at the following levels and targeted base pay salary ranges outlined below. In addition, individuals may be eligible for an annual discretionary bonus. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.\n• Denver, Portland\n• Consultant: $91,000 - $122,000\n• Seattle\n• Consultant: $99,000 - $133,000\n\nWe will accept applicants until April 5th, 2026, or until the position is filled.\n\nWe are committed to pay transparency and compliance with applicable laws. If you have questions or concerns about the pay range or other compensation information in this posting, please contact us at: peopleone@slalom.com.\n\nEEO and Accommodations\n\nSlalom is an equal opportunity employer and is committed to attracting, developing and retaining highly qualified talent who empower our innovative teams through unique perspectives and experiences. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements. Slalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the selection process. Please advise the talent acquisition team or contact accomodationrequest@slalom.com if you require accommodations during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "Oregon City, OR",
    "job_city": "Oregon City",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 45.3556099,
    "job_longitude": -122.605853,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DJ3qD7yLhFTV7L6odAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Big Data Platforms (Apache Spark, Presto, Amazon EMR)",
        "NoSQL Databases (DynamoDB, Cosmos DB, MongoDB)",
        "Modern Data Workflows (Apache Airflow, dbt, Dagster)",
        "Slalom will also consider qualified applications with criminal histories, consistent with legal requirements"
      ],
      "Benefits": [
        "Compensation And Benefits",
        "Slalom prides itself on helping team members thrive in their work and life",
        "As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, parental leave, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability",
        "We also offer yearly $350 reimbursement account for any well-being-related expenses, as well as discounted home, auto, and pet insurance",
        "Slalom is committed to fair and equitable compensation practices",
        "For this role, we are hiring at the following levels and targeted base pay salary ranges outlined below",
        "In addition, individuals may be eligible for an annual discretionary bonus",
        "Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors",
        "The salary pay range is subject to change and may be modified at any time",
        "Consultant: $99,000 - $133,000"
      ],
      "Responsibilities": [
        "Data engineering consisting of streaming / real-time data solutions, modern data platforms and data systems within products (i.e., database systems, graph databases, key-value stores, document databases and transactional systems)",
        "Enhancing Machine Learning and Artificial Intelligence capabilities",
        "You will work collaboratively with teams in a hybrid environment, with expectation to be in-person with Slalom team members and clients as needed",
        "You also must be within commutable distance to one of the listed Slalom office locations for this role",
        "Slalom Data Engineering discipline is comprised of passionate, flexible technologists who love to practice and hone their craft",
        "As a Data Engineer for Slalom, you will work in collaborative teams to deliver innovative solutions on Amazon Web Services, Microsoft Azure, and Google Cloud Platform using core cloud data warehouse tools, distributed processing engines, event streaming platforms, and other modern data technologies",
        "In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics",
        "You will be engaged to participate in design sessions and be responsible for the timely completion of development items assigned to you in a project backlog",
        "You will have an interest to become the best at what you do and will have many opportunities to gain hands-on experience with new data platforms and programming languages as you explore the range of technologies that we help our clients with including:",
        "Cloud Data Warehouses (Amazon Redshift, Snowflake, Google BigQuery)",
        "Object Oriented Coding (Java, Python)",
        "Container Management Systems (Kubernetes, Amazon ECS)",
        "Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)",
        "Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)",
        "Visual Analytics (Tableau, PowerBI)"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-data-engineer-at-slalom-4369144916",
    "_source": "new_jobs"
  },
  {
    "job_id": "XQf3XW1gRIsW1UiuAAAAAA==",
    "job_title": "AWS Redshift Data Engineer Data Architect",
    "employer_name": "Numentica LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTootVXWXzkNN4CMEZIpwL0zphlLjGSX6fP41El&s=0",
    "employer_website": "https://numentica.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This is a remote position.\n\nRole : AWS Redshift Data Engineer / Data Architect - Remote\n\nOverview\n\nWe are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture.\n\nKey Responsibilities\n\nOptimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS).\n\nLead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime.\n\nAnalyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads.\n\nReview and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis.\n\nDesign data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency.\n\nCollaborate with stakeholders on architecture recommendations and best practices.\n\nRequired Skills\n\nStrong hands-on experience with Amazon Redshift (Provisioned Serverless RMS).\n\nDeep SQL performance tuning and query optimization expertise.\n\nSolid AWS data engineering background (RDS DynamoDB Kinesis).\n\nExperience with modern lakehouse concepts especially Apache Iceberg.\n\nData architecture and analytical platform design experience.\n\nNice to Have\n\nExposure to AI-assisted query optimization or analytics platforms.\n\nExperience building or supporting BI and analytics use cases at scale.\n\nRequired Skills :\n\nAWS Redshift Data Engineer / Data Architect\n\nKey Skills\n\nFund Management,Drafting,End User Support,Infrastructure,Airlines,Catia\n\nEmployment Type : Full Time\n\nExperience : years\n\nVacancy : 1",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXQf3XW1gRIsW1UiuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture",
        "Strong hands-on experience with Amazon Redshift (Provisioned Serverless RMS)",
        "Deep SQL performance tuning and query optimization expertise",
        "Solid AWS data engineering background (RDS DynamoDB Kinesis)",
        "Experience with modern lakehouse concepts especially Apache Iceberg",
        "Data architecture and analytical platform design experience",
        "Exposure to AI-assisted query optimization or analytics platforms",
        "Experience building or supporting BI and analytics use cases at scale",
        "AWS Redshift Data Engineer / Data Architect"
      ],
      "Responsibilities": [
        "Optimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS)",
        "Lead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime",
        "Analyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads",
        "Review and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis",
        "Design data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency",
        "Collaborate with stakeholders on architecture recommendations and best practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "652Pig-K-CYFplikAAAAAA==",
    "job_title": "Principal Software Engineer - Big Data Processing",
    "employer_name": "Oracle",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS8Ob8wBy30IQeYJfM4qn_PksuLPYaC4sUtJLRK&s=0",
    "employer_website": "https://www.oracle.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/data-engineering?id=2377355977&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-engineering?id=2377355977&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "**Job Description**\nOracle Health Data Intelligence has a rare opportunity to play a critical role in how Oracle Health products impact and disrupt the healthcare industry by transforming how healthcare and technology intersect.\nYou will have the opportunity to:\n+ Reach billions of people with our products and services\n+ Create technology which truly impacts the world\n+ Have immediate impact on developing this technology\n+ Unlimited growth potential with inspiring work\n+ Work with the best minds in the industry\n+ Work in an open, diverse, and productive environment\n• *About the Job**\nOracle Health Data Intelligence is growing and looking for a Principal Software Engineer to join the Population Intelligence team. This team aggregates data across multiple disparate sources and normalizes the data, enabling advanced decision support, predictive algorithms, population identification, and advanced analytics.\n• *Responsibilities**\n• *What You'll Do**\nAs a member of the software engineering division, you will apply intermediate to advanced knowledge of software architecture to perform software development tasks associated with developing, debugging, or designing software applications or operating systems according to provided design specifications. Build enhancements within an existing software architecture and suggest improvements to the architecture. Work involves problem solving, understanding, and applying company policies and processes.\n+ High level of fluency with at least one modern programming language such as Java, C++, or C#.\n+ Expertise in design concepts and technologies around Big Data Processing, ETL, Relational Databases, Hadoop Ecosystem, Apache Crunch, Spark, Hive, and SQL.\n+ Experience building and maintaining large scalable enterprise applications, distributed systems, and cloud-based platforms.\n+ Solid understanding of modern web applications from database to user interface.\n+ Familiarity with micro-services architecture and RESTful APIs.\n+ Familiarity with web front-end technologies such as React, Preact, Ruby, Ruby on Rails, JavaScript, and Oracle JET.\n+ Proficiency in design patterns, SOLID principles, and writing well-modularized and maintainable software.\n+ Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.\n+ Knowledge of operations processes and DevOps technologies such as Jenkins, Kubernetes, Spinnaker, Maven.\n+ Excellent communication skills in both technical and non-technical contexts.\n• *Required Qualifications:**\n+ 8+ years of technical experience relevant to this position.\n+ Ability to communicate effectively and build rapport with team members.\n+ BS or MS in Computer Science, or equivalent.\nDisclaimer:\n• *Certain US customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.**\n• *Range and benefit information provided in this posting are specific to the stated locations only**\nUS: Hiring Range in USD from: $96,800 to $223,400 per annum. May be eligible for bonus and equity.\nOracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle's differing products, industries and lines of business.\nCandidates are typically placed into the range based on the preceding factors as well as internal peer equity.\nOracle US offers a comprehensive benefits package which includes the following:\n1. Medical, dental, and vision insurance, including expert medical opinion\n2. Short term disability and long term disability\n3. Life insurance and AD&D\n4. Supplemental life insurance (Employee/Spouse/Child)\n5. Health care and dependent care Flexible Spending Accounts\n6. Pre-tax commuter and parking benefits\n7. 401(k) Savings and Investment Plan with company match\n8. Paid time off: Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. Accrued Vacation is provided to all other employees eligible for vacation benefits. For employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment. Vacation accrual is prorated for employees working between 20 and 34 hours per week. Employees working fewer than 20 hours per week are not eligible for vacation.\n9. 11 paid holidays\n10. Paid sick leave: 72 hours of paid sick leave upon date of hire. Refreshes each calendar year. Unused balance will carry over each year up to a maximum cap of 112 hours.\n11. Paid parental leave\n12. Adoption assistance\n13. Employee Stock Purchase Plan\n14. Financial planning and group legal\n15. Voluntary benefits including auto, homeowner and pet insurance\nThe role will generally accept applications for at least three calendar days from the posting date or as long as the job remains posted.\nCareer Level - IC4\n• *About Us**\nOnly Oracle brings together the data, infrastructure, applications, and expertise to power everything from industry innovations to life-saving care. And with AI embedded across our products and services, we help customers turn that promise into a better future for all. Discover your potential at a company leading the way in AI and cloud solutions that impact billions of lives.\nTrue innovation starts when everyone is empowered to contribute. That's why we're committed to growing a workforce that promotes opportunities for all with competitive benefits that support our people with flexible medical, life insurance, and retirement options. We also encourage employees to give back to their communities through our volunteer programs.\nWe're committed to including people with disabilities at all stages of the employment process. If you require accessibility assistance or accommodation for a disability at any point, let us know by emailing or by calling in the United States.\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans' status, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.",
    "job_is_remote": false,
    "job_posted_at": "10 days ago",
    "job_posted_at_timestamp": 1770422400,
    "job_posted_at_datetime_utc": "2026-02-07T00:00:00.000Z",
    "job_location": "Madison, NH",
    "job_city": "Madison",
    "job_state": "New Hampshire",
    "job_country": "US",
    "job_latitude": 43.8992395,
    "job_longitude": -71.1484028,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D652Pig-K-CYFplikAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Work with the best minds in the industry",
        "Work in an open, diverse, and productive environment",
        "High level of fluency with at least one modern programming language such as Java, C++, or C#",
        "Expertise in design concepts and technologies around Big Data Processing, ETL, Relational Databases, Hadoop Ecosystem, Apache Crunch, Spark, Hive, and SQL",
        "Experience building and maintaining large scalable enterprise applications, distributed systems, and cloud-based platforms",
        "Solid understanding of modern web applications from database to user interface",
        "Familiarity with micro-services architecture and RESTful APIs",
        "Familiarity with web front-end technologies such as React, Preact, Ruby, Ruby on Rails, JavaScript, and Oracle JET",
        "Proficiency in design patterns, SOLID principles, and writing well-modularized and maintainable software",
        "Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations",
        "Knowledge of operations processes and DevOps technologies such as Jenkins, Kubernetes, Spinnaker, Maven",
        "Excellent communication skills in both technical and non-technical contexts",
        "8+ years of technical experience relevant to this position",
        "Ability to communicate effectively and build rapport with team members",
        "BS or MS in Computer Science, or equivalent"
      ],
      "Benefits": [
        "Unlimited growth potential with inspiring work",
        "US: Hiring Range in USD from: $96,800 to $223,400 per annum",
        "May be eligible for bonus and equity",
        "Oracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle's differing products, industries and lines of business",
        "Oracle US offers a comprehensive benefits package which includes the following:",
        "Medical, dental, and vision insurance, including expert medical opinion",
        "Short term disability and long term disability",
        "Life insurance and AD&D",
        "Supplemental life insurance (Employee/Spouse/Child)",
        "Health care and dependent care Flexible Spending Accounts",
        "Pre-tax commuter and parking benefits",
        "401(k) Savings and Investment Plan with company match",
        "Paid time off: Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position",
        "Accrued Vacation is provided to all other employees eligible for vacation benefits",
        "For employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment",
        "Vacation accrual is prorated for employees working between 20 and 34 hours per week",
        "Employees working fewer than 20 hours per week are not eligible for vacation",
        "11 paid holidays",
        "Paid sick leave: 72 hours of paid sick leave upon date of hire",
        "Refreshes each calendar year",
        "Unused balance will carry over each year up to a maximum cap of 112 hours",
        "Paid parental leave",
        "Adoption assistance",
        "Employee Stock Purchase Plan",
        "Financial planning and group legal",
        "Voluntary benefits including auto, homeowner and pet insurance",
        "That's why we're committed to growing a workforce that promotes opportunities for all with competitive benefits that support our people with flexible medical, life insurance, and retirement options"
      ],
      "Responsibilities": [
        "Reach billions of people with our products and services",
        "Create technology which truly impacts the world",
        "As a member of the software engineering division, you will apply intermediate to advanced knowledge of software architecture to perform software development tasks associated with developing, debugging, or designing software applications or operating systems according to provided design specifications",
        "Build enhancements within an existing software architecture and suggest improvements to the architecture",
        "Work involves problem solving, understanding, and applying company policies and processes",
        "*Certain US customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.**"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-data-engineering",
    "_source": "new_jobs"
  },
  {
    "job_id": "VpHFbpEQe5weQK_1AAAAAA==",
    "job_title": "Walmart Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQV0kWbPp7sIFysOBbrsohip-f8-kSq7M6lcXJq&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Walmart",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://walmart.rightspotway.com/jobpage/rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Walmart",
        "apply_link": "https://walmart.rightspotway.com/jobpage/rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Walmart Data Engineer\n\nCompany Overview:\n\nWalmart Global Tech is a team of over 15,000 software engineers, data scientists, and service professionals delivering innovative technology solutions that enhance the retail experience for millions of customers and empower Walmart's 2.2 million associates. You will join a virtual-first, collaborative team focused on building the foundational intelligence through data, AI, and semantic modeling that drives autonomous decision-making across Walmart’s ecosystem.\n\nRole and Responsibilities of Walmart Data Engineer:\n\nYou will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously. You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning.\n• Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows.\n• Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state.\n• Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access.\n• Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations.\n• Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals.\n• Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources.\n• Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows.\n\nRequired Skills and Experience of Walmart Data Engineer:\n• 7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role.\n• Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem).\n• Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming).\n• Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI).\n• Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale.\n• Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning.\n• Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design.\n• Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience.\n• 4 years’ experience in data engineering, database engineering, business intelligence, or business analytics.\n• 1 year’s supervisory experience.\n\nPreferred Qualifications:\n• Master’s degree in Computer Science or related field with 5 years’ experience in software engineering or related field.\n• Experience with ETL tools and managing large datasets in cloud environments.\n• Demonstrated knowledge of inclusive digital experiences, Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, and assistive technologies.\n• Background in implementing accessibility best practices for inclusive product and service delivery.\n\nCompensation and Benefits of Walmart Data Engineer:\n• Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses.\n• Stock options and participation in Walmart’s stock purchase plan.\n• Health benefits including medical, vision, and dental coverage.\n• 401(k) plan with company match.\n• Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting.\n• Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement.\n• Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart.\n\nAbout Walmart:\n\nAt Walmart Global Tech, we innovate to simplify the retail experience, ensuring technology benefits millions of customers and supports associates worldwide. Our culture emphasizes inclusion, collaboration, and continuous learning. We empower teams to deliver meaningful solutions while fostering a flexible virtual-first work environment that encourages creativity, ownership, and professional growth. Being human-led is at the heart of every innovation we drive.\n\nFAQ:\n\nQ: What does a Walmart Data Engineer do?\n\nA: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making.\n\nQ: What skills are needed for a Data Engineer?\n\nA: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities.\n\nQ: What level is a Data Engineer?\n\nA: This is typically a mid-level technical role requiring experience in data engineering and analytics.\n\nQ: How can someone be successful as a Data Engineer?\n\nA: Success comes from building efficient data pipelines, ensuring data quality, collaborating with teams, and supporting business insights.",
    "job_is_remote": false,
    "job_posted_at": "14 hours ago",
    "job_posted_at_timestamp": 1771243200,
    "job_posted_at_datetime_utc": "2026-02-16T12:00:00.000Z",
    "job_location": "Glendale, CO",
    "job_city": "Glendale",
    "job_state": "Colorado",
    "job_country": "US",
    "job_latitude": 39.7049873,
    "job_longitude": -104.9335904,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVpHFbpEQe5weQK_1AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Required Skills and Experience of Walmart Data Engineer:",
        "7–10+ years of experience in Data Engineering with large-scale, distributed, fault-tolerant data platforms, including at least 3+ years in a leadership role",
        "Deep expertise in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, CAP theorem)",
        "Extensive experience with Kafka and event-driven architectures, including state management and schema evolution in streaming environments (Kafka Streams, Flink, Spark Streaming)",
        "Familiarity with Vector Databases (Pinecone, Milvus), Knowledge Graphs, and AI orchestration frameworks (LangChain, LlamaIndex, CrewAI)",
        "Cloud platform experience in GCP or Azure with BigQuery, Dataflow, and Pub/Sub at petabyte scale",
        "Advanced understanding of data modeling for LLMs, including schema design, latency, and context window management for AI reasoning",
        "Proficiency in Python, Java, or Scala, with strong focus on testability, maintainability, and high-performance system design",
        "Bachelor’s degree in Computer Science and 5 years’ experience in software engineering or related field, OR 7 years’ experience in software engineering, OR Master’s degree in Computer Science with 3 years’ experience",
        "4 years’ experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "A: Key skills include SQL, Python, ETL processes, cloud platforms, and strong data modeling capabilities",
        "A: This is typically a mid-level technical role requiring experience in data engineering and analytics"
      ],
      "Benefits": [
        "Annual salary range: $143,000 - $286,000, with additional annual or quarterly performance bonuses",
        "Stock options and participation in Walmart’s stock purchase plan",
        "Health benefits including medical, vision, and dental coverage",
        "401(k) plan with company match",
        "Paid time off including PTO, parental leave, family care leave, bereavement, jury duty, and voting",
        "Additional benefits including short-term and long-term disability, company discounts, Military Leave Pay, and adoption or surrogacy expense reimbursement",
        "Live Better U program covering education from high school completion to bachelor’s degrees, with tuition, books, and fees fully paid by Walmart"
      ],
      "Responsibilities": [
        "You will lead the transformation of Walmart’s massive data ecosystem into a semantic and contextual layer, enabling AI agents to reason, plan, and act autonomously",
        "You will manage a high-performing engineering team and architect scalable, real-time data pipelines optimized for AI reasoning",
        "Define the technical vision for a unified semantic layer, transforming structured, semi-structured, and unstructured data into context-aware representations consumable by LLMs and multi-agent workflows",
        "Own streaming architecture using Kafka, building low-latency event-driven pipelines to provide agents with real-time member context and environmental state",
        "Ensure data engineering excellence through scalable schemas, partitioning strategies, and high-performance indexing for petabyte-scale data access",
        "Lead design of Agentic Data pipelines that deliver high-fidelity, optimized data enriched with business logic to prevent hallucinations",
        "Manage, mentor, and develop a team of engineers, bridging AI/ML research, product, and platform engineering to align data roadmaps with autonomous decision-making goals",
        "Develop and optimize retrieval systems (RAG) to enable agents to efficiently discover and interact with proprietary knowledge bases across batch and streaming sources",
        "Implement operational excellence standards including telemetry, lineage, and auditability for autonomous and regulated AI workflows",
        "A: A Data Engineer designs, builds, and maintains data pipelines and systems to support analytics and business decision-making"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "walmart-rightspotway-com-jobpage-rii46wcir8db-78d9d5e918b01c-496430c4ad6c-3df6df",
    "_source": "new_jobs"
  },
  {
    "job_id": "XhgeNP4cggggaxCtAAAAAA==",
    "job_title": "(USA) Senior Manager, Data Engineering",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0XcK_pPTIiFuiAYrGraOGjkX3xDSPgjHRhL3B&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do...The Mission\nAt Sam’s Club, we are no longer just building dashboards or static pipelines; we are building the \"brain\" of the retail experience. As the Senior Engineering Manager for Agentic Data, you will be at the forefront of the AI revolution. Your mission is to evolve our massive data ecosystem into a Semantic & Contextual Layer—the foundational intelligence that allows AI Agents to reason, plan, and act autonomously for millions of members. You aren't just managing a team; you are architecting the bridge between raw data and autonomous action. You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time.\n________________________________________\nWhat You’ll Do\n• Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows.\n• Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents. You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state.\n• Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles. You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models.\n• Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations.\n• Strategic Leadership: Manage and mentor a high-performing team of engineers. You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals.\n• Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources.\n• Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows.\n________________________________________\nWhat You’ll Bring\n• Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role.\n• Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals. You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem).\n• Real-Time Expertise: Deep experience with Kafka and event-driven architectures. You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming).\n• Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI).\n• Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads.\n• Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality.\n• Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design.\n\nBenefits & Perks:\nBeyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWe believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.\n\nAbout Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.\n\nWe’re virtual\nWorking virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nThe annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years’ experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.\n4 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n1 year’s supervisory experience.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n809 11th Ave, Sunnyvale, CA 94089-4731, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "Cupertino, CA",
    "job_city": "Cupertino",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.322997799999996,
    "job_longitude": -122.03218229999999,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXhgeNP4cggggaxCtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 143000,
    "job_max_salary": 286000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role",
        "Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals",
        "You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem)",
        "You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming)",
        "Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI)",
        "Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads",
        "Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design",
        "Option 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field",
        "Option 2: 7 years’ experience in software engineering or related field",
        "Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field",
        "4 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "Beyond competitive pay, you can receive incentive awards for your performance",
        "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "The annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time",
        "Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows",
        "Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents",
        "You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state",
        "Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles",
        "You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models",
        "Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations",
        "Strategic Leadership: Manage and mentor a high-performing team of engineers",
        "You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals",
        "Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources",
        "Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows",
        "Real-Time Expertise: Deep experience with Kafka and event-driven architectures",
        "Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality"
      ]
    },
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-walmart-job-usa-senior-manager-data-engineering-in-cupertino-ca",
    "_source": "new_jobs"
  },
  {
    "job_id": "ybwkukVOvvN7CZ4IAAAAAA==",
    "job_title": "Data Engineer Prin",
    "employer_name": "American Electric Power",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0RrFTOdrhzngcyQL-2zVSyVH1kAYBiD2Gj2L_&s=0",
    "employer_website": null,
    "job_publisher": "Tallo",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Tallo",
        "apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This job listing in Franklin - OH has been recently added. Tallo will add a summary here for this job shortly.",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Franklin, OH",
    "job_city": "Franklin",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.5589474,
    "job_longitude": -84.30410739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DybwkukVOvvN7CZ4IAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "tallo-com-jobs-technology-data-engineer-oh-franklin-data-engineer-prin-5d6c21303973",
    "_source": "new_jobs"
  },
  {
    "job_id": "q4w1sbGeUlXZqnSwAAAAAA==",
    "job_title": "(USA) Senior, Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": null,
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Talentify",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do... Role summary:\n\nAs a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights. You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets. Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance. This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment.\nAbout the team:\n\nWe’re a small but high‑impact Data Engineering team building the data foundations that power measurement and feature innovation for our advertising products. Operating at the intersection of engineering, analytics, and product strategy, we deliver scalable pipelines and accurate performance insights that drive data‑informed decisions. Our agility and end‑to‑end ownership set us apart—we move quickly, collaborate closely across functions, and build systems that directly influence how advertisers measure success and how new ad features evolve. If you want to build reliable, scalable data systems with visible impact, this is the team for you.\nWhat you'll do:\n• Design and build scalable data pipelines and models using Databricks, PySpark, and SQL.\n• Integrate and transform data from multiple sources with strong quality, observability, and governance.\n• Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions.\n• Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management.\n• Optimize pipelines for reliability, performance, and cost across cloud environments.\n• Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions.\n\nWhat you'll bring:\n• Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling.\n• Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow.\n• Solid understanding of cloud platforms, especially Google Cloud Platform.\n• Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform.\n• Strong grounding in data governance, data quality, and compliance best practices.\n• Ability to design resilient data architectures across warehouses, lakes, and streaming systems.\n• Proven skill in translating complex business needs into effective data solutions.\n• Familiarity with machine learning concepts and how they integrate with data engineering workflows.\n• A passion for finding ways to integrate AI and LLMs into daily engineering\nactivities and products, while not compromising simplicity and code\nmaintainability.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nDallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00\nDenver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years’ experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related\nfield.\n2 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n14901 Quorum Dr, Dallas, TX 75254-7521, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Rangeley, ME",
    "job_city": "Rangeley",
    "job_state": "Maine",
    "job_country": "US",
    "job_latitude": 44.965682099999995,
    "job_longitude": -70.6427102,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dq4w1sbGeUlXZqnSwAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment",
        "Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling",
        "Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow",
        "Solid understanding of cloud platforms, especially Google Cloud Platform",
        "Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform",
        "Strong grounding in data governance, data quality, and compliance best practices",
        "Ability to design resilient data architectures across warehouses, lakes, and streaming systems",
        "Proven skill in translating complex business needs into effective data solutions",
        "Familiarity with machine learning concepts and how they integrate with data engineering workflows",
        "A passion for finding ways to integrate AI and LLMs into daily engineering",
        "activities and products, while not compromising simplicity and code",
        "Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field",
        "Option 2: 5 years’ experience in",
        "software engineering or related field",
        "Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related",
        "2 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "Dallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00",
        "Denver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "As a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights",
        "You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets",
        "Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance",
        "Design and build scalable data pipelines and models using Databricks, PySpark, and SQL",
        "Integrate and transform data from multiple sources with strong quality, observability, and governance",
        "Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions",
        "Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management",
        "Optimize pipelines for reliability, performance, and cost across cloud environments",
        "Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talentify-io-job-usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845",
    "_source": "new_jobs"
  },
  {
    "job_id": "h69CGzgflQdWtmsHAAAAAA==",
    "job_title": "Advanced Cloud Data Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=4d4ccca8d83f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=4d4ccca8d83f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for an Advanced Cloud Data Engineer for a contract position that is 100% remote.\n\nKey Responsibilities\n\nComplete analysis, design, and development of BI solutions\n\nDatabase development primarily in SSIS, Databricks, and SQL\n\nCollaborate with other developers to create and implement optimal solutions\n\nRequired Qualifications\n\nBachelor's degree in Data Analytics, MIS, Computer Science, or related area\n\n3+ years of experience in data engineering within a data warehouse\n\n3+ years of experience designing and developing ETLs with tools like SSIS, Databricks, or Python\n\nExperience working as part of an Agile Scrum team",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Raleigh, NC",
    "job_city": "Raleigh",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.779589699999995,
    "job_longitude": -78.6381787,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dh69CGzgflQdWtmsHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree in Data Analytics, MIS, Computer Science, or related area",
        "3+ years of experience in data engineering within a data warehouse",
        "3+ years of experience designing and developing ETLs with tools like SSIS, Databricks, or Python",
        "Experience working as part of an Agile Scrum team"
      ],
      "Responsibilities": [
        "Complete analysis, design, and development of BI solutions",
        "Database development primarily in SSIS, Databricks, and SQL",
        "Collaborate with other developers to create and implement optimal solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "Y4E5Ry_jMXZhJqZ0AAAAAA==",
    "job_title": "Senior Data Engineer (Hybrid: New York, NY - US)",
    "employer_name": "Energy Solutions - USA",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRpgxcxiBAtVhfhrhaRjnyyT-NmzJHIzfHTuZx_&s=0",
    "employer_website": null,
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-JV_IC1132348_KO0,42_KE43,63.htm?jl=1009998197539&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-JV_IC1132348_KO0,42_KE43,63.htm?jl=1009998197539&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/senior-data-engineer_7ea1a5828cd50166b000f5270394bdcb6b252?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/senior-pre-sales-solutions-engineer-av-it-new-york-ny-at-shure-incorporated-4364456775?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-pre-sales-solutions-engineer-av-it-new-york-ny-new-york-ny--a7599309-30a9-4b3c-aa9d-c8d4d1d024d5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Climatebase",
        "apply_link": "https://climatebase.org/job/68869193/senior-data-engineer-hybrid-new-york-ny---us?source=jobs_directory_algolia&queryID=8e8f39c6ab4ed11cd4744f33d6d81692&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Remotejob",
        "apply_link": "https://remotejob.bsebexam.org.in/senior-data-engineer-hybrid-new-york-ny-us?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5629776722?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Recruit.net",
        "apply_link": "https://www.recruit.net/job/pre-sales-solutions-engineer-av-jobs/E511D8BEFAF8E875?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Interested in joining a growing company where you will work with talented colleagues, enhance a supportive and energetic culture, and be part of the climate solution? At Energy Solutions, we focus on the big impacts. And we believe that market-based programs can be a powerful force to deliver large-scale energy, carbon, and water-use savings. Since 1995, we've harnessed that power to offer proven, performance-based solutions for our utility, government, and institutional customers.\n\nWe are currently seeking a Senior Data Engineer to join our Information Systems team to design, develop, and maintain data platforms that support the data needs across Energy Solutions. In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies. They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights. This unique position is perfect for individuals with technical prowess in data field who want to have an impact on energy efficiency markets and greenhouse gas reductions through our work for major North American utilities and other clients around the country.\n\nThis is a hybrid work opportunity. ES has offices in Oakland, CA, Orange, CA, Portland, OR, Chicago, IL, and Boston, MA.\n\nResponsibilities include but are not limited to:\n• Build, automate, and manage near-real-time scalable data ingestion pipelines for master data management, deep-learning, and predictive analytics.\n• Build and maintain cloud native big data environments on AWS, that are highly secure, scalable, flexible, and highly performant using appropriate SQL, NoSQL and NewSQL technologies.\n• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.\n• Provide technical input into build/buy/partner decisions for all components of the data infrastructure.\n• Partner closely with Data Scientists, BI developers, and Product Managers to design and implement data models, database schemas, data structures, and processing logic to support various data science, analytics, machine learning, and BI initiatives.\n• Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics, and model features, populate data models etc., using Spark, Python, SQL, and other technologies in the AWS.\n• Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage.\n• Lead by example, demonstrating best practices for code development and optimization, unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response to ensure data availability, data quality, and usability.\n• Define SLAs for data availability and correctness. Automate data availability and quality monitoring and respond to alerts when data delivery SLAs are not being met.\n• Communicate progress across organizations and levels from individual contributor to executive. Identify and clarify the critical few issues that need action and drive appropriate decisions and actions. Communicate results clearly.\n\nMinimum Qualifications:\n• A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience.\n• High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python. Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows.\n• Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena. Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions.\n• Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server. Knowledge of database design, optimization techniques, and advanced querying capabilities.\n• Experience in performance tuning and optimizing database operations.\n• Familiarity with data governance frameworks and data security best practices.\n• Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development.\n\nThe salary range for this role is $140,000 – $165,000/Annually with a target compensation of $156,750 based on experience and qualifications\n\nCompensation is commensurate with experience and includes a generous retirement package. Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP).\n\nAI Use\n\nAt Energy Solutions we believe in the importance of authentic interactions and equitable opportunities. We base our candidate selection on one's own skills, knowledge, and experience. To ensure the integrity and fairness of our interview process, the use of artificial intelligence (AI) tools (including Generative AI) or other means to generate or assist with responses during interviews is strictly prohibited. This practice supports our commitment to create a transparent and equitable space where skills, knowledge and experience skills can truly shine.\n\nEqual Opportunity Employer\n\nEnergy Solutions is an affirmative action-equal opportunity employer and prohibits discrimination and harassment of any type. We afford equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristics protected by law. Energy Solutions conforms to the spirit as well as to the letter of all applicable laws and regulations.\n\nOffice Locations and a Remote Workforce\n\nEnergy Solutions operates as a predominantly remote workforce with offices in six different locations. Employees who reside within 40 miles of an office (except New York) will be assigned to that location, though in-office attendance requirements may vary by team. At this time, we are not accepting applications from candidates residing in the following states: Delaware, Kentucky, Mississippi, Montana, Nebraska, North Dakota, and Wyoming.\n\nBackground Check Information\n\nInformation will be requested to perform the compulsory background check. A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment. Energy Solutions is an equal opportunity employer.\n\nReasonable Accommodations\n\nEnergy Solutions is committed to providing access and reasonable accommodation for individuals with disabilities. If you require accommodations in completing this application, interviewing, and/or completing any pre-employment testing, or otherwise participating in the employee selection process, please email accommodation@energy-solution.com.\n\nPrivacy Notice for Job Applicants",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": [
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DY4E5Ry_jMXZhJqZ0AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 140000,
    "job_max_salary": 165000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "In this role, the ideal candidate is a passionate and highly skilled professional with expertise in analytics tools and cloud technologies like AWS, Azure or similar technologies",
        "They should be proficient in programming languages such as SQL, NoSQL, and Python, capable of processing large data sets to deliver high-quality, customer-facing data solutions and insights",
        "A bachelor's degree in computer science or information technology plus 8 years minimum of relevant experience",
        "High proficiency in programming languages commonly used in ETL development, such as PLSQL, SQL, Python",
        "Ability to write efficient SQL queries, SQL store procedures, develop scripts for data transformations, and utilize programming frameworks and libraries to create/enhance ETL mappings and workflows",
        "Expertise in utilizing AWS services, including but not limited to Amazon s3, glue, data catalog, Amazon redshift, redshift spectrum and Amazon Athena",
        "Ability to leverage these services to build scalable, reliable, and performant data pipelines and analytics solutions",
        "Proficiency in working with relational databases such as Postgres, Oracle, MySQL, or SQL Server",
        "Knowledge of database design, optimization techniques, and advanced querying capabilities",
        "Experience in performance tuning and optimizing database operations",
        "Familiarity with data governance frameworks and data security best practices",
        "Passion for learning new technologies, staying up to date with industry trends, and exploring innovative approaches to ETL development",
        "Information will be requested to perform the compulsory background check",
        "A drug screen and authorization to work in the U.S. indefinitely are preconditions of employment"
      ],
      "Benefits": [
        "The salary range for this role is $140,000 – $165,000/Annually with a target compensation of $156,750 based on experience and qualifications",
        "Compensation is commensurate with experience and includes a generous retirement package",
        "Energy Solutions provides an excellent benefits package including medical, dental and vision insurance, other pre-tax contribution plans and an Employee Stock Ownership Plan (ESOP)"
      ],
      "Responsibilities": [
        "Build, automate, and manage near-real-time scalable data ingestion pipelines for master data management, deep-learning, and predictive analytics",
        "Build and maintain cloud native big data environments on AWS, that are highly secure, scalable, flexible, and highly performant using appropriate SQL, NoSQL and NewSQL technologies",
        "Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage",
        "Provide technical input into build/buy/partner decisions for all components of the data infrastructure",
        "Partner closely with Data Scientists, BI developers, and Product Managers to design and implement data models, database schemas, data structures, and processing logic to support various data science, analytics, machine learning, and BI initiatives",
        "Design and develop ETL (extract-transform-load) processes to validate and transform data, calculate metrics, and model features, populate data models etc., using Spark, Python, SQL, and other technologies in the AWS",
        "Lead data governance and data profiling efforts to ensure data quality and proper metadata documentation for data lineage",
        "Lead by example, demonstrating best practices for code development and optimization, unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response to ensure data availability, data quality, and usability",
        "Define SLAs for data availability and correctness",
        "Automate data availability and quality monitoring and respond to alerts when data delivery SLAs are not being met",
        "Communicate progress across organizations and levels from individual contributor to executive",
        "Identify and clarify the critical few issues that need action and drive appropriate decisions and actions",
        "Communicate results clearly"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-glassdoor-com-job-listing-senior-data-engineer-hybrid-new-york-ny-us-energy-solutions-usa-jv_ic1132348_ko0-42_ke43-63-htm",
    "_source": "new_jobs"
  },
  {
    "job_id": "N_UbHgFrV8mC2FPMAAAAAA==",
    "job_title": "Data Center Engineer",
    "employer_name": "Advocate Aurora Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRmPrUobFkxZ6o2Sn3cafDGFnva_mdPRfZBzxfr&s=0",
    "employer_website": "https://www.advocatehealth.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Advocate-Aurora-Health/Job/Data-Center-Engineer/-in-Des-Plaines,IL?jid=3d297bce27dfbaca&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Advocate-Aurora-Health/Job/Data-Center-Engineer/-in-Des-Plaines,IL?jid=3d297bce27dfbaca&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "SaluteMyJob",
        "apply_link": "https://salutemyjob.com/jobs/data-center-engineer-des-plaines-illinois/2605090811-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "KGET Jobs",
        "apply_link": "https://jobs.kget.com/jobs/data-center-engineer-des-plaines-illinois/2605090811-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "FOX16 Jobs",
        "apply_link": "https://jobs.fox16.com/jobs/data-center-engineer-des-plaines-illinois/2605090811-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Department:\n\n12232 Advocate Aurora Health Corporate - Data Centers and Disaster Recovery\n\nStatus:\n\nFull time\n\nBenefits Eligible:\n\nYes\n\nHours Per Week:\n\n40\n\nSchedule Details/Additional Information:\n\nTeam member will be responsible for data center in IL & WI. Will be working hybrid, in IL & WI. 15% travel as needed after training.\n\nPay Range\n$32.45 - $48.70\n\nMajor Responsibilities:\n• Plan for adequate power, cooling, and physical data center layout for primary and secondary data centers to meet the technology infrastructure capacity requirements.\n• Coordinate vendor activities within the data centers for scheduled maintenance.\n• Coordinate data center infrastructure projects, in conjunction with Facilities teams and contractors, to ensure minimal service interruptions, safety and timeliness of completion.\n• Document and maintain data center standards across primary and secondary data centers locations.\n• Work closely with the security team to ensure data center physical security procedures are in place and compliant with security policies.\n• Generate trend analysis reporting of power, cooling, and physical data center layout to ensure projected capacity levels are met.\n\nLicensure, Registration, and/or Certification Required:\n• Certified Data Centre Professional issued by EXIN,The global independent certification institute for ICT Professionals.\n\nEducation Required:\n• Bachelor's Degree (or equivalent knowledge) in Computer Science or related field.\n\nExperience Required:\n• Typically requires 3 years of experience in a Data Center in a large Computer Operations environment.\n\nKnowledge, Skills & Abilities Required:\n• Knowledge of data center architecture, power and cooling distribution, mechanical systems, physical security, racks, servers, storage devices, network, and cabling.\n• Experience with Building Management Systems (BMS) and Data Center Infrastructure Management (DCIM).\n• Experience with data center capacity and trend analysis reporting.\n• Strong project management skills.\n• Operational understanding of database terms and functionality.\n• Operational experience with Networking terms and functions including LAN and WAN.\n• Experience with large scale disaster recovery planning and implementation.\n• Demonstrated ability to work with a limited amount of supervision and work with internal and external customers.\n• Demonstrated ability for analytical and logical thinking. Ability to organize, troubleshoot efficiently and effectively.\n• Detail oriented, creative and self-motivated.\n• Ability to work on multiple in-process tasks simultaneously, set appropriate priorities and accomplish assignments in a thorough and timely fashion.\n\nPhysical Requirements and Working Conditions:\n• Exposed to a normal office environment.\n• Must be able to sit up to 80% of each workday.\n• Requires travel as necessary.\n• Operates all equipment necessary to perform the job.\n• Must be able to perform fine hand manipulation when using a keyboard.\n\nThis job description indicates the general nature and level of work expected of the incumbent. It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities required of the incumbent. Incumbent may be required to perform other related duties.\n\nOur CommitmenttoYou:\n\nAdvocate Health offers a comprehensive suite of Total Rewards: benefits and well-being programs, competitive compensation, generous retirement offerings, programs that invest in your career development and so much more - so you can live fully at and away from work, including:\n\nCompensation\n• Base compensation listed within the listed pay range based on factors such as qualifications, skills, relevant experience, and/or training\n• Premium pay such as shift, on call, and more based on a teammate's job\n• Incentive pay for select positions\n• Opportunity for annual increases based on performance\n\nBenefits and more\n• Paid Time Off programs\n• Health and welfare benefits such as medical, dental, vision, life, andShort- and Long-Term Disability\n• Flexible Spending Accounts for eligible health care and dependent care expenses\n• Family benefits such as adoption assistance and paid parental leave\n• Defined contribution retirement plans with employer match and other financial wellness programs\n• Educational Assistance Program\n\nAbout Advocate Health\n\nAdvocate Health is the third-largest nonprofit, integrated health system in the United States, created from the combination of Advocate Aurora Health and Atrium Health. Providing care under the names Advocate Health Care in Illinois; Atrium Health in the Carolinas, Georgia and Alabama; and Aurora Health Care in Wisconsin, Advocate Health is a national leader in clinical innovation, health outcomes, consumer experience and value-based care. Headquartered in Charlotte, North Carolina, Advocate Health services nearly 6 million patients and is engaged in hundreds of clinical trials and research studies, with Wake Forest University School of Medicine serving as the academic core of the enterprise. It is nationally recognized for its expertise in cardiology, neurosciences, oncology, pediatrics and rehabilitation, as well as organ transplants, burn treatments and specialized musculoskeletal programs. Advocate Health employs 155,000 teammates across 69 hospitals and over 1,000 care locations, and offers one of the nation's largest graduate medical education programs with over 2,000 residents and fellows across more than 200 programs. Committed to providing equitable care for all, Advocate Health provides more than $6 billion in annual community benefits.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Des Plaines, IL",
    "job_city": "Des Plaines",
    "job_state": "Illinois",
    "job_country": "US",
    "job_latitude": 42.0416537,
    "job_longitude": -87.8876429,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DN_UbHgFrV8mC2FPMAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 32.45,
    "job_max_salary": 48.7,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "Certified Data Centre Professional issued by EXIN,The global independent certification institute for ICT Professionals",
        "Bachelor's Degree (or equivalent knowledge) in Computer Science or related field",
        "Typically requires 3 years of experience in a Data Center in a large Computer Operations environment",
        "Knowledge of data center architecture, power and cooling distribution, mechanical systems, physical security, racks, servers, storage devices, network, and cabling",
        "Experience with Building Management Systems (BMS) and Data Center Infrastructure Management (DCIM)",
        "Experience with data center capacity and trend analysis reporting",
        "Strong project management skills",
        "Operational understanding of database terms and functionality",
        "Operational experience with Networking terms and functions including LAN and WAN",
        "Experience with large scale disaster recovery planning and implementation",
        "Demonstrated ability to work with a limited amount of supervision and work with internal and external customers",
        "Demonstrated ability for analytical and logical thinking",
        "Ability to organize, troubleshoot efficiently and effectively",
        "Detail oriented, creative and self-motivated",
        "Ability to work on multiple in-process tasks simultaneously, set appropriate priorities and accomplish assignments in a thorough and timely fashion",
        "Exposed to a normal office environment"
      ],
      "Benefits": [
        "15% travel as needed after training",
        "Pay Range",
        "$32.45 - $48.70",
        "Advocate Health offers a comprehensive suite of Total Rewards: benefits and well-being programs, competitive compensation, generous retirement offerings, programs that invest in your career development and so much more - so you can live fully at and away from work, including:",
        "Base compensation listed within the listed pay range based on factors such as qualifications, skills, relevant experience, and/or training",
        "Premium pay such as shift, on call, and more based on a teammate's job",
        "Incentive pay for select positions",
        "Opportunity for annual increases based on performance",
        "Benefits and more",
        "Paid Time Off programs",
        "Health and welfare benefits such as medical, dental, vision, life, andShort- and Long-Term Disability",
        "Flexible Spending Accounts for eligible health care and dependent care expenses",
        "Family benefits such as adoption assistance and paid parental leave",
        "Defined contribution retirement plans with employer match and other financial wellness programs",
        "Educational Assistance Program",
        "Committed to providing equitable care for all, Advocate Health provides more than $6 billion in annual community benefits"
      ],
      "Responsibilities": [
        "Team member will be responsible for data center in IL & WI",
        "Will be working hybrid, in IL & WI",
        "Plan for adequate power, cooling, and physical data center layout for primary and secondary data centers to meet the technology infrastructure capacity requirements",
        "Coordinate vendor activities within the data centers for scheduled maintenance",
        "Coordinate data center infrastructure projects, in conjunction with Facilities teams and contractors, to ensure minimal service interruptions, safety and timeliness of completion",
        "Document and maintain data center standards across primary and secondary data centers locations",
        "Work closely with the security team to ensure data center physical security procedures are in place and compliant with security policies",
        "Generate trend analysis reporting of power, cooling, and physical data center layout to ensure projected capacity levels are met",
        "Must be able to sit up to 80% of each workday",
        "Requires travel as necessary",
        "Operates all equipment necessary to perform the job",
        "Must be able to perform fine hand manipulation when using a keyboard",
        "This job description indicates the general nature and level of work expected of the incumbent",
        "It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities required of the incumbent",
        "Incumbent may be required to perform other related duties"
      ]
    },
    "job_onet_soc": "15114200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-advocate-aurora-health-job-data-center-engineer-in-des-plaines-il",
    "_source": "new_jobs"
  },
  {
    "job_id": "4xeUU9iH2JeNXzFtAAAAAA==",
    "job_title": "Cloud Data Engineer 17",
    "employer_name": "equiliem",
    "employer_logo": null,
    "employer_website": "https://equiliem.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-ks-all-cities-cloud-data-engineer-17-equiliem-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-ks-all-cities-cloud-data-engineer-17-equiliem-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Cloud Data Engineer\nRemote (with potential travel to the Washington D.C. metro area area on special occasions)\nActive Public Trust\nPay : $58.00 - $60.00 per hour\n25-30748Job Summary\nThe Cloud Data Engineer designs, builds, and maintains enterprise-scale data pipeline solutions supporting a Microsoft Azure-based analytics and reporting platform. This role translates business requirements into robust data engineering solutions, supports legacy SQL Server ETL operations, and leads the transition to modern cloud-native architectures. The ideal candidate is delivery-focused, customer-oriented, and comfortable solving complex technical challenges in regulated environments.\n\nJob Responsibilities\n• Maintain and operate legacy ETL processes using Microsoft SSIS, SQL Server, SQL Agent, PowerShell, SSAS, .NET, and related technologies\n• Design, develop, and operate full lifecycle Azure cloud-native data pipelines\n• Collaborate with stakeholders and technical teams to gather data requirements and implement appropriate solutions\n• Design and implement data models and pipelines across relational, dimensional, data warehouse, data mart, lakehouse (medallion architecture), SQL, and NoSQL data stores\n• Develop high-performing pipelines using Azure Data Factory, Azure Synapse Pipelines, Apache Spark Notebooks, Python, SQL, and stored procedures\n• Migrate and modernize existing SSIS ETL processes to Azure Data Factory and Synapse\n• Prepare and transform data for analytics, visualization, reporting, and AI/ML use cases\n• Implement data migration, integrity, quality, metadata management, and security controls\n• Monitor, troubleshoot, and optimize data pipelines for availability and performance\n• Implement governance, CI/CD, deployment, and monitoring to automate platform operations\n• Support Agile DevOps practices, including Program Increment (PI) planning\n• Maintain strict versioning and configuration control to ensure data integrity\n• Continuously learn and adopt new tools and best practices\nJob Requirements\n• Public Trust clearance required\n• Bachelor's degree in Computer Science or related field with 8+ years of experience, or Master's degree with 6+ years of experience\n• 4+ years of experience developing and maintaining ETL solutions using Microsoft SSIS, SQL Server, SQL Agent, and PowerShell\n• 4+ years of experience with scripting/programming languages such as SQL, T-SQL, Python, and/or PySpark\n• Strong experience with Microsoft data and BI tools including SQL Server, stored procedures, SSIS, SSRS, and SSAS\n• Experience designing and building data engineering solutions using Azure services such as Azure Data Lake, Azure Synapse, and Azure Data Factory\nEducation\n• Bachelor's degree in Computer Science or related field required\nWork Experience\n• 6-8+ years of relevant data engineering or ETL experience depending on degree level\n• ZR",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Kansas",
    "job_city": null,
    "job_state": "Kansas",
    "job_country": "US",
    "job_latitude": 39.011902,
    "job_longitude": -98.4842465,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4xeUU9iH2JeNXzFtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate is delivery-focused, customer-oriented, and comfortable solving complex technical challenges in regulated environments",
        "Public Trust clearance required",
        "Bachelor's degree in Computer Science or related field with 8+ years of experience, or Master's degree with 6+ years of experience",
        "4+ years of experience developing and maintaining ETL solutions using Microsoft SSIS, SQL Server, SQL Agent, and PowerShell",
        "4+ years of experience with scripting/programming languages such as SQL, T-SQL, Python, and/or PySpark",
        "Strong experience with Microsoft data and BI tools including SQL Server, stored procedures, SSIS, SSRS, and SSAS",
        "Experience designing and building data engineering solutions using Azure services such as Azure Data Lake, Azure Synapse, and Azure Data Factory",
        "Bachelor's degree in Computer Science or related field required",
        "6-8+ years of relevant data engineering or ETL experience depending on degree level"
      ],
      "Benefits": [
        "Pay : $58.00 - $60.00 per hour"
      ],
      "Responsibilities": [
        "Remote (with potential travel to the Washington D.C. metro area area on special occasions)",
        "Active Public Trust",
        "The Cloud Data Engineer designs, builds, and maintains enterprise-scale data pipeline solutions supporting a Microsoft Azure-based analytics and reporting platform",
        "This role translates business requirements into robust data engineering solutions, supports legacy SQL Server ETL operations, and leads the transition to modern cloud-native architectures",
        "Maintain and operate legacy ETL processes using Microsoft SSIS, SQL Server, SQL Agent, PowerShell, SSAS, .NET, and related technologies",
        "Design, develop, and operate full lifecycle Azure cloud-native data pipelines",
        "Collaborate with stakeholders and technical teams to gather data requirements and implement appropriate solutions",
        "Design and implement data models and pipelines across relational, dimensional, data warehouse, data mart, lakehouse (medallion architecture), SQL, and NoSQL data stores",
        "Develop high-performing pipelines using Azure Data Factory, Azure Synapse Pipelines, Apache Spark Notebooks, Python, SQL, and stored procedures",
        "Migrate and modernize existing SSIS ETL processes to Azure Data Factory and Synapse",
        "Prepare and transform data for analytics, visualization, reporting, and AI/ML use cases",
        "Implement data migration, integrity, quality, metadata management, and security controls",
        "Monitor, troubleshoot, and optimize data pipelines for availability and performance",
        "Implement governance, CI/CD, deployment, and monitoring to automate platform operations",
        "Support Agile DevOps practices, including Program Increment (PI) planning",
        "Maintain strict versioning and configuration control to ensure data integrity",
        "Continuously learn and adopt new tools and best practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-jobilize-com-job-us-ks-all-cities-cloud-data-engineer-17-equiliem-hiring-now-job-immediately",
    "_source": "new_jobs"
  },
  {
    "job_id": "Zl_lLJqOx3OHfJdpAAAAAA==",
    "job_title": "Tech Lead/Data Engineer III",
    "employer_name": "Medica",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_FelXxkN3248jJhI07DlC_ZOvbFg5RbXHeQzt&s=0",
    "employer_website": "https://www.medica.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/medica/hopkins-mn/lead-data-engineer/788bbbb1a1cac318a6dc43a1b175d33c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5613772574?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-mn-hopkins-tech-lead-data-engineer-iii-medica-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Medica is seeking an experienced Tech Lead/Data Engineer III. This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives. This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance.\n\nKey Responsibilities:\n\nWe're a team that owns our work with accountability, makes data-driven decisions, embraces continuous learning, and celebrates collaboration — because success is a team sport. It's our mission to be there in the moments that matter most for our members and employees. Join us in creating a community of connected care, where coordinated, quality service is the norm and every member feels valued.\n\nDesign, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources. Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data. Write complex SQL queries to support analytics needs. Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark.\n\nKey Accountabilities:\n• Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics.\n• Ensuring data quality, integrity, and security across all stages of the data lifecycle.\n• Collaborating with stakeholders to define data requirements and translate them into technical specifications.\n• Monitoring and optimizing performance of data systems and ETL processes.\n• Supporting the deployment and maintenance of data infrastructure in cloud environments.\n• Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting.\n• Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows.\n\nIn addition to engineering responsibilities, the Data Generalist component of this role includes:\n• Performing exploratory data analysis and generating actionable insights.\n• Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel.\n• Collaborating with cross-functional teams to align data efforts with business goals.\n• Automating data workflows using scripting languages such as Python or R.\n• Supporting business intelligence initiatives and translating data into strategic recommendations.\n• Documenting data processes and contributing to data governance standards.\n• Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets.\n• Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation.\n\nQualifications:\n• Bachelor's degree or equivalent experience in related field\n• 5 years of related work experience beyond the degree\n• Proficient use of SQL\n\nSkills And Abilities\n• Microsoft Certified: (or equivalent cloud) Azure Data Engineer\n• Microsoft Certified: Azure Fundamentals\n• Snowflake SnowPro Core Certification\n• Snowflake Advanced Architect Certification\n• Healthcare industry experience is desired\n\nThis position is an Office role, which requires an employee to work onsite at our Minnetonka, MN office, on average, 3 days per week.\n\nThe full salary grade for this position is $100,300 - $172,000. While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465. Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary. Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees.\n\nThe compensation and benefits information is provided as of the date of this posting. Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law.\n\nEligibility to work in the US: Medica does not offer work visa sponsorship for this role. All candidates must be legally authorized to work in the United States at the time of application. Employment is contingent on verification of identity and eligibility to work in the United States.\n\nWe are an Equal Opportunity employer, where all qualified candidates receive consideration for employment indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Hopkins, MN",
    "job_city": "Hopkins",
    "job_state": "Minnesota",
    "job_country": "US",
    "job_latitude": 44.9244005,
    "job_longitude": -93.41143989999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DZl_lLJqOx3OHfJdpAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 100000,
    "job_max_salary": 172000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives",
        "This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance",
        "Bachelor's degree or equivalent experience in related field",
        "5 years of related work experience beyond the degree",
        "Proficient use of SQL",
        "Microsoft Certified: (or equivalent cloud) Azure Data Engineer",
        "Microsoft Certified: Azure Fundamentals",
        "Snowflake SnowPro Core Certification",
        "Snowflake Advanced Architect Certification",
        "All candidates must be legally authorized to work in the United States at the time of application"
      ],
      "Benefits": [
        "The full salary grade for this position is $100,300 - $172,000",
        "While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465",
        "Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary",
        "Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees",
        "The compensation and benefits information is provided as of the date of this posting",
        "Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law"
      ],
      "Responsibilities": [
        "Design, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources",
        "Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data",
        "Write complex SQL queries to support analytics needs",
        "Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark",
        "Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics",
        "Ensuring data quality, integrity, and security across all stages of the data lifecycle",
        "Collaborating with stakeholders to define data requirements and translate them into technical specifications",
        "Monitoring and optimizing performance of data systems and ETL processes",
        "Supporting the deployment and maintenance of data infrastructure in cloud environments",
        "Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting",
        "Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows",
        "In addition to engineering responsibilities, the Data Generalist component of this role includes:",
        "Performing exploratory data analysis and generating actionable insights",
        "Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel",
        "Collaborating with cross-functional teams to align data efforts with business goals",
        "Automating data workflows using scripting languages such as Python or R",
        "Supporting business intelligence initiatives and translating data into strategic recommendations",
        "Documenting data processes and contributing to data governance standards",
        "Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets",
        "Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-tech-lead-data-engineer-iii-at-medica-4368808744",
    "_source": "new_jobs"
  }
]