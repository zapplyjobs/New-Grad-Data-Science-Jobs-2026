[
  {
    "job_id": "T9iATWXFpDt6gkCWAAAAAA==",
    "job_title": "Data Analytics Engineer Level 3/4",
    "employer_name": "Northrop Grumman",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT0guZ8n1kKZ_nu4eaicJ4PROiwjA-BfwwxOY7w&s=0",
    "employer_website": "https://www.northropgrumman.com",
    "job_publisher": "Jobs At Northrop Grumman",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.northropgrumman.com/careers/job/1340069874797-data-analytics-engineer-level-3-4-united-states-california-sunnyvale?domain=ngc.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobs At Northrop Grumman",
        "apply_link": "https://jobs.northropgrumman.com/careers/job/1340069874797-data-analytics-engineer-level-3-4-united-states-california-sunnyvale?domain=ngc.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs | Northrop Grumman",
        "apply_link": "https://northropgrumman.jobs/sunnyvale-ca/data-analytics-engineer-level-34/A65F6AF4F8A94823B83D6AB088A30371/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9a28c7a6de84fae3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-analytics-engineer-level-34-northrop-grumman-JV_IC1147442_KO0,32_KE33,49.htm?jl=1010026818998&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JSfirm.com",
        "apply_link": "https://www.jsfirm.com/Engineering/Data+Analytics+Engineer+Level+3/4/Sunnyvale-California/jobID_1850672?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Northrop-Grumman/Job/Data-Analytics-Engineer-Level-5/-in-Sunnyvale,CA?jid=9675d9d66bab3f26&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Hire Heroes USA Job Board",
        "apply_link": "https://jobs.hireheroesusa.org/jobs/497361476-data-analytics-engineer-level-3-4-at-northrop-grumman?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/data-analytics-engineer-level-5-northropgrumman-sunnyvale-ca_85623010?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "RELOCATION ASSISTANCE: Relocation assistance may be available\n\nCLEARANCE TYPE: Secret\n\nTRAVEL: Yes, 10% of the Time\n\nDescription\n\nAt Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work — and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.\n\nNorthrop Grumman Mission Systems is a trusted provider of mission-enabling solutions for global security. Our Engineering and Sciences (E&S) organization pushes the boundaries of innovation, redefines engineering capabilities, and drives advances in various sciences. Our team is chartered with providing the skills, innovative technologies to develop, design, produce and sustain optimized product lines across the sector while providing a decisive advantage to the warfighter. Come be a part of our mission!\n\nNorthrop Grumman Mission Systems is seeking a Data Analytics Engineer Level 3 or 4 to support the development and ongoing support of world-class US Navy machinery (turbines, generators, motors, and other system components based out of Sunnyvale, CA. In this role, you will leverage data to drive decision-making within engineering, and production processes. Your work will involve analyzing complex data sets, developing insights to improve manufacturing efficiency and quality, and collaborating with cross-functional teams to enhance production and design capabilities. If you are passionate about using data to improve manufacturing processes and enhance engineering outcomes, join our team and contribute to critical national defense projects.\n\nWhat You’ll get to Do:\n• Use classical analysis techniques to establish an understanding of machinery acoustic performance and resolve issues.\n• Develop a variety of component and system level predictive / simulation models.\n• Perform modal, vibration, modal, direct frequency response, and sound transmission analyses to predict performance and resolve issues.\n• Gather data from tests, and analyze the results.\n• Correlate model predictions to test data to validate the approach / model.\n• Work closely with cross-function internal and customer teams.\n• Prepare technical reports and presentations summarizing findings and recommendations for internal and external stakeholders.\n\nBasic Qualifications for Data Analytics Engineer level 3:\n• Bachelor's Degree in STEM (Science, Technology, Engineering or Math) discipline with 5 years relevant work experience; or Master’s Degree in STEM (Science, Technology, Engineering or Math) with 3 years of relevant experience; or Ph.D in STEM (Science, Technology, Engineering or Math) with no relevant work experience.\n• Experience with simulation software tools: Hypermesh, Nastran, Abaqus, MATLAB (or similar)\n• Must be a US citizen.\n• Ability to obtain and maintain a Secret Clearance.\n\nBasic Qualifications for Data Analytics Engineer level 4:\n• Bachelor’s Degree in STEM (Science, Technology, Engineering or Math) discipline with 8 years relevant work experience; or Master’s Degree in STEM (Science, Technology, Engineering or Math) discipline with 6 years relevant work experience; or Ph.D in STEM (Science, Technology, Engineering or Math) with 3 years of relevant work experience.\n• Experience with simulation software tools: Hypermesh, Nastran, Abaqus, MATLAB (or similar)\n• Must be a US citizen.\n• Ability to obtain and maintain a Secret Clearance.\n\nPreferred Qualifications:\n• Experience in developing system-level finite element models.\n• Experience in performing a classical and finite element analysis solution types.\n• Experience collecting and analyzing large data sets.\n\nPrimary Level Salary Range: $114,000.00 - $171,000.00\n\nSecondary Level Salary Range: $142,200.00 - $213,400.00\n\nThe above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.\n\nDepending on the position, employees may be eligible for overtime, shift differential, and a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.\n\nThe application period for the job is estimated to be 20 days from the job posting date. However, this timeline may be shortened or extended depending on business needs and the availability of qualified candidates.\n\nNorthrop Grumman is an Equal Opportunity Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO and pay transparency statement, please visit http://www.northropgrumman.com/EEO. U.S. Citizenship is required for all positions with a government clearance and certain other restricted positions.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "Sunnyvale, CA",
    "job_city": "Sunnyvale",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.368829999999996,
    "job_longitude": -122.0363496,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DT9iATWXFpDt6gkCWAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's Degree in STEM (Science, Technology, Engineering or Math) discipline with 5 years relevant work experience; or Master’s Degree in STEM (Science, Technology, Engineering or Math) with 3 years of relevant experience; or Ph.D in STEM (Science, Technology, Engineering or Math) with no relevant work experience",
        "Experience with simulation software tools: Hypermesh, Nastran, Abaqus, MATLAB (or similar)",
        "Must be a US citizen",
        "Ability to obtain and maintain a Secret Clearance",
        "Bachelor’s Degree in STEM (Science, Technology, Engineering or Math) discipline with 8 years relevant work experience; or Master’s Degree in STEM (Science, Technology, Engineering or Math) discipline with 6 years relevant work experience; or Ph.D in STEM (Science, Technology, Engineering or Math) with 3 years of relevant work experience",
        "Experience with simulation software tools: Hypermesh, Nastran, Abaqus, MATLAB (or similar)",
        "Must be a US citizen",
        "Ability to obtain and maintain a Secret Clearance"
      ],
      "Benefits": [
        "Primary Level Salary Range: $114,000.00 - $171,000.00",
        "Secondary Level Salary Range: $142,200.00 - $213,400.00",
        "The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions",
        "Depending on the position, employees may be eligible for overtime, shift differential, and a discretionary bonus in addition to base pay",
        "Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results",
        "Employees in Vice President or Director positions may be eligible for Long Term Incentives",
        "In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business"
      ],
      "Responsibilities": [
        "TRAVEL: Yes, 10% of the Time",
        "In this role, you will leverage data to drive decision-making within engineering, and production processes",
        "Your work will involve analyzing complex data sets, developing insights to improve manufacturing efficiency and quality, and collaborating with cross-functional teams to enhance production and design capabilities",
        "If you are passionate about using data to improve manufacturing processes and enhance engineering outcomes, join our team and contribute to critical national defense projects",
        "Use classical analysis techniques to establish an understanding of machinery acoustic performance and resolve issues",
        "Develop a variety of component and system level predictive / simulation models",
        "Perform modal, vibration, modal, direct frequency response, and sound transmission analyses to predict performance and resolve issues",
        "Gather data from tests, and analyze the results",
        "Correlate model predictions to test data to validate the approach / model",
        "Work closely with cross-function internal and customer teams",
        "Prepare technical reports and presentations summarizing findings and recommendations for internal and external stakeholders"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "jobs-northropgrumman-com-careers-job-1340069874797-data-analytics-engineer-level-3-4-united-states-california-sunnyvale",
    "_source": "new_jobs"
  },
  {
    "job_id": "VtUm40K6nxadOu5rAAAAAA==",
    "job_title": "Analytic Data Engineer",
    "employer_name": "CVS Health",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQudUQ7bAJjtUW_FO_Jjs6PIFXc_7l7lfcD4fbb&s=0",
    "employer_website": "https://www.cvshealth.com",
    "job_publisher": "AISES Career Hub",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.aises.org/job/analytic-data-engineer/82482430/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "AISES Career Hub",
        "apply_link": "https://careers.aises.org/job/analytic-data-engineer/82482430/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/neurgoSDnK6c7VwuAq7-SEqW1cJsf6di4c7bACSYU50Ym-QZwxz4Ww?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/analytic-data-engineer?id=2449626286&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-mn-all-cities-data-engineer-24-insight-global-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "We're building a world of health around every individual - shaping a more connected, convenient and compassionate health experience. At CVS Health®, you'll be surrounded by passionate colleagues who care deeply, innovate with purpose, hold ourselves accountable and prioritize safety and quality in everything we do. Join us and be part of something bigger - helping to simplify health care one person, one family and one community at a time.\n\nPosition Summary\n\nWe're seeking a Data Engineer to design and implement data pipelines that power analytical capabilities. This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.\n\nYou will be part of a dedicated team creating datasets for analytic and data science workloads. You will work with other Data Engineers and Sr. Data Engineers on designing, implementing, and testing data pipelines. Your key responsibilities are:\n• Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform datasets from multiple sources.\n• Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries.\n• Data Quality & Governance: follow data quality standards, data governance frameworks, and security policies for data storage and access.\n• Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements.\n• Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models\n• Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes.\n\nRequired Qualifications\n• 2+ years of applicable work experience\n• Proficiency in Python, specifically with ETL pipelines\n• Strong proficiency in SQL and experience in developing complex queries\n• Experience deploying data pipelines in a cloud environment (any of Azure, AWS, GCP)\n• Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists and analysts.\n\nPreferred Qualifications\n• Experience working with healthcare data, especially Epic.\n• Experience using GCP's BigQuery\n• Knowledge of data governance best practices in a cloud environment.\n• Experience working with Machine Learning processes.\n\nEducation\nCollege degree or certification in related fields\n\nAnticipated Weekly Hours\n40\n\nTime Type\nFull time\n\nPay Range\n\nThe typical pay range for this role is:\n\n$64,890.00 - $173,040.00\n\nThis pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.\n\nOur people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\nGreat benefits for great people\n\nWe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:\n• Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan.\n• No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n• Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\n\nFor more information, visit https://jobs.cvshealth.com/us/en/benefits\n\nWe anticipate the application window for this opening will close on: 02/20/2026\n\nQualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Minnesota",
    "job_city": null,
    "job_state": "Minnesota",
    "job_country": "US",
    "job_latitude": 46.729552999999996,
    "job_longitude": -94.6858998,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DVtUm40K6nxadOu5rAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions",
        "2+ years of applicable work experience",
        "Proficiency in Python, specifically with ETL pipelines",
        "Strong proficiency in SQL and experience in developing complex queries",
        "Experience deploying data pipelines in a cloud environment (any of Azure, AWS, GCP)",
        "Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists and analysts",
        "College degree or certification in related fields"
      ],
      "Benefits": [
        "Pay Range",
        "$64,890.00 - $173,040.00",
        "This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls",
        "The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors",
        "This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above",
        "We take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be",
        "In addition to our competitive wages, our great benefits include:",
        "Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan",
        "No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching",
        "Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility"
      ],
      "Responsibilities": [
        "We're seeking a Data Engineer to design and implement data pipelines that power analytical capabilities",
        "You will be part of a dedicated team creating datasets for analytic and data science workloads",
        "You will work with other Data Engineers and Sr",
        "Data Engineers on designing, implementing, and testing data pipelines",
        "Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform datasets from multiple sources",
        "Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries",
        "Data Quality & Governance: follow data quality standards, data governance frameworks, and security policies for data storage and access",
        "Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements",
        "Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models",
        "Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-aises-org-job-analytic-data-engineer-82482430",
    "_source": "new_jobs"
  },
  {
    "job_id": "4ZAd4t_9Sqb36hMNAAAAAA==",
    "job_title": "Engineer, AI/ML & Analytics Platform Engineering",
    "employer_name": "Genmab",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQKwQYBXkEQeJlspkZdk8fOUOpEUzTn1RJiqf7Y&s=0",
    "employer_website": "https://www.genmab.com",
    "job_publisher": "Careers - Genmab",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.genmab.com/global/en/job/R13311/Engineer-AI-ML-Analytics-Platform-Engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers - Genmab",
        "apply_link": "https://careers.genmab.com/global/en/job/R13311/Engineer-AI-ML-Analytics-Platform-Engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BioNJ Career Center",
        "apply_link": "https://bionjtalentnetwork.org/job/engineer-aiml-analytics-platform-engineering/82129138/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9961945e59190411&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "The Muse",
        "apply_link": "https://www.themuse.com/jobs/genmab/engineer-aiml-analytics-platform-engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6945531363141d188454d663?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/bZo8jVen6M9bL44OWJE2njh9kCd-pBRLQEtJYMUzGkXC8EfC3wP2zg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/engineer-ai-ml-analytics-platform-engineering-at-genmab-4328058219?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Genmab/Job/Engineer,-AI-ML-&-Analytics-Platform-Engineering/-in-Princeton,NJ?jid=8cf307f90329adda&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "At Genmab, we are dedicated to building extra[not]ordinary® futures, together, by developing antibody products and groundbreaking, knock-your-socks-off KYSO antibody medicines® that change lives and the future of cancer treatment and serious diseases. We strive to create, champion and maintain a global workplace where individuals’ unique contributions are valued and drive innovative solutions to meet the needs of our patients, care partners, families and employees.\n\nOur people are compassionate, candid, and purposeful, and our business is innovative and rooted in science. We believe that being proudly authentic and determined to be our best is essential to fulfilling our purpose. Yes, our work is incredibly serious and impactful, but we have big ambitions, bring a ton of care to pursuing them, and have a lot of fun while doing so.\n\nDoes this inspire you and feel like a fit? Then we would love to have you join us!\n\nAt Genmab, AI & analytics technology is more than just a solution, it powers the R&D, Commercial, and functional business units that work tirelessly to save lives and bring valuable treatments to patients around the world. The new AI & Analytics Platform Engineering & Standards team, is building quickly to meet the business demand of immediate, real-time, in-house built technology innovation. Our technology experts tackle exciting challenges in collaborative teams but work in an environment where individual and career development is always valued. Our team members leverage their talents and passion, building new and innovative systems, creating programs founded in automation in agile frameworks, and driving existing and cutting-edge innovation.\n\nWe are seeking an experienced AI/ML & Analytics platform engineer who is passionate about data, digital, and AI. This role is pivotal in securing the efficient and compliant operations of our developer platform, ensuring successful value delivery to Genmab’s business.\n\nThis role is based out of our Princeton or Copenhagen office and requires for you to be on site 60% of the time.\n\nPosition Overview:\n\nYou are highly technical and a hands-on individual who will build out the core features and capabilities of our AI/ML & Analytics developer platform. You will solve technical & architectural challenges to deliver a scalable, secure, and feature robust platform. You will work closely with our cross functional spoke teams to understand current and evolving AI/ML & Analytics needs to align platform and feature build-out. You will champion self-service usage patterns for end users and accelerate our usage of IaC and GitOps to build and scale these solutions. You will drive continuous improvements to the platform to ease of use and efficiency for end-users.\n\nYou embody the idea that ‘good platform engineering is rarely ever seen only felt’. You have an automation-first mindset, are security-conscious, and keen on improving the in-house developer experience.\n\nResponsibilities\n• Contribute to the build out of the AI/ML & Analytics platform, services, and tools (across dev, test, and prod) that accelerate model training, inference, and deployment within our spoke teams\n• Build platform capabilities to support both batch and real-time workflows at scale with flexible deployment strategies to accommodate varying use cases (e.g. low-latency predictions, offline model inference)\n• Improve platform performance, reduce manual intervention, scale compute, and increase deployment efficiency.\n• Work with the foundational cloud teams to ensure platform operational effectiveness, reliability, security and efficiency.\n• Work with team members to provide technical guidance and implementations for monitoring systems (e.g. registry, alerting, etc.) and governance frameworks (e.g. regulatory compliance).\n• Collaborate with our spoke teams for AI/ML & Analytics system architecture design, deployment pipelines, and solution scaling.\n\nQualifications\n• Bachelors or Masters in a quantitative subject (e.g. Computer Science, Engineering, Data Science, Mathematics, Statistics, Operations Research) or a related field with 5+ years of experience.\n• Experience in building AI/ML & Analytics or related platforms for ML Researchers, ML Engineers, Data Scientists, and Data Analysts\n• Experience building scalable self-service systems or platforms using microservices and/or event-based services\n• Strong knowledge of commonly used AI/ML & Analytics programming languages such as Python, Spark, SQL or similar, with experience in machine learning frameworks like PyTorch or TensorFlow.\n• Experience with the AWS cloud-service ecosystem including AI/ML & Analytics related services (e.g. Sagemaker, etc.)\n• Experience implementing IaC (Terraform, OpenTofu, CDK, Pulumi, etc.) + CI/CD for deploying cloud-based platform infrastructure at scale\n• Knowledge of basic software development tools including VCS (GitHub, GitLab, etc.), CI/CD (GitHub/Lab Actions, Jenkins, etc.), JIRA\n• Knowledge of containerization (e.g. Docker, Podman, etc.) and orchestration tools (e.g. Kubernetes, Rancher, etc.)\n• Experience with large scale CPU, GPU and/or multi-GPU infrastructure (bonus for CUDA fundamentals)\n• Knowledge of fundamental ops capabilities such as registries, tracking, observability, and monitoring.\n• Experience analyzing and improving system performance and reducing costs.\n• Strong communication skills and ability to engage with stakeholders effectively.\n\nBonus Qualifications:\n• Prior experience working within the pharma/biotech domain\n• Proficiency in at least one or more strongly typed programming language such as C/C++, Java, Go, Rust, or similar with associated OO or functional design principals.\n• Experience with large-scale distributed systems (e.g. Ray, Dask, Spark, etc.) and high-performance computing environments (e.g. Slurm, etc.)\n• In-depth knowledge of data platforms (e.g. Databricks, Snowflake, or Lake Formation) and tools (e.g. dbt) and their underlying technologies (e.g. Delta, Iceberg, Hudi, Spark).\n• Prior work building and using real-time/streaming infrastructure (e.g., Kafka, Spark Streaming).\n• Experience with GitOps style tools for building and enabling developer platforms (e.g. ArgoCD, Crossplane, etc.)\n• Experience with multi-cloud platform development (e.g. some combination of AWS, GCP, Azure)\n• Knowledge of high-performance frameworks for inference and training/fine-tuning (e.g. onnxRT, tensorRT, Triton, etc.) or resource intensive GenAI.\n\nFor US based candidates, the proposed salary band for this position is as follows:\n\n$104,880.00---$157,320.00\n\nThe actual salary offer will carefully consider a wide range of factors, including your skills, qualifications, experience, and location. Also, certain positions are eligible for additional forms of compensation, such as discretionary bonuses and long-term incentives.\n\nWhen you join Genmab, you’re joining a culture that supports your physical, financial, social, and emotional wellness. Within the first year, regular full-time U.S. employees are eligible for:\n• 401(k) Plan: 100% match on the first 6% of contributions\n• Health Benefits: Two medical plan options (including HDHP with HSA), dental, and vision insurance\n• Voluntary Plans: Critical illness, accident, and hospital indemnity insurance\n• Time Off: Paid vacation, sick leave, holidays, and 12 weeks of discretionary paid parental leave\n• Support Resources: Access to child and adult backup care, family support programs, financial wellness tools, and emotional well-being support\n• Additional Perks: Commuter benefits, tuition reimbursement, and a Lifestyle Spending Account for wellness and personal expenses\n\nAbout You\n• You are genuinely passionate about our purpose\n• You bring precision and excellence to all that you do\n• You believe in our rooted-in-science approach to problem-solving\n• You are a generous collaborator who can work in teams with a broad spectrum of backgrounds\n• You take pride in enabling the best work of others on the team\n• You can grapple with the unknown and be innovative\n• You have experience working in a fast-growing, dynamic company (or a strong desire to)\n• You work hard and are not afraid to have a little fun while you do so!\n\nLocations\n\nGenmab maximizes the efficiency of an agile working environment, when possible, for the betterment of employee work-life balance. Our offices are crafted as open, community-based spaces that work to connect employees while being immersed in our powerful laboratories. Whether you’re in one of our office spaces or working remotely, we thrive on connecting with each other to innovate.\n\nAbout Genmab\n\nGenmab is an international biotechnology company with a core purpose to improve the lives of patients through innovative and differentiated antibody therapeutics. For 25 years, its hard-working, innovative and collaborative team has invented next-generation antibody technology platforms and harnessed translational, quantitative and data sciences, resulting in a proprietary pipeline including bispecific T-cell engagers, antibody-drug conjugates, next-generation immune checkpoint modulators and effector function-enhanced antibodies. By 2030, Genmab’s vision is to transform the lives of people with cancer and other serious diseases with Knock-Your-Socks-Off (KYSO®) antibody medicines.\n\nEstablished in 1999, Genmab is headquartered in Copenhagen, Denmark with international presence across North America, Europe and Asia Pacific. For more information, please visit Genmab.com and follow us on LinkedIn and X.\n\nGenmab is committed to protecting your personal data and privacy. Please see our privacy policy for handling your data in connection with your application on our website Job Applicant Privacy Notice (genmab.com).\n\nPlease note that if you are applying for a position in the Netherlands, Genmab’s policy for all permanently budgeted hires in NL is initially to offer a fixed-term employment contract for a year, if the employee performs well and if the business conditions do not change, renewal for an indefinite term may be considered after the fixed-term employment contract.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Princeton, NJ",
    "job_city": "Princeton",
    "job_state": "New Jersey",
    "job_country": "US",
    "job_latitude": 40.3503931,
    "job_longitude": -74.6571416,
    "job_benefits": [
      "dental_coverage",
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4ZAd4t_9Sqb36hMNAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "careers-genmab-com-global-en-job-r13311-engineer-ai-ml-analytics-platform-engineering",
    "_source": "new_jobs"
  },
  {
    "job_id": "C-jX8U0HtfP_C52zAAAAAA==",
    "job_title": "Senior Data & Analytics Platform Engineer (ETL & OLAP Focus)",
    "employer_name": "Netbuilder",
    "employer_logo": null,
    "employer_website": "https://www.netbuilder.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/analytics-engineer?id=2459734245&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/analytics-engineer?id=2459734245&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Candidates ideally reside near Plano, TX or Long Island, NY , though remote applicants within the U.S. are welcome if they can work Central Time Zone hours . This is a full-time position anticipated to run for at least 12 months , with potential to continue beyond. Role Summary\n\nThe Senior Data & Analytics Platform Engineer is responsible for the design, implementation, and operation of the enterprise data ingestion and analytical platforms. This role provides hands-on technical leadership across ETL and OLAP capabilities, ensuring reliable data pipelines, performant analytical models, and high-quality datasets that power reporting, dashboards, and downstream AI and observability use cases.\n\nThe role combines solution design, implementation, and team leadership, guiding junior ETL and OLAP engineers while remaining deeply involved in delivery.\nKey Responsibilities Design end-to-end data ingestion and analytics architectures covering batch, streaming, and near-real-time pipelines Define OLAP data models, semantic layers, aggregation strategies, and query optimization patterns Establish integration patterns between ETL pipelines and analytical/observability platforms Ensure scalability, performance, cost efficiency, and data quality across the analytics stack Strong Business Analysis skills (gathering project requirements) Project planning abilities Hands-on Engineering Build and review complex ETL pipelines and analytical data models Lead development of reusable transformation logic, data quality frameworks, and monitoring Troubleshoot performance bottlenecks, data inconsistencies, and pipeline failures Contribute production-grade code, configurations, and documentation Provide technical guidance and mentorship to ETL and OLAP junior engineers Conduct code reviews, design reviews, and technical walkthroughs Break down designs into executable tasks and guide implementation sequencing Raise engineering standards and consistency across the data team Delivery & Stakeholder Ownership Own delivery outcomes for data ingestion and analytics workstreams Collaborate with Observability and AI Ops counterparts on shared data needs Communicate progress, risks, and trade-offs to customer stakeholders Support operational handover, runbooks, and ongoing platform evolution Required Qualifications 5+ years of experience in data engineering or analytics platforms Strong hands-on experience with ETL/ELT pipelines and OLAP systems Deep expertise in SQL, data modeling, and analytical query optimization Experience operating production data platforms with SLAs Strong business analysis and project planning abilities, including translating business requirements into technical solutions and executable delivery plans Preferred Qualifications Experience with time-series or operational data Exposure to observability-driven analytics or AI feature pipelines About NETbuilder\n\nFounded 26 years ago, NETbuilder is a trusted partner to leading global software vendors, delivering enterprise-scale technology and cybersecurity solutions. We are known for our reliability, expertise, and strategic insight, helping clients strengthen resilience, improve visibility, and drive operational excellence.\n\nAs we continue to grow, 2026 marks the start of an ambitious new phase focused on scaling our capabilities, deepening strategic partnerships, and delivering high-impact enterprise solutions. Our collaborative culture empowers experienced professionals to shape strategy and make a lasting impact.\n\nNETbuilder is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Fairview, TX",
    "job_city": "Fairview",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 33.1578952,
    "job_longitude": -96.6316593,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DC-jX8U0HtfP_C52zAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-analytics-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "5ulgfnwks5AIA6RnAAAAAA==",
    "job_title": "Data Mining and Analytics (Databricks) Engineer (Future Need) - Security Clearance Required",
    "employer_name": "IIIIIIUS",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Careers At ICF",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.icf.com/us/en/job/R2502772/Data-Mining-and-Analytics-Databricks-Engineer-Future-Need-Security-Clearance-Required?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Careers At ICF",
        "apply_link": "https://careers.icf.com/us/en/job/R2502772/Data-Mining-and-Analytics-Databricks-Engineer-Future-Need-Security-Clearance-Required?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=549f629a5c28296a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Tbe.taleo.net",
        "apply_link": "https://phg.tbe.taleo.net/phg03/ats/careers/requisition.jsp?org=CYDE&cws=38&rid=2715&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LMI - ICIMS",
        "apply_link": "https://careers-lmi.icims.com/jobs/13566/senior-acquisition-consultant---aviation---clearance-required/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/LMI-Consulting/Job/Senior-Acquisition-Consultant-Aviation-Clearance-Required/-in-Arlington,VA?jid=9c3fc3eed428272c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/senior-acquisition-consultant-aviation-arlington-logistics-management-institute-8ed0929060b02cb46bf1ed4b81299e81?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-acquisition-consultant-aviation-clearance-required-arlington-va--2f8f079a-6e41-4354-a180-e06c005a8d35?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/senior-acquisition-consultant-aviation-clearance-required-logisticsmanagementinstitute-arlington-va_85417462?ir=1&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description\n\nICF International seeks an experienced Data Mining and Analytics (Databricks) Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate.  Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale.\n\nAs the Data Mining and Analytics (Databricks) Engineer, your exceptional skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.\n\nThe ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!\n\nThis role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the Washington, DC metro area.\n\nWhat You Will Be Doing:\n• Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions\n• Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners\n• Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training\n• Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment\n• Develop custom data modeling procedures to assist with data mining, modeling, and production\n• Assess the effectiveness and accuracy of new data sources and data gathering techniques\n• Develop processes and tools to monitor and analyze model performance and data accuracy\n• Interpret and communicate results to non-technical customers\n\nWhat You Must Have:\n• Bachelor’s or Master’s degree in Computer Science, Mathematics, Engineering, or related field\n• Position requires a minimum of 5 years experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools\n• Demonstrated hands-on working experience and advanced knowledge of cyber threats, tools, techniques, and processes is required\n• Active security clearance required as part of client contract requirements\n• US Citizenship required as part of client contract requirements\n\n​\n\nPreferred Skills/Experience:\n• Practical experience with the Databricks Intelligence Platform\n• Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details\n• Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise\n• Experience with computational notebook technologies such as Databricks, Zeppelin, or Jupyter\n• Experience with the application of visual analytics to computational analytic results\n• Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)\n• Experience with database querying like SQL\n• Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products\n• Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired\n• CompTIA Security+ or higher cybersecurity certification preferred\n\n#ICFNS\n\nWorking at ICF\nICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.\n\nWe can only solve the world's toughest challenges by building a workplace that allows everyone to thrive. We are an equal opportunity employer. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO policy.\n\nWe will consider for employment qualified applicants with arrest and conviction records.\n\nReasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation, please email Candidateaccommodation@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n\nRead more about workplace discrimination rights or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act.\n\nCandidate AI Usage Policy\n\nAt ICF, we are committed to ensuring a fair interview process for all candidates based on their own skills and knowledge. As part of this commitment, the use of artificial intelligence (AI) tools to generate or assist with responses during interviews (whether in-person or virtual) is not permitted. This policy is in place to maintain the integrity and authenticity of the interview process. \n\nHowever, we understand that some candidates may require accommodation that involves the use of AI. If such an accommodation is needed, candidates are instructed to contact us in advance at candidateaccommodation@icf.com. We are dedicated to providing the necessary support to ensure that all candidates have an equal opportunity to succeed.  \n\nPay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position.\n\nThe pay range for this position based on full-time employment is:\n$98,614.00 - $167,644.00\n\nVirginia Remote Office (VA99)",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771286400,
    "job_posted_at_datetime_utc": "2026-02-17T00:00:00.000Z",
    "job_location": "Arlington, VA",
    "job_city": "Arlington",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.8816208,
    "job_longitude": -77.09098089999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D5ulgfnwks5AIA6RnAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor’s or Master’s degree in Computer Science, Mathematics, Engineering, or related field",
        "Position requires a minimum of 5 years experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools",
        "Demonstrated hands-on working experience and advanced knowledge of cyber threats, tools, techniques, and processes is required",
        "Active security clearance required as part of client contract requirements",
        "US Citizenship required as part of client contract requirements"
      ],
      "Benefits": [
        "Read more about workplace discrimination rights or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act",
        "Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position",
        "$98,614.00 - $167,644.00"
      ],
      "Responsibilities": [
        "The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program",
        "Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate.  Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale",
        "As the Data Mining and Analytics (Databricks) Engineer, your exceptional skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project",
        "You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration",
        "The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems",
        "This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the Washington, DC metro area",
        "Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions",
        "Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners",
        "Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training",
        "Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment",
        "Develop custom data modeling procedures to assist with data mining, modeling, and production",
        "Assess the effectiveness and accuracy of new data sources and data gathering techniques",
        "Develop processes and tools to monitor and analyze model performance and data accuracy",
        "Interpret and communicate results to non-technical customers"
      ]
    },
    "job_onet_soc": "15113300",
    "job_onet_job_zone": "4",
    "id": "careers-icf-com-us-en-job-r2502772-data-mining-and-analytics-databricks-engineer-future-need-security-clearance-required",
    "_source": "new_jobs"
  },
  {
    "job_id": "zfD8D0M8uxVfRNpfAAAAAA==",
    "job_title": "Analytics Engineer",
    "employer_name": "Hearst",
    "employer_logo": null,
    "employer_website": "https://www.hearst.com",
    "job_publisher": "Sign In",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://eevd.fa.us6.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions/preview/2026058?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Sign In",
        "apply_link": "https://eevd.fa.us6.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions/preview/2026058?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Hearst/Job/Analytics-Engineer/-in-Charlotte,NC?jid=c7558868074bbdef&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/analytics-engineer-charlotte-hearst-communications-b17bfbb0a880de508a0d7afa6d4c3e88?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/analytics-engineer-at-hearst-television-charlotte-4364464181?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/f8d07f12457608b4310958373c78053f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Broadcast Career Link",
        "apply_link": "https://jobs.broadcastcareerlink.com/job/analytics-engineer/82498609/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer",
        "apply_link": "https://us.experteer.com/career/view-jobs/analytics-engineer-charlotte-nc-usa-56181061?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Hearst Television (HTV) is hiring an Analytics Engineer. The ideal candidate will act as the vital bridge between raw data and executive insights. You will take \"Bronze\" data and transform it into high-value Silver (Cleansed) and Gold (Aggregated) layers. Your primary goal is to codify complex media sales logic such as Revenue Pacing, Share of Wallet, and Yield into high-performance Semantic Models that power our Power BI dashboards and AI Data Agents. This role ensures our data is not just accessible, but actionable and intelligent.\n\nWhat You’ll Do\n\n· Medallion Transformation: Use Spark and SQL to architect the Silver and Gold layers of our Lakehouse, ensuring all data is rigorously tested, clean, and documented.\n\n· Semantic Modeling: Design and maintain the Microsoft Fabric Semantic Model (Direct Lake mode) to provide a \"Single Source of Truth\" across the entire organization.\n\n· Business Logic Codification: Partner closely with Sales and Finance stakeholders to translate \"What\" and \"Why\" business questions into precise technical data definitions.\n\n· AI Data Readiness: Structure data specifically for LLM consumption, enabling our AI Data Agents to accurately navigate complex hierarchies of stations, advertisers, and categories.\n\n· Data Governance & Security: Implement Row-Level Security (RLS) and maintain data lineage to ensure compliance with Hearst’s unified data strategy.\n\n· Technical Leadership: Lead the design of project-specific data models, prepare design specifications, and conduct feature estimations in collaboration with cross-functional teams.\n\n· Optimization & RCA: Perform root cause analysis on anomalies and implement solutions to minimize points of failure and optimize data pipeline performance.\n\nRequirements\n\n· Experience: 7+ years in business analytics, data modeling, or data engineering (or a Master’s degree with 5+ years of experience).\n\n· Modeling Expertise: 5+ years of deep experience with relational modeling concepts and Microsoft tabular semantic models.\n\n· Technical Proficiency: Expert-level SQL skills and 5+ years of experience with Power BI, DAX, and Spark (Spark SQL/PySpark/Scala).\n\n· Platform Knowledge: Strong hands-on experience with Microsoft Fabric tools and the Delta Lake format.\n\n· Logic & Design: Demonstrated experience translating complex business requirements into technical requirements using Star Schema/Dimensional modeling.\n\n· DevOps: Familiarity with Git, CI/CD, and modern development workflows.\n\n· Communication: Proven ability to collaborate with stakeholders and escalate data requirements or constraints effectively.\n\nPreferred Qualifications\n\n· Familiarity with governance, security best practices, and performance tuning in a cloud environment.\n\n· Experience in the media or broadcasting industry is a plus.\n\n· Knowledge of serverless architectures and containerized deployment.\n\nValues in Action\n\nAt Hearst Television we tell stories every day. Stories about people of all backgrounds, perspectives, and identities. That’s why, behind the scenes, we believe in being an organization that fosters collaboration and open communication, ensuring that the content we create is authentic, accurate, and connected to the communities we serve.\n\nBenefits\n\nHearst's benefit programs are modern, flexible and designed to focus on you. As a Hearst employee, you and your spouse or partner or dependents would have access to the following benefits:\n\n· Medical | Dental | Vision\n\n· 401(k) matching\n\n· Emotional Wellness Support\n\n· Paid Time Off & Parental Leave\n\n· LGBTQ+ Health Services",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Charlotte, NC",
    "job_city": "Charlotte",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.2270768,
    "job_longitude": -80.84089329999999,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzfD8D0M8uxVfRNpfAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Experience: 7+ years in business analytics, data modeling, or data engineering (or a Master’s degree with 5+ years of experience)",
        "Modeling Expertise: 5+ years of deep experience with relational modeling concepts and Microsoft tabular semantic models",
        "Technical Proficiency: Expert-level SQL skills and 5+ years of experience with Power BI, DAX, and Spark (Spark SQL/PySpark/Scala)",
        "Platform Knowledge: Strong hands-on experience with Microsoft Fabric tools and the Delta Lake format",
        "Logic & Design: Demonstrated experience translating complex business requirements into technical requirements using Star Schema/Dimensional modeling",
        "DevOps: Familiarity with Git, CI/CD, and modern development workflows",
        "Communication: Proven ability to collaborate with stakeholders and escalate data requirements or constraints effectively"
      ],
      "Benefits": [
        "Hearst's benefit programs are modern, flexible and designed to focus on you",
        "Medical | Dental | Vision",
        "401(k) matching",
        "Emotional Wellness Support",
        "Paid Time Off & Parental Leave",
        "LGBTQ+ Health Services"
      ],
      "Responsibilities": [
        "The ideal candidate will act as the vital bridge between raw data and executive insights",
        "You will take \"Bronze\" data and transform it into high-value Silver (Cleansed) and Gold (Aggregated) layers",
        "Your primary goal is to codify complex media sales logic such as Revenue Pacing, Share of Wallet, and Yield into high-performance Semantic Models that power our Power BI dashboards and AI Data Agents",
        "Medallion Transformation: Use Spark and SQL to architect the Silver and Gold layers of our Lakehouse, ensuring all data is rigorously tested, clean, and documented",
        "Semantic Modeling: Design and maintain the Microsoft Fabric Semantic Model (Direct Lake mode) to provide a \"Single Source of Truth\" across the entire organization",
        "Business Logic Codification: Partner closely with Sales and Finance stakeholders to translate \"What\" and \"Why\" business questions into precise technical data definitions",
        "AI Data Readiness: Structure data specifically for LLM consumption, enabling our AI Data Agents to accurately navigate complex hierarchies of stations, advertisers, and categories",
        "Data Governance & Security: Implement Row-Level Security (RLS) and maintain data lineage to ensure compliance with Hearst’s unified data strategy",
        "Technical Leadership: Lead the design of project-specific data models, prepare design specifications, and conduct feature estimations in collaboration with cross-functional teams",
        "Optimization & RCA: Perform root cause analysis on anomalies and implement solutions to minimize points of failure and optimize data pipeline performance"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "eevd-fa-us6-oraclecloud-com-hcmui-candidateexperience-en-sites-cx_1-requisitions-preview-2026058",
    "_source": "new_jobs"
  },
  {
    "job_id": "XAY1ZGCPVXZ2djEhAAAAAA==",
    "job_title": "Senior Analytics Engineer, Revenue Operations",
    "employer_name": "DNSFilter",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSNX15ECaIW708N2bP7xkfHys6oDsq3bdDMp-jE&s=0",
    "employer_website": "https://www.dnsfilter.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/DNSFilter/Job/Senior-Analytics-Engineer,-Revenue-Operations/-in-Tampa,FL?jid=2bd1004a3b8d5fcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/DNSFilter/Job/Senior-Analytics-Engineer,-Revenue-Operations/-in-Tampa,FL?jid=2bd1004a3b8d5fcc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Energy Jobline",
        "apply_link": "https://www.energyjobline.com/job/senior-analytics-engineer-revenue-operations-tampa-29339333?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-analytics-engineer-revenue-operations-tampa-fl--5b88feb5-7b6b-4dae-88b1-d4fe5bc04a4b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/b349576177e17bb28ec69bd9b5a00a28?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6983dc5b01214b4cdacbfbfc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/senior-analytics-engineer-revenue-operations--tampa--e3fcc5e3d10e3b4e113602d5a797a69c3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/tampa/florida/info_technology/4862446604/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LifeworQ",
        "apply_link": "https://lifeworq.com/job/500e9772-2bdd-4470-9cac-e2b4e2716170?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "DNSFilter's mission is to protect our customers and partners with products they love to use! We are revolutionizing network security by providing fast, accurate, and reliable threat protection and content filtering. We're a rapidly growing company dedicated to creating a safer internet for businesses and organizations worldwide. Leveraging AI-driven threat intelligence, DNSFilter empowers our customers to proactively block threats before they impact their networks. We foster a collaborative, innovative, and results-oriented culture where every team member contributes to our mission of making the internet safer.\n\nAs we continue our product-fueled growth by adding new features and broadening our solution to meet the needs of the global market, it's clear there's a missing piece. That's where you come in!\n\nWe're seeking a Senior Analytics Engineer, Revenue Operations, to own and scale the RevOps data domain. This role sits on the Revenue Operations team, will report to the Senior Director, RevOps, and partners closely with Sales, Marketing, Customer Support, Customer Success, Finance, Product, and the Data Platform team to deliver trusted, analytics-ready data that powers GTM systems, reporting, and decision-making.\n\nThis is a senior individual contributor role with deep ownership. You'll treat the RevOps data domain as a product, not a collection of dashboards—balancing rigor, scalability, and stakeholder alignment in a fast-moving environment.\n\nEligible candidates have and can work successfully in a small to mid-sized fast-paced, hyper-growth, SaaS start-up or scale-up. This is a full-time role open to candidates in the United States and Canada.\n\nWe recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If you feel like this job is for you, please apply. We believe diversity of experience and skills, including transferable skills, combined with passion, is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions!\n\nAt DNSFilter, You Will:\n\nOwn the RevOps Data Domain\n• Architect and own the RevOps / BizOps data zone within our Data Mesh, treating it as a product that is high-quality, discoverable, well-documented, and the authoritative source of truth for revenue data\n• Establish clear ownership, documentation, and governance for core RevOps datasets and metrics\n\nBuild & Scale Analytics Foundations\n• Own the end-to-end design, development, and maintenance of analytics solutions that power downstream GTM systems, reporting, and decision-making\n• Partner with the Data Platform (Data Engineering) team to transform data landed from internal databases, third-party APIs, and Airflow-managed pipelines into analytics-ready models\n• Develop, optimize, and maintain ELT pipelines using dbt Cloud, transforming PostgreSQL and other source data into analytics-ready datasets in Amazon Athena\n• Ensure reliable data availability, performance, and quality within our Athena-based analytics data zone\n\nOwn Data State, History, and Performance\n• Lead our approach to capturing data evolution and history, making heavy use of dbt snapshots and incremental models to support point-in-time and trend analysis\n• Write performant, cost-aware SQL optimized for distributed query engines (Amazon Athena), including queries operating over tens of billions of rows\n• Design models with scalability and maintainability in mind, prioritizing long-term clarity over clever but fragile solutions\n\nDefine, Govern & Operationalize Metrics\n• Design, implement, and maintain a semantic layer (e.g., dbt semantic models/metrics) that serves as the authoritative source for business definitions and revenue metrics\n• Introduce and steward a centralized data dictionary and metric catalog, ensuring consistent definitions across dashboards, reports, and GTM systems\n• Partner closely with RevOps, Finance, Product, and GTM stakeholders to define, govern, and evolve shared metrics—aligning on \"what is what\" as the business grows\n• Act as a trusted arbiter for metric definitions, managing versioning, documentation, and change communication\n\nModel Revenue Data for Downstream GTM Systems\n• Design data models that are consumable by the GTM Systems team for use in downstream operational tools such as Salesforce, Hubspot, and Zendesk\n• Expose clean, well-documented datasets and metrics that can be reliably reused across reporting, automation, and operational workflows\n• Partner with the GTM Systems team to ensure data models meet operational needs, performance requirements, and system constraints\n• Ensure consistency between analytical models and operational system logic, minimizing metric drift between analytics and GTM tooling\n\nOwn BI Reporting & Visualization\n• Own the design, development, and ongoing maintenance of RevOps dashboards, reports, and visualizations in our BI tool\n• Ensure all reporting is powered by governed, well-modeled, and tested datasets—avoiding one-off queries and metric drift\n• Partner with GTM, RevOps, and executive stakeholders to translate business needs into scalable, self-service dashboards\n• Continuously audit and improve existing dashboards for accuracy, performance, usability, and clarity\n• Define standards and best practices for dashboard design, metric presentation, and reporting governance\n\nDrive Accuracy, Simplicity & Trust\n• Get \"down in the weeds\" to validate data end-to-end, using SQL, spreadsheets, and source-system analysis to trace and resolve discrepancies\n• Uphold a \"simplicity at scale\" philosophy—choosing readable, maintainable SQL over over-engineered abstractions that accrue technical debt\n• Maintain logic at the appropriate layer: when metrics require new or corrected product data, partner with upstream owners rather than introducing brittle downstream workarounds\n• Review, audit, and refactor existing dbt models, ELT jobs, and dashboards to improve accuracy, performance, and maintainability\n\nEnable the Business & Look Forward\n• Design, build, and maintain dashboards and reports powered by well-modeled, tested data—avoiding one-off queries and metric drift\n• Support the GTM Systems team by ensuring analytics and reporting dependencies are reliable and well-documented\n• Leverage AI-assisted development tools to accelerate SQL development, dbt modeling, testing, documentation, and refactoring\n• Explore and pilot AI-enabled approaches to improve data quality, observability, and operational efficiency with sound judgment around accuracy and governance\n• Establish and promote best practices for data modeling, testing, documentation, and dashboard governance\n\nTo Qualify for this Role, You Have:\n• 5+ years of experience in analytics engineering or data engineering, specifically supporting GTM, RevOps, or BizOps functions\n• Expert dbt knowledge, including advanced use of incremental strategies, snapshotting, and modular project structure—you know when to use a macro and when not to\n• Deep proficiency in SQL with experience optimizing queries for modern distributed warehouses (e.g., Amazon Athena, BigQuery, Snowflake), including partitioning and cost optimization\n• Hands-on experience designing and maintaining analytics-ready data models and ELT pipelines from application and operational data sources\n• Experience implementing or working with semantic layers or governed metrics frameworks (e.g., dbt semantic layer or equivalent)\n• A meticulous, almost obsessive approach to data accuracy—you aren't satisfied until the numbers tie out 1:1 and you can prove it\n• Demonstrated ability to reconcile complex datasets across systems and identify root causes of discrepancies\n• A strong \"do it right\" mindset, including the ability to push back on unscalable requests and prioritize durable solutions over short-term fixes\n• Understanding of data mesh principles, domain ownership, and the discipline required to maintain a standalone analytics data zone\n• Strong communication skills and comfort level in influencing both technical and non-technical stakeholders\n\nBonus points for:\n• Direct experience working on a Revenue Operations team\n• Experience supporting Sales, Marketing, Customer Support, and Customer Success analytics\n• Experience introducing or maturing a centralized data dictionary and driving organizational adoption of governed metrics\n• Exposure to data mesh or domain-oriented data ownership models in production environments\n• Experience applying AI-assisted development tools to analytics engineering workflows (SQL, dbt, testing, documentation, refactoring)\n• Familiarity with revenue lifecycle metrics (pipeline, conversion rates, ARR/MRR, churn, expansion, forecasting)\n\nWe Offer:\n• Pathway to promotion to additional organizational positions and responsibilities based upon results and performance, not just time in the chair. You help us grow, and we will help you grow.\n• Passionate and intelligent colleagues who work hard and have a good time doing it\n• Paid company-wide week off at the end of each year\n• Flexible Vacation Policy\n• Awesome company swag\n• Full medical, dental, and vision benefits for US, UK, and Canada-based employees\n• Full short-term disability and life benefits; available long-term disability\n• Retirement savings account options with vested company matching for qualifying employees\n• In-person annual gatherings. Last time we all spent a week on a beach in Cancun!\n\nDNSFilter is a pay-for-performance organization, which means there is an opportunity to advance your compensation based on performance over time. The hiring base pay is dependent on several factors, including level, function, training, transferable skills, work experience, business needs, and geographic location. As a hybrid company, our compensation reflects the cost of labor across several U.S. and global geographic markets. We pay differently based on those defined markets. Our Talent Team can share more about the specific salary range for the job location during the hiring process.\n\nDNSFilter participates in the E-Verify program.\n\nAt DNSFilter, we utilize sophisticated software and tools to identify and eliminate Deepfake candidates. This approach helps us maintain the integrity of our hiring process, ensuring that we select the most qualified and genuine individuals to join our team.\nU.S. hiring salary range\n$130,000—$170,000 USD",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Tampa, FL",
    "job_city": "Tampa",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 27.951689599999998,
    "job_longitude": -82.45875269999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXAY1ZGCPVXZ2djEhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Eligible candidates have and can work successfully in a small to mid-sized fast-paced, hyper-growth, SaaS start-up or scale-up",
        "Drive Accuracy, Simplicity & Trust",
        "Get \"down in the weeds\" to validate data end-to-end, using SQL, spreadsheets, and source-system analysis to trace and resolve discrepancies",
        "5+ years of experience in analytics engineering or data engineering, specifically supporting GTM, RevOps, or BizOps functions",
        "Expert dbt knowledge, including advanced use of incremental strategies, snapshotting, and modular project structure—you know when to use a macro and when not to",
        "Deep proficiency in SQL with experience optimizing queries for modern distributed warehouses (e.g., Amazon Athena, BigQuery, Snowflake), including partitioning and cost optimization",
        "Hands-on experience designing and maintaining analytics-ready data models and ELT pipelines from application and operational data sources",
        "Experience implementing or working with semantic layers or governed metrics frameworks (e.g., dbt semantic layer or equivalent)",
        "A meticulous, almost obsessive approach to data accuracy—you aren't satisfied until the numbers tie out 1:1 and you can prove it",
        "Demonstrated ability to reconcile complex datasets across systems and identify root causes of discrepancies",
        "A strong \"do it right\" mindset, including the ability to push back on unscalable requests and prioritize durable solutions over short-term fixes",
        "Understanding of data mesh principles, domain ownership, and the discipline required to maintain a standalone analytics data zone",
        "Strong communication skills and comfort level in influencing both technical and non-technical stakeholders",
        "Direct experience working on a Revenue Operations team",
        "Experience supporting Sales, Marketing, Customer Support, and Customer Success analytics",
        "Experience introducing or maturing a centralized data dictionary and driving organizational adoption of governed metrics",
        "Exposure to data mesh or domain-oriented data ownership models in production environments",
        "Experience applying AI-assisted development tools to analytics engineering workflows (SQL, dbt, testing, documentation, refactoring)",
        "Familiarity with revenue lifecycle metrics (pipeline, conversion rates, ARR/MRR, churn, expansion, forecasting)"
      ],
      "Benefits": [
        "Pathway to promotion to additional organizational positions and responsibilities based upon results and performance, not just time in the chair",
        "Passionate and intelligent colleagues who work hard and have a good time doing it",
        "Paid company-wide week off at the end of each year",
        "Flexible Vacation Policy",
        "Awesome company swag",
        "Full medical, dental, and vision benefits for US, UK, and Canada-based employees",
        "Full short-term disability and life benefits; available long-term disability",
        "Retirement savings account options with vested company matching for qualifying employees",
        "In-person annual gatherings",
        "DNSFilter is a pay-for-performance organization, which means there is an opportunity to advance your compensation based on performance over time",
        "The hiring base pay is dependent on several factors, including level, function, training, transferable skills, work experience, business needs, and geographic location",
        "$130,000—$170,000 USD"
      ],
      "Responsibilities": [
        "We're seeking a Senior Analytics Engineer, Revenue Operations, to own and scale the RevOps data domain",
        "This role sits on the Revenue Operations team, will report to the Senior Director, RevOps, and partners closely with Sales, Marketing, Customer Support, Customer Success, Finance, Product, and the Data Platform team to deliver trusted, analytics-ready data that powers GTM systems, reporting, and decision-making",
        "This is a senior individual contributor role with deep ownership",
        "You'll treat the RevOps data domain as a product, not a collection of dashboards—balancing rigor, scalability, and stakeholder alignment in a fast-moving environment",
        "Architect and own the RevOps / BizOps data zone within our Data Mesh, treating it as a product that is high-quality, discoverable, well-documented, and the authoritative source of truth for revenue data",
        "Establish clear ownership, documentation, and governance for core RevOps datasets and metrics",
        "Build & Scale Analytics Foundations",
        "Own the end-to-end design, development, and maintenance of analytics solutions that power downstream GTM systems, reporting, and decision-making",
        "Partner with the Data Platform (Data Engineering) team to transform data landed from internal databases, third-party APIs, and Airflow-managed pipelines into analytics-ready models",
        "Develop, optimize, and maintain ELT pipelines using dbt Cloud, transforming PostgreSQL and other source data into analytics-ready datasets in Amazon Athena",
        "Ensure reliable data availability, performance, and quality within our Athena-based analytics data zone",
        "Own Data State, History, and Performance",
        "Lead our approach to capturing data evolution and history, making heavy use of dbt snapshots and incremental models to support point-in-time and trend analysis",
        "Write performant, cost-aware SQL optimized for distributed query engines (Amazon Athena), including queries operating over tens of billions of rows",
        "Design models with scalability and maintainability in mind, prioritizing long-term clarity over clever but fragile solutions",
        "Design, implement, and maintain a semantic layer (e.g., dbt semantic models/metrics) that serves as the authoritative source for business definitions and revenue metrics",
        "Introduce and steward a centralized data dictionary and metric catalog, ensuring consistent definitions across dashboards, reports, and GTM systems",
        "Partner closely with RevOps, Finance, Product, and GTM stakeholders to define, govern, and evolve shared metrics—aligning on \"what is what\" as the business grows",
        "Act as a trusted arbiter for metric definitions, managing versioning, documentation, and change communication",
        "Model Revenue Data for Downstream GTM Systems",
        "Design data models that are consumable by the GTM Systems team for use in downstream operational tools such as Salesforce, Hubspot, and Zendesk",
        "Expose clean, well-documented datasets and metrics that can be reliably reused across reporting, automation, and operational workflows",
        "Partner with the GTM Systems team to ensure data models meet operational needs, performance requirements, and system constraints",
        "Ensure consistency between analytical models and operational system logic, minimizing metric drift between analytics and GTM tooling",
        "Own BI Reporting & Visualization",
        "Own the design, development, and ongoing maintenance of RevOps dashboards, reports, and visualizations in our BI tool",
        "Ensure all reporting is powered by governed, well-modeled, and tested datasets—avoiding one-off queries and metric drift",
        "Partner with GTM, RevOps, and executive stakeholders to translate business needs into scalable, self-service dashboards",
        "Continuously audit and improve existing dashboards for accuracy, performance, usability, and clarity",
        "Define standards and best practices for dashboard design, metric presentation, and reporting governance",
        "Uphold a \"simplicity at scale\" philosophy—choosing readable, maintainable SQL over over-engineered abstractions that accrue technical debt",
        "Maintain logic at the appropriate layer: when metrics require new or corrected product data, partner with upstream owners rather than introducing brittle downstream workarounds",
        "Review, audit, and refactor existing dbt models, ELT jobs, and dashboards to improve accuracy, performance, and maintainability",
        "Enable the Business & Look Forward",
        "Design, build, and maintain dashboards and reports powered by well-modeled, tested data—avoiding one-off queries and metric drift",
        "Support the GTM Systems team by ensuring analytics and reporting dependencies are reliable and well-documented",
        "Leverage AI-assisted development tools to accelerate SQL development, dbt modeling, testing, documentation, and refactoring",
        "Explore and pilot AI-enabled approaches to improve data quality, observability, and operational efficiency with sound judgment around accuracy and governance",
        "Establish and promote best practices for data modeling, testing, documentation, and dashboard governance"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-dnsfilter-job-senior-analytics-engineer-revenue-operations-in-tampa-fl",
    "_source": "new_jobs"
  },
  {
    "job_id": "FBlRk5khvOty77PNAAAAAA==",
    "job_title": "Senior Analytics Engineer",
    "employer_name": "Breakthrough",
    "employer_logo": null,
    "employer_website": "https://www.breakthroughclean.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/senior-analytics-engineer?id=2459354831&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/senior-analytics-engineer?id=2459354831&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-wi-all-cities-senior-analytics-engineer-49-airgarage-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary\nAbout Breakthrough\n\nAt Breakthrough, we empower our clients with data, technology, and market knowledge to reduce costs, create efficient networks, and decarbonize transportation. As a strategic partner to our clients, we challenge legacy practices in the $700 billion transportation industry, delivering sustainable fuel and freight products that foster fair partnerships and environmental responsibility.\n\nAs a digital product company, our culture thrives on curiosity, autonomy, and purpose-driven innovation. Here, you're not just executing tasks; you're collaborating with a team dedicated to transforming transportation and making a tangible impact on the global economy and the planet.\n\nThe Role\n\nAs a Senior Analytics Engineer, you will play a crucial role in the design, development, and implementation of robust, scalable, and high-performance analytics solutions. You will be responsible for analytic data modeling, building efficient ETL processes, and seamless integrations with SaaS products and visualization tools, leveraging your expertise to drive technical excellence across the team.\n\nWe're seeking individuals who excel in collaborative environments, are driven by the \"why\" behind their work and the core problems they're trying to solve, and are eager to develop innovative solutions while influencing the evolution of our systems, teams, and culture.\n\n\"At Breakthrough, we're figuring out things for the first time; things that no one else has ever done before\" - Engineering leader, Breakthrough\n\nThis role is open to work on-site in Green Bay or remote/hybrid options based on relevancy of candidate experience.\n\nJob Responsibilities\nWhat You'll Do\nCollaborate with cross-functional teams—including product owners, designers, and fellow engineers—to build reliable, scalable systems. Working closely with our clients, market experts, and the broader Breakthrough team, you will build innovative solutions that solve industry problems. Lead the end-to-end design and implementation of new product features and services. Learn and apply the latest technologies to our cloud native solutions. Write clean, maintainable code accompanied by comprehensive automated tests. Review pull requests, mentor other engineers, and elevate the technical standards within the team. Contribute to technical strategy, system design, and process improvements. Participate in our on-call rotation and support production systems, embracing a shared ownership model.\n\nLearn more about the work you'll do at Breakthrough and our culture here.\n\nThis Role Might Be a Great Fit If…\nYou're enthusiastic about tackling complex challenges and can distill them into actionable solutions. You prioritize writing clean, readable, and testable code, while understanding the importance of pragmatism. Understanding the underlying purpose of your work motivates you, beyond merely delivering features. You thrive in collaborative settings, engaging with engineers, product owners, and designers to achieve common goals. You're committed to advancing sustainability in transportation and reducing environmental impact through technological innovation.\n\nThis Role Might Not Be the Best Fit If…\nYou prefer working in isolation or solely on predefined tasks without broader context. Adaptability to shifting priorities in a dynamic environment is challenging for you. Collaborating with non-engineering disciplines, such as product and design, doesn't align with your working style. You seek a rigid hierarchical structure to guide all decision-making processes. Mentoring others and contributing to team growth aren't areas of interest for you.\n\nHow We Work\nHybrid-Friendly: While many team members are based in Green Bay, we embrace remote work and prioritize impact over location. Cross-Functional Teams: You'll be part of an agile team of fellow engineers, fostering a holistic approach to product development by working with Product Owners, Quality Assurance, and others throughout the organization. Continuous Improvement: We regularly conduct retrospectives, refine our processes, and invest in addressing technical debt to enhance our workflows. Empowered Engineers: Beyond task execution, you're encouraged to influence both what we build and how we build it, ensuring alignment with our strategic objectives.\n\nQualifications\nWhat you bring\nBachelor's degree in Computer Science or a related technical field involving coding (e.g, physics or mathematics), or equivalent technical experience. 10+ years as a data or analytics engineer, with a focus on a combination of data modeling and engineering. Experience in designing and implementing scalable and high-performance analytics features. Proficiency in multiple programming languages, frameworks, and technologies, specifically dbt, SQL, Jinja, BigQuery, cube, and Python. Knowledge of data architectures, security best practices, and data integration concepts. Experience with ETL technologies and data engineering with a focus on building and maintaining data platforms, data pipelines, and data ingestion tools. Exposure to modern infrastructure such as code technologies like Docker, Kubernetes, Terraform, and Airflow. In depth understanding of database concepts, data modeling, and data warehousing principles. Understanding of distributed data management systems and related applications. Familiarity with public cloud infrastructure design, tools, and strategies. Experience with software development methodologies including Agile, Kanban, and Scrum A growth mindset and excitement for using AI to drive innovation, improve workflows and solve complex problems. Collaborative by nature, you value pairing, whiteboarding sessions, and learning from diverse perspectives. Passionate about delivering high-quality products that enhance client experiences and drive sustainability in transportation.\n\nTech We Use\nBackend: Python, Postgres Frontend: Node, React, TypeScript, graphQL Cloud & Infrastructure: Google Cloud Platform (GCP), Terraform, Docker Data & Analytics: BigQuery, dbt Monitoring & Observability: GCP Monitoring\n\nWhy Breakthrough\nMission-Driven Work: Engage in projects that have a tangible impact on the economy and the environment. Hear more about our innovation in this video. Supportive Culture: Experience a workplace that values autonomy, growth, and meaningful contributions. Leadership Opportunities: Take on roles that allow you to mentor, guide, and shape the future of our products and technological direction. Established Backing: As a U.S. Venture company, we benefit from a legacy of innovation and a commitment to sustainable practices.\n\nApply Now\nIf you're driven to develop smarter, cleaner transportation solutions and want to be part of a team that's making a difference, we'd love to hear from you\nBreakthrough\n\nDIVISION\n:\n\nBreakthrough\n\nU.S. Venture requires that a team member have and maintain authorization to work in the country in which the role is based. In general, U.S. Venture does not sponsor candidates for nonimmigrant visas or permanent residency unless based on business need.\nU.S. Venture will not accept unsolicited resumes from recruiters or employment agencies. In the absence of an executed recruitment Master Service Agreement, there will be no obligation to any referral compensation or recruiter fee. In the event a recruiter or agency submits a resume or candidate without an agreement, U.S. Venture shall reserve the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency. Any unsolicited resumes, including those submitted to hiring managers, shall be deemed the property of U.S. Venture.\nU.S. Venture, Inc. is an equal opportunity employer that is committed to inclusion and diversity. We ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic.\nIf you need assistance or an accommodation due to a disability, you may call Human Resources",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Wisconsin",
    "job_city": null,
    "job_state": "Wisconsin",
    "job_country": "US",
    "job_latitude": 43.7844397,
    "job_longitude": -88.7878678,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DFBlRk5khvOty77PNAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "You're enthusiastic about tackling complex challenges and can distill them into actionable solutions",
        "You prioritize writing clean, readable, and testable code, while understanding the importance of pragmatism",
        "You prefer working in isolation or solely on predefined tasks without broader context",
        "Adaptability to shifting priorities in a dynamic environment is challenging for you",
        "Collaborating with non-engineering disciplines, such as product and design, doesn't align with your working style",
        "Bachelor's degree in Computer Science or a related technical field involving coding (e.g, physics or mathematics), or equivalent technical experience",
        "10+ years as a data or analytics engineer, with a focus on a combination of data modeling and engineering",
        "Experience in designing and implementing scalable and high-performance analytics features",
        "Proficiency in multiple programming languages, frameworks, and technologies, specifically dbt, SQL, Jinja, BigQuery, cube, and Python",
        "Knowledge of data architectures, security best practices, and data integration concepts",
        "Experience with ETL technologies and data engineering with a focus on building and maintaining data platforms, data pipelines, and data ingestion tools",
        "Exposure to modern infrastructure such as code technologies like Docker, Kubernetes, Terraform, and Airflow",
        "In depth understanding of database concepts, data modeling, and data warehousing principles",
        "Understanding of distributed data management systems and related applications",
        "Familiarity with public cloud infrastructure design, tools, and strategies",
        "Experience with software development methodologies including Agile, Kanban, and Scrum A growth mindset and excitement for using AI to drive innovation, improve workflows and solve complex problems",
        "Collaborative by nature, you value pairing, whiteboarding sessions, and learning from diverse perspectives",
        "Passionate about delivering high-quality products that enhance client experiences and drive sustainability in transportation"
      ],
      "Responsibilities": [
        "As a Senior Analytics Engineer, you will play a crucial role in the design, development, and implementation of robust, scalable, and high-performance analytics solutions",
        "You will be responsible for analytic data modeling, building efficient ETL processes, and seamless integrations with SaaS products and visualization tools, leveraging your expertise to drive technical excellence across the team",
        "Collaborate with cross-functional teams—including product owners, designers, and fellow engineers—to build reliable, scalable systems",
        "Working closely with our clients, market experts, and the broader Breakthrough team, you will build innovative solutions that solve industry problems",
        "Lead the end-to-end design and implementation of new product features and services",
        "Learn and apply the latest technologies to our cloud native solutions",
        "Write clean, maintainable code accompanied by comprehensive automated tests",
        "Review pull requests, mentor other engineers, and elevate the technical standards within the team",
        "Contribute to technical strategy, system design, and process improvements",
        "Participate in our on-call rotation and support production systems, embracing a shared ownership model",
        "Understanding the underlying purpose of your work motivates you, beyond merely delivering features",
        "You thrive in collaborative settings, engaging with engineers, product owners, and designers to achieve common goals",
        "You're committed to advancing sustainability in transportation and reducing environmental impact through technological innovation",
        "You seek a rigid hierarchical structure to guide all decision-making processes",
        "Cross-Functional Teams: You'll be part of an agile team of fellow engineers, fostering a holistic approach to product development by working with Product Owners, Quality Assurance, and others throughout the organization",
        "Continuous Improvement: We regularly conduct retrospectives, refine our processes, and invest in addressing technical debt to enhance our workflows",
        "Empowered Engineers: Beyond task execution, you're encouraged to influence both what we build and how we build it, ensuring alignment with our strategic objectives"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-senior-analytics-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "TS4IgSLzbYSiKuL6AAAAAA==",
    "job_title": "Lead Data Product Engineer",
    "employer_name": "Disney Experiences",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFU7L5wyZ-tHqOMi8SFhH7dXkKGqRCp8KvYWMN&s=0",
    "employer_website": "https://disneyexperiences.com",
    "job_publisher": "Disney Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.disneycareers.com/en/job/glendale/lead-data-product-engineer/391/87077257008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Disney Careers",
        "apply_link": "https://www.disneycareers.com/en/job/glendale/lead-data-product-engineer/391/87077257008?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talents By Vaia",
        "apply_link": "https://talents.vaia.com/companies/disneyland-hong-kong/lead-data-engineer-38583087/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Swooped",
        "apply_link": "https://swooped.co/job-postings/lead-data-engineer-glendale-disney-media-entertainment-distribution-e47e7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "GetHiredToday - Women For Hire",
        "apply_link": "https://jobs.womenforhire.com/job/usa/glendale-ca/lead-data-product-engineer-600741/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5630484438?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobMonkey Jobs",
        "apply_link": "https://www.jobmonkeyjobs.com/career/27443459/Lead-Data-Product-Engineer-California-Glendale-7308?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Bandana.com",
        "apply_link": "https://bandana.com/jobs/e1d2fbef-0671-4ce1-8d17-98815dd76276?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "J-O-B-Z",
        "apply_link": "https://j-o-b-z.com/seo/job/136602141/ca/glendale/lead-data-product-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Disney Experiences Technology, our team creates world-class immersive digital experiences for the Company’s premier vacation brands including Disney’s Parks & Resorts worldwide, Disney Cruise Line, Aulani, A Disney Resort & Spa, and Disney Vacation Club. The Disney Experiences Technology team is responsible for the end-to-end digital and physical Guest experience for all technology & digital-led initiatives across the Attractions & Entertainment, Food & Beverage, Resorts & Transportation, and Merchandise lines of business as well as other initiatives including the MyDisneyExperience app and Hey, Disney!\n\nIf you’re passionate about designing and engineering complex, large-scale data products, come join us! We’re seeking a Lead Data Product Engineer to architect, build, and optimize enterprise-grade data solutions. You’ll own the full lifecycle of data products, from ingestion and transformation to architecture and enablement, while ensuring reliability and scalability.\n\nAs a technical lead, you’ll shape data product design, define contracts and schemas, and partner closely with data source owners, engineers, architects, and end-user teams. This is not a pure coding role, instead, you will engineer the product layer of data, designing assets, writing queries, and translating complex source data into consumable products that data engineers implement and scale.\n\nYou will sit within Data Products & Platforms for Disney Experiences, collaborating with internal teams and technical leaders across the company.\n\nYou Will\n• Architect and deliver enterprise data models, schemas, and contracts to support multi-source ingestion, transformation, and consumption at scale.\n• Own the full product lifecycle for data domains, including design, versioning, governance, and deprecation.\n• Define and evolve technical strategies for high-volume operational and analytical workloads.\n• Act as the data and technical subject matter expert, covering not only your domain and products but also the enablement capabilities of our modern data ecosystem with partners.\n• Partner with architects and data engineers to design and implement scalable, secure, high-throughput pipelines and consumption-ready data products.\n• Translate business requirements into technical specifications and engineering deliverables.\n• Perform deep reviews and gap assessments of existing data flows, modernizing legacy pipelines into modern cloud-native ecosystems.\n• Manage technical acceptance criteria, SLAs, timelines, and cost controls for data products.\n• Represent the data product engineering function in executive forums, advocating for technical excellence and data-driven strategy.\n\nYou Have\n• 7+ years designing and delivering large-scale data products, with proven experience leading data solutions and/or engineering teams.\n• Hands-on expertise in data product management or data engineering, especially in complex business and operational domains.\n• Advanced skills in data modeling and documentation (conceptual, logical, physical), driving solutions end-to-end: discovery to deployed pipelines supporting BI, analytics, and AI/ML.\n• Deep technical fluency with data engineering concepts and platforms (AWS: S3, Lambda, Step Functions, Glue), data platforms (Snowflake), governance (data contracts), transformation and orchestration (dbt, Airflow), and streaming/event technologies (Kinesis, Pub/Sub patterns, Kafka).\n• Strong understanding of batch and real-time architectures, including event-driven and streaming solutions.\n• Proven ability to partner and influence across technical and business teams; clear, concise communicator who can translate complexity.\n• Recognized technical authority in data product engineering practices, defining best-in-class methods and coaching others on process and execution.\n\nRequired Education\n• Bachelor’s degree in Computer Science, Information Systems, or related field—or equivalent professional experience.\n\n#DISNEYTECH\n\nThe hiring range for this position in California is $166,800 - $223,600 per year based on a 40 hour work week. The amount of hours scheduled per week may vary based on business needs. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Glendale, CA",
    "job_city": "Glendale",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.146367399999995,
    "job_longitude": -118.2488703,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DTS4IgSLzbYSiKuL6AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "7+ years designing and delivering large-scale data products, with proven experience leading data solutions and/or engineering teams",
        "Hands-on expertise in data product management or data engineering, especially in complex business and operational domains",
        "Advanced skills in data modeling and documentation (conceptual, logical, physical), driving solutions end-to-end: discovery to deployed pipelines supporting BI, analytics, and AI/ML",
        "Deep technical fluency with data engineering concepts and platforms (AWS: S3, Lambda, Step Functions, Glue), data platforms (Snowflake), governance (data contracts), transformation and orchestration (dbt, Airflow), and streaming/event technologies (Kinesis, Pub/Sub patterns, Kafka)",
        "Strong understanding of batch and real-time architectures, including event-driven and streaming solutions",
        "Proven ability to partner and influence across technical and business teams; clear, concise communicator who can translate complexity",
        "Recognized technical authority in data product engineering practices, defining best-in-class methods and coaching others on process and execution",
        "Bachelor’s degree in Computer Science, Information Systems, or related field—or equivalent professional experience"
      ],
      "Benefits": [
        "The hiring range for this position in California is $166,800 - $223,600 per year based on a 40 hour work week",
        "The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors",
        "A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered"
      ],
      "Responsibilities": [
        "You’ll own the full lifecycle of data products, from ingestion and transformation to architecture and enablement, while ensuring reliability and scalability",
        "As a technical lead, you’ll shape data product design, define contracts and schemas, and partner closely with data source owners, engineers, architects, and end-user teams",
        "This is not a pure coding role, instead, you will engineer the product layer of data, designing assets, writing queries, and translating complex source data into consumable products that data engineers implement and scale",
        "You will sit within Data Products & Platforms for Disney Experiences, collaborating with internal teams and technical leaders across the company",
        "Architect and deliver enterprise data models, schemas, and contracts to support multi-source ingestion, transformation, and consumption at scale",
        "Own the full product lifecycle for data domains, including design, versioning, governance, and deprecation",
        "Define and evolve technical strategies for high-volume operational and analytical workloads",
        "Act as the data and technical subject matter expert, covering not only your domain and products but also the enablement capabilities of our modern data ecosystem with partners",
        "Partner with architects and data engineers to design and implement scalable, secure, high-throughput pipelines and consumption-ready data products",
        "Translate business requirements into technical specifications and engineering deliverables",
        "Perform deep reviews and gap assessments of existing data flows, modernizing legacy pipelines into modern cloud-native ecosystems",
        "Manage technical acceptance criteria, SLAs, timelines, and cost controls for data products",
        "Represent the data product engineering function in executive forums, advocating for technical excellence and data-driven strategy"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-disneycareers-com-en-job-glendale-lead-data-product-engineer-391-87077257008",
    "_source": "new_jobs"
  },
  {
    "job_id": "Sb3uO7REaIixrkvHAAAAAA==",
    "job_title": "Global IT Data Engineer Expert Director",
    "employer_name": "Boston Consulting Group",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxMy3OJRSwXcqB_s6dIHtI8zYYudxFVfaOcfGQ&s=0",
    "employer_website": null,
    "job_publisher": "BCG Careers - Boston Consulting Group",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.bcg.com/global/en/job/55729/Global-IT-Data-Engineer-Expert-Director?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BCG Careers - Boston Consulting Group",
        "apply_link": "https://careers.bcg.com/global/en/job/55729/Global-IT-Data-Engineer-Expert-Director?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/The-Boston-Consulting-Group/Job/Global-IT-Data-Engineer-Expert-Director/-in-Boston,MA?jid=17684eef51a37d98&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/e4byAxy-W_1hzBi_eJs6OAgWyx4fcuvm_4R4U2Pi128Nw6uZW-kIbA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=7031f37702de&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/global-it-data-engineer-expert-director-at-boston-consulting-group-bcg-4318684330?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "InHerSight",
        "apply_link": "https://www.inhersight.com/company/the-boston-consulting-group/job/ihs/zf4bmxvw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "KSNT Jobs",
        "apply_link": "https://jobs.ksnt.com/jobs/global-it-data-engineer-expert-director-boston-massachusetts/2608970072-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital ventures—and business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nAs Global IT Data Engineer Expert Director, you will shape and lead the vision for enterprise data architecture, pipelines, and governance while remaining a hands-on technical contributor. You will ensure high-quality, trusted data powers analytics, reporting, and emerging GenAI use cases. Your leadership will bridge strategy, execution, and innovation—guiding the team while actively engaging in design and delivery.\n\nWHAT YOU’LL DO\n• Architect and build enterprise-grade data pipelines and models enabling downstream analytics, reporting, and GenAI-driven insights.\n• Actively contribute to the development of scalable data solutions using dbt, Apache Airflow, FiveTran/AWS Glue and Snowflake, optimizing for reliability, observability, and performance.\n• Champion data quality by ensuring completeness, accuracy, timeliness, and lineage across ingestion, transformation, and delivery.\n• Partner with analytics, AI, and business teams to deliver high-value data assets that power BI dashboards, predictive models, and generative AI applications.\n• Lead cross-functional collaboration with product owners, architects, and governance teams to define and evolve data standards and architectures.\n• Drive experimentation, developing proofs of concept and minimum viable products to evaluate new technologies and patterns.\n• Establish and enforce data performance, quality, and security policies aligned with enterprise governance and regulatory frameworks.\n• Serve as a player-coach—mentoring engineers while contributing directly to complex design and code reviews.\n• Lead resolution of critical production issues, triaging technical challenges that demand immediate action.\n\nYOU’RE GOOD AT\n\nYou bring a rare combination of strategic leadership and deep technical ability. You can design data architectures at scale while still contributing directly to engineering excellence.\n• Expert in data modeling, warehousing, and pipeline orchestration, with proven ability to design and implement production-grade data systems.\n• Hands-on experience developing and optimizing ETL/ELT pipelines with SQL, Python, dbt, Airflow, FiveTran, AWS Glue, and Snowflake.\n• Skilled at analyzing and resolving performance bottlenecks in complex data flows, transformations, and reporting layers.\n• Deep understanding of data quality frameworks such as SCD, CDC, and DQ or DV, with a focus on automated validation and observability.\n• Comfortable working with structured, semi-structured, and unstructured data for analytics and GenAI consumption.\n• Experienced in integrating pipelines with AI, ML, and GenAI workflows, ensuring models are powered by high-fidelity, explainable data.\n• Collaborative and pragmatic—able to dive into the codebase when needed while guiding architectural direction for scalability and reliability.\n• Strong advocate for Agile principles, iterative improvement, and an engineering culture rooted in accountability, innovation, and mentorship.\n\nWhat You'll Bring\n• Over ten years of experience in data engineering, architecture, or platform leadership, including at least three years leading technical teams.\n• Proven experience as a hands-on data engineer or architect capable of designing, coding, and reviewing complex data solutions.\n• Expertise in modern data stacks including Snowflake, dbt, Airflow, AWS, GCP, or Azure, and integration tools such as FiveTran or Glue.\n• Strong track record in data modeling, ETL or ELT orchestration, and implementation of data governance frameworks.\n• Demonstrated ability to design secure, compliant, and high-performing data architectures.\n• Deep understanding of data observability, monitoring, and lineage tools.\n• Familiarity with APIs, event-driven architectures, and near real-time data pipelines.\n• Excellent communication and presentation skills, with the ability to translate technical solutions into strategic business value.\n\nEssential Education\n• Bachelor's degree or equivalent combination of education and experience.\n• Bachelor's degree in information science, data management, computer science or related field preferred.\n\nWho You'll Work With\n• Legal product portfolio technical leaders, product owners, functional area teams across levels\n• Global IT Teams (Enterprise Architecture, Global Data Portfolio)\n• Consulting and internal Data Product Portfolio teams across BCG\n\nAdditional info\n• ** For US locations only ***\n\nIn the US, we have a compensation transparency approach.\n\nTotal compensation for this role includes base salary, annual discretionary performance bonus, retirement contribution, and a market leading benefits package described below.\n\n• The base salary range for this role in Boston is $180,000.00 - $219,300.00.\n\nThis is an estimated range, however, specific base salaries within the range depend on various factors such as experience and skill set. It is not common for new BCG employees to be hired at the high-end of the salary range. BCG regularly reviews its ranges to ensure market competitiveness.\n\nIn addition to your base salary, your total compensation will include a bonus of up to 30% and a generous retirement contribution that starts at 5% and moves to 10% after 2 years.\n\nAll of our plans provide best in class coverage:\n• Zero dollar ($0) health insurance premiums for BCG employees, spouses, and children\n• Low $10 (USD) copays for trips to the doctor, urgent care visits and prescriptions for generic drugs\n• Dental coverage, including up to $5,000 in orthodontia benefits\n• Vision insurance with coverage for both glasses and contact lenses annually\n• Reimbursement for gym memberships and other fitness activities\n• Fully vested Profit Sharing Retirement Fund contributions made annually, whether you contribute or not, plus the option for employees to make personal contributions to a 401(k) plan\n• Paid Parental Leave and other family benefits such as elective egg freezing, surrogacy, and adoption reimbursement\n• Generous paid time off including 12 holidays per year, an annual office closure between Christmas and New Years, and 15 vacation days per year (earned at 1.25 days per month)\n• Paid sick time on an as needed basis\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771200000,
    "job_posted_at_datetime_utc": "2026-02-16T00:00:00.000Z",
    "job_location": "Boston, MA",
    "job_city": "Boston",
    "job_state": "Massachusetts",
    "job_country": "US",
    "job_latitude": 42.355507599999996,
    "job_longitude": -71.0565364,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DSb3uO7REaIixrkvHAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "You bring a rare combination of strategic leadership and deep technical ability",
        "You can design data architectures at scale while still contributing directly to engineering excellence",
        "Expert in data modeling, warehousing, and pipeline orchestration, with proven ability to design and implement production-grade data systems",
        "Hands-on experience developing and optimizing ETL/ELT pipelines with SQL, Python, dbt, Airflow, FiveTran, AWS Glue, and Snowflake",
        "Over ten years of experience in data engineering, architecture, or platform leadership, including at least three years leading technical teams",
        "Proven experience as a hands-on data engineer or architect capable of designing, coding, and reviewing complex data solutions",
        "Expertise in modern data stacks including Snowflake, dbt, Airflow, AWS, GCP, or Azure, and integration tools such as FiveTran or Glue",
        "Strong track record in data modeling, ETL or ELT orchestration, and implementation of data governance frameworks",
        "Demonstrated ability to design secure, compliant, and high-performing data architectures",
        "Deep understanding of data observability, monitoring, and lineage tools",
        "Familiarity with APIs, event-driven architectures, and near real-time data pipelines",
        "Excellent communication and presentation skills, with the ability to translate technical solutions into strategic business value",
        "Bachelor's degree or equivalent combination of education and experience"
      ],
      "Benefits": [
        "Total compensation for this role includes base salary, annual discretionary performance bonus, retirement contribution, and a market leading benefits package described below",
        "The base salary range for this role in Boston is $180,000.00 - $219,300.00",
        "This is an estimated range, however, specific base salaries within the range depend on various factors such as experience and skill set",
        "In addition to your base salary, your total compensation will include a bonus of up to 30% and a generous retirement contribution that starts at 5% and moves to 10% after 2 years",
        "Zero dollar ($0) health insurance premiums for BCG employees, spouses, and children",
        "Low $10 (USD) copays for trips to the doctor, urgent care visits and prescriptions for generic drugs",
        "Dental coverage, including up to $5,000 in orthodontia benefits",
        "Vision insurance with coverage for both glasses and contact lenses annually",
        "Reimbursement for gym memberships and other fitness activities",
        "Fully vested Profit Sharing Retirement Fund contributions made annually, whether you contribute or not, plus the option for employees to make personal contributions to a 401(k) plan",
        "Paid Parental Leave and other family benefits such as elective egg freezing, surrogacy, and adoption reimbursement",
        "Generous paid time off including 12 holidays per year, an annual office closure between Christmas and New Years, and 15 vacation days per year (earned at 1.25 days per month)",
        "Paid sick time on an as needed basis"
      ],
      "Responsibilities": [
        "As Global IT Data Engineer Expert Director, you will shape and lead the vision for enterprise data architecture, pipelines, and governance while remaining a hands-on technical contributor",
        "You will ensure high-quality, trusted data powers analytics, reporting, and emerging GenAI use cases",
        "Your leadership will bridge strategy, execution, and innovation—guiding the team while actively engaging in design and delivery",
        "Architect and build enterprise-grade data pipelines and models enabling downstream analytics, reporting, and GenAI-driven insights",
        "Actively contribute to the development of scalable data solutions using dbt, Apache Airflow, FiveTran/AWS Glue and Snowflake, optimizing for reliability, observability, and performance",
        "Champion data quality by ensuring completeness, accuracy, timeliness, and lineage across ingestion, transformation, and delivery",
        "Partner with analytics, AI, and business teams to deliver high-value data assets that power BI dashboards, predictive models, and generative AI applications",
        "Lead cross-functional collaboration with product owners, architects, and governance teams to define and evolve data standards and architectures",
        "Drive experimentation, developing proofs of concept and minimum viable products to evaluate new technologies and patterns",
        "Establish and enforce data performance, quality, and security policies aligned with enterprise governance and regulatory frameworks",
        "Serve as a player-coach—mentoring engineers while contributing directly to complex design and code reviews",
        "Lead resolution of critical production issues, triaging technical challenges that demand immediate action",
        "Skilled at analyzing and resolving performance bottlenecks in complex data flows, transformations, and reporting layers",
        "Deep understanding of data quality frameworks such as SCD, CDC, and DQ or DV, with a focus on automated validation and observability",
        "Comfortable working with structured, semi-structured, and unstructured data for analytics and GenAI consumption",
        "Experienced in integrating pipelines with AI, ML, and GenAI workflows, ensuring models are powered by high-fidelity, explainable data",
        "Collaborative and pragmatic—able to dive into the codebase when needed while guiding architectural direction for scalability and reliability",
        "Strong advocate for Agile principles, iterative improvement, and an engineering culture rooted in accountability, innovation, and mentorship",
        "Legal product portfolio technical leaders, product owners, functional area teams across levels",
        "Global IT Teams (Enterprise Architecture, Global Data Portfolio)",
        "Consulting and internal Data Product Portfolio teams across BCG"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-bcg-com-global-en-job-55729-global-it-data-engineer-expert-director",
    "_source": "new_jobs"
  },
  {
    "job_id": "4PMXCF0Zj3csVibCAAAAAA==",
    "job_title": "Data Engineering Manager - Advanced Analytics",
    "employer_name": "Niagara Bottling",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQD1lp7Y_nPmktOrGBYuyI9Zyj9tBqyLbRlieZB&s=0",
    "employer_website": "https://www.niagarawater.com",
    "job_publisher": "Niagara Bottling LLC Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.niagarawater.com/us/en/job/R45735/Data-Engineering-Manager-Advanced-Analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Niagara Bottling LLC Careers",
        "apply_link": "https://careers.niagarawater.com/us/en/job/R45735/Data-Engineering-Manager-Advanced-Analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=b3a692c155871761&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/advanced-analytics-manager-data-engineering-diamond-bar-niagara-bottling-f8a97cc14e8a3b1597c041e2c1db509c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In LA",
        "apply_link": "https://www.builtinla.com/job/lead-power-bi-developer/4193106?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Tech Jobs Personalized",
        "apply_link": "https://builtin.com/job/lead-power-bi-developer/4193106?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Niagara-Bottling/Job/Data-Engineering-Manager-Advanced-Analytics/-in-Diamond-Bar,CA?jid=3b90f12867465791&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineering-manager-advanced-analytics-at-niagara-bottling-4354561864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Niagara, we’re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.\n\nConsider applying here, if you want to:\n• Work in an entrepreneurial and dynamic environment with a chance to make an impact.\n• Develop lasting relationships with great people.\n• Have the opportunity to build a satisfying career.\n\nWe offer competitive compensation and benefits packages for our Team Members.\n\nData Engineering Manager - Advanced Analytics\n\nAs a key people leader within our data and analytics function, the Advanced Analytics Manager plays a critical role in architecting and maintaining the data infrastructure that underpins enterprise analytics. This role leads a team of data engineers and analytics professionals, focusing on the design, implementation, and optimization of scalable, reliable, and secure data pipelines, especially for complex, high-volume sources such as IoT and sensor-based systems. Working cross-functionally with operations, IT, and business stakeholders, the Advanced Analytics Manager ensures that data from diverse sources, including real-time telemetry, manufacturing systems, and traditional enterprise platforms are efficiently ingested, transformed, and made accessible for analytical consumption. In addition, the role includes light but growing exposure to Generative AI use cases, such as document summarization, chat interfaces for data access, and large language model (LLM) integration—especially in scenarios that augment data accessibility and user experience across the organization. This position requires not only technical expertise but also strategic thinking and leadership skills to evolve data infrastructure, support analytics scalability, and drive a data-first culture. A proven ability to lead high-performing teams and deliver impactful solutions in dynamic, data-rich environments is essential for success.\n• Lead and manage a team of data engineers and analytics professionals, providing strategic direction, mentorship, train, coach, performance reviews, and hands-on support to foster a collaborative, high-performing team environment focused on delivering impactful data solutions.\n• Establish clear objectives and key results (OKRs) for the team that align with the enterprise analytics and data strategy, ensuring close coordination with business goals and evolving priorities.\n• Conduct regular performance reviews, deliver constructive feedback, and champion continuous learning by identifying training, upskilling, and development opportunities for team members.\n• Partner with cross-functional teams, including business analysts, data scientists, IT, and key business stakeholders, to understand data requirements and deliver solutions that support business intelligence, operational reporting, and advanced analytics use cases.\n• Define and implement best practices in data engineering, covering data modeling, pipeline orchestration (ETL/ELT), data integration, and data quality, while ensuring reliable access to data from diverse sources such as databases, APIs, cloud platforms, and IoT systems.\n• Drive continuous improvement and innovation in data architecture and engineering techniques, with a focus on increasing scalability, performance, and reusability across the analytics ecosystem.\n• Oversee project planning and execution, balancing team capacity with priority management, delivery timelines, and quality standards to ensure successful outcomes for strategic and operational analytics initiatives.\n• Collaborate with cross-functional business and IT stakeholders to define and implement data and analytics strategies that support enterprise decision-making, ensuring data integrity, security, and governance across all analytical initiatives.\n• Provide technical leadership in data engineering and analytics infrastructure, leveraging modern cloud-native tools to enable high-performance data platforms that support BI, reporting, and advanced analytics.\n• Champion a culture of data innovation and continuous learning, encouraging the team to explore new data patterns, tooling, and architectural practices that improve scalability, reliability, and time-to-insight.\n• Oversee resource planning, vendor coordination, and budget management for the analytics engineering function, ensuring alignment with strategic priorities and operational efficiency.\n• Lead the design and implementation of high-volume, scalable data pipelines, models, and data marts across Data Lakes and Data Warehouses to support reporting, dashboarding, and analytics workloads.\n• Translate technical and project outcomes into business context through executive-ready presentations and analytics steering committee briefings, facilitating alignment and decision-making.\n• Demonstrate proficiency in Microsoft Azure Cloud (preferred) and/or Amazon Web Services, particularly within data engineering and analytics service ecosystems (e.g., Azure Data Factory, Synapse, Databricks, Redshift).\n• Build and manage DevOps pipelines for CI/CD and analytics solution deployment, ensuring consistency, automation, and compliance with change control standards.\n• Serve as a proactive leader who delivers on commitments and identifies opportunities for process improvement, team development, and organizational data literacy.\n• Coordinate and collaborate with onsite and offshore consultants to support scalable delivery and knowledge transfer across global analytics initiatives.\n• Maintain flexibility and responsiveness to evolving business needs; this job description may be adjusted to reflect emerging priorities or strategic direction.\n\nPlease note that this job description is not designed to contain a comprehensive list of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without prior notice.\n\nAdditionally, the Advanced Analytics Manager is expected to demonstrate:\n• Excellent communication, leadership and collaboration skills\n• Possesses solid project management skills\n• Advanced decision making and problem-solving skills\n• Ability to guide technical projects successfully from inception to completion\n• Experienced with Microsoft Word, Excel, PowerPoint, Visio and Outlook\n• Team Work - balances team and individual responsibilities; gives and welcomes feedback; contributes to building a positive team spirit; puts success of team above own interests; able to build morale and group commitments to goals and objectives\n• Adaptability - adapts to changes in the work environment; able to deal with frequent change, delays, or unexpected events\n• Planning/Organizing - uses time efficiently; plans for additional resources; sets goals and objectives; organizes or schedules other people and their tasks\n• Project Management - coordinates projects; communicates changes and progress; completes projects on time; budget management; manages project team activities\n• Oral Communication - speaks clearly and persuasively in positive or negative situations; listens and gets clarification; responds well to questions\n• Excellent team player\n\nWork Experience\n• Required:\n• 8-10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities\n• 8-10 years – Experience in data modeling, ETL processes, and data warehousing\n• 8-10 years – Experience in building and managing enterprise scale data pipelines\n• 8-10 years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)\n• 8-10 years – Experience in DevOps, CI/CD pipelines\n• 8-10 years – Experience in Python/R, Spark/Kafka\n• 8-10 years – Experience with Azure Databricks/Data Factory or similar technology\n• 8-10 years – Experience in Project management\n• Preferred:\n• 10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities\n• 10+ years – Experience in data modeling, ETL processes, and data warehousing\n• 10+ years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)\n• 10+ years – Experience in DevOps, CI/CD pipelines\n• 10+ years – Experience in Python/R, Spark/Kafka\n• 10+ years – Experience with Azure Databricks/Data Factory or similar technology\n• 10+ years – Experience in Project management\n• 2 years – Experience with Large language Models and building Gen AI applications\n\nEducation\n• Minimum Required:\n• Bachelor's Degree in Computer Science or Engineering\n• Preferred:\n• Master's Degree in Computer Science or Engineering\n\nCertification/License:\n• Required: None Required\n• Preferred: Data Engineering/Cloud certifications/Gen AI\n\nTypical Compensation Range\nPay Rate Type: Salary\n\n$136,778.46 - $198,328.77 / Yearly\n\nBonus Target: 10% Annual\n\nBenefits\n\nhttps://careers.niagarawater.com/us/en/benefits\n\nAny employment agency, person or entity that submits a résumé into this career site or to a hiring manager does so with the understanding that the applicant's résumé will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.\n\nEmployment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit résumé to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit résumé into this career site to be eligible for placement fees.\nNiagara Plant Name\nCORP-MAIN",
    "job_is_remote": false,
    "job_posted_at": "1 day ago",
    "job_posted_at_timestamp": 1771286400,
    "job_posted_at_datetime_utc": "2026-02-17T00:00:00.000Z",
    "job_location": "Diamond Bar, CA",
    "job_city": "Diamond Bar",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 34.0009951,
    "job_longitude": -117.8112041,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4PMXCF0Zj3csVibCAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This position requires not only technical expertise but also strategic thinking and leadership skills to evolve data infrastructure, support analytics scalability, and drive a data-first culture",
        "Possesses solid project management skills",
        "Advanced decision making and problem-solving skills",
        "Ability to guide technical projects successfully from inception to completion",
        "Experienced with Microsoft Word, Excel, PowerPoint, Visio and Outlook",
        "Team Work - balances team and individual responsibilities; gives and welcomes feedback; contributes to building a positive team spirit; puts success of team above own interests; able to build morale and group commitments to goals and objectives",
        "Excellent team player",
        "8-10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities",
        "8-10 years – Experience in data modeling, ETL processes, and data warehousing",
        "8-10 years – Experience in building and managing enterprise scale data pipelines",
        "8-10 years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)",
        "8-10 years – Experience in DevOps, CI/CD pipelines",
        "8-10 years – Experience in Python/R, Spark/Kafka",
        "8-10 years – Experience with Azure Databricks/Data Factory or similar technology",
        "8-10 years – Experience in Project management",
        "10 years – Experience managing an onsite team of 5-6 full time direct reports, to include mentoring, coaching, performance reviews, and team building activities",
        "10+ years – Experience in data modeling, ETL processes, and data warehousing",
        "10+ years – Experience with cloud-based data platforms (e.g., AWS, Azure, GCP)",
        "10+ years – Experience in DevOps, CI/CD pipelines",
        "10+ years – Experience in Python/R, Spark/Kafka",
        "10+ years – Experience with Azure Databricks/Data Factory or similar technology",
        "10+ years – Experience in Project management",
        "2 years – Experience with Large language Models and building Gen AI applications",
        "Bachelor's Degree in Computer Science or Engineering",
        "Master's Degree in Computer Science or Engineering",
        "Required: None Required"
      ],
      "Benefits": [
        "Work in an entrepreneurial and dynamic environment with a chance to make an impact",
        "Develop lasting relationships with great people",
        "Have the opportunity to build a satisfying career",
        "We offer competitive compensation and benefits packages for our Team Members",
        "Typical Compensation Range",
        "Pay Rate Type: Salary",
        "$136,778.46 - $198,328.77 / Yearly",
        "Bonus Target: 10% Annual"
      ],
      "Responsibilities": [
        "This role leads a team of data engineers and analytics professionals, focusing on the design, implementation, and optimization of scalable, reliable, and secure data pipelines, especially for complex, high-volume sources such as IoT and sensor-based systems",
        "Working cross-functionally with operations, IT, and business stakeholders, the Advanced Analytics Manager ensures that data from diverse sources, including real-time telemetry, manufacturing systems, and traditional enterprise platforms are efficiently ingested, transformed, and made accessible for analytical consumption",
        "In addition, the role includes light but growing exposure to Generative AI use cases, such as document summarization, chat interfaces for data access, and large language model (LLM) integration—especially in scenarios that augment data accessibility and user experience across the organization",
        "A proven ability to lead high-performing teams and deliver impactful solutions in dynamic, data-rich environments is essential for success",
        "Lead and manage a team of data engineers and analytics professionals, providing strategic direction, mentorship, train, coach, performance reviews, and hands-on support to foster a collaborative, high-performing team environment focused on delivering impactful data solutions",
        "Establish clear objectives and key results (OKRs) for the team that align with the enterprise analytics and data strategy, ensuring close coordination with business goals and evolving priorities",
        "Conduct regular performance reviews, deliver constructive feedback, and champion continuous learning by identifying training, upskilling, and development opportunities for team members",
        "Partner with cross-functional teams, including business analysts, data scientists, IT, and key business stakeholders, to understand data requirements and deliver solutions that support business intelligence, operational reporting, and advanced analytics use cases",
        "Define and implement best practices in data engineering, covering data modeling, pipeline orchestration (ETL/ELT), data integration, and data quality, while ensuring reliable access to data from diverse sources such as databases, APIs, cloud platforms, and IoT systems",
        "Drive continuous improvement and innovation in data architecture and engineering techniques, with a focus on increasing scalability, performance, and reusability across the analytics ecosystem",
        "Oversee project planning and execution, balancing team capacity with priority management, delivery timelines, and quality standards to ensure successful outcomes for strategic and operational analytics initiatives",
        "Collaborate with cross-functional business and IT stakeholders to define and implement data and analytics strategies that support enterprise decision-making, ensuring data integrity, security, and governance across all analytical initiatives",
        "Provide technical leadership in data engineering and analytics infrastructure, leveraging modern cloud-native tools to enable high-performance data platforms that support BI, reporting, and advanced analytics",
        "Champion a culture of data innovation and continuous learning, encouraging the team to explore new data patterns, tooling, and architectural practices that improve scalability, reliability, and time-to-insight",
        "Oversee resource planning, vendor coordination, and budget management for the analytics engineering function, ensuring alignment with strategic priorities and operational efficiency",
        "Lead the design and implementation of high-volume, scalable data pipelines, models, and data marts across Data Lakes and Data Warehouses to support reporting, dashboarding, and analytics workloads",
        "Translate technical and project outcomes into business context through executive-ready presentations and analytics steering committee briefings, facilitating alignment and decision-making",
        "Build and manage DevOps pipelines for CI/CD and analytics solution deployment, ensuring consistency, automation, and compliance with change control standards",
        "Serve as a proactive leader who delivers on commitments and identifies opportunities for process improvement, team development, and organizational data literacy",
        "Coordinate and collaborate with onsite and offshore consultants to support scalable delivery and knowledge transfer across global analytics initiatives",
        "Maintain flexibility and responsiveness to evolving business needs; this job description may be adjusted to reflect emerging priorities or strategic direction",
        "Duties, responsibilities and activities may change at any time with or without prior notice",
        "Additionally, the Advanced Analytics Manager is expected to demonstrate:",
        "Excellent communication, leadership and collaboration skills",
        "Adaptability - adapts to changes in the work environment; able to deal with frequent change, delays, or unexpected events",
        "Planning/Organizing - uses time efficiently; plans for additional resources; sets goals and objectives; organizes or schedules other people and their tasks",
        "Project Management - coordinates projects; communicates changes and progress; completes projects on time; budget management; manages project team activities",
        "Oral Communication - speaks clearly and persuasively in positive or negative situations; listens and gets clarification; responds well to questions"
      ]
    },
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "careers-niagarawater-com-us-en-job-r45735-data-engineering-manager-advanced-analytics",
    "_source": "new_jobs"
  },
  {
    "job_id": "4GHeUqb67eilko0QAAAAAA==",
    "job_title": "Simulation Engineer- Supply Chain",
    "employer_name": "Cencora",
    "employer_logo": null,
    "employer_website": "https://www.cencora.com",
    "job_publisher": "Cencora Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.cencora.com/us/en/job/R2523242/Simulation-Engineer-Supply-Chain?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Cencora Careers",
        "apply_link": "https://careers.cencora.com/us/en/job/R2523242/Simulation-Engineer-Supply-Chain?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/cCMwXHk6HcfhsFhWMSOkJvsMbRYzJa1VoQehAqbdWDeno7Y_Oz0JNQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/8a691ec6b6682787516e82bdcee83da8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Bandana.com",
        "apply_link": "https://bandana.com/jobs/5a9763e9-b513-41d2-8e16-b08333bbbf2a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\n\nJob Details\n\nWe are seeking a talented and experienced Supply Chain Simulation Engineer to lead and collaborate on the design, development, and deployment of tools and strategies that will enable more effective data-driven decision making.\n\nThis individual will provide guidance through advanced analytics modeling and optimizing facilities within the Supply Chain.\n\nThe Simulation Engineer should have a firm understanding of distribution operational and automation processes.\n\nThe ideal candidate will be a skilled communicator with experience telling compelling stories through data-driven insights.\n\nRoles and Responsibilities:\n• Create simulation models of our distribution centers that will help determine efficiency opportunities.\n• Understand and have experience with design and / or implementation of automation solutions such as conveyors, sorters, high speed picking and packaging lines, robotic work cells.\n• Work closely with other teams, such as Engineering and Operations, in the validation of the models and in the design of potential scenarios that will improve the total outcome.\n• Collaborate in developing a cost to serve model.\n• Implement and support the new inhouse simulation capabilities – process, tools, training.\n• Collaborate in the creation of business cases to close gaps and define capex requirements.\n• Provide advice and insights on improving existing simulation capabilities.\n• Design interactive business intelligence dashboards to share input and outputs.\n• Develop and train team to utilize self-service modeling functionality.\n• Stay up to date with modeling and simulations trends.\n• Perform other duties as assigned.\n\nEducation:\n• A Bachelors degree in Industrial Engineering, Operations Research, Quantitative Methods, Data Science or related field or equivalent professional qualification/workplace skills.\n• Master’s degree preferred.\n\nExperience:\n• Requires five (5) to seven (7) years of directly related and progressively responsible experience.\n• Hands-on experience using simulation and analytical tools to solve operational problems.\n• Practical experience with Simulation frameworks (discrete-event, agent-based) and technologies such as AnyLogic/Arena/EnterpriseDynamics/FlexSim/etc\n• Project Management / Agile hands-on experience.\n• Experience working in an Operations environment.\n• Experience with business intelligence tools (PowerBI, Tableau, Qlik).\n• Experience pulling and cleaning data from Cloud Platforms (Google, Azure), or Supply Chain Tools (including ERP, WMS, TMS, Inventory & Forecasting Tools).\n• Experience programming with data analytic tools such as SQL, Python, DataBricks, etc.\n• Experience transforming CAD data into simulation tools.\n• Experience building business cases and cost/benefit analysis to support strategic decision making.\n\nSkills and Abilities:\n• Strong analytical and critical thinking skills, with the ability to identify and resolve complex issues quickly and innovative. Experience leveraging analytic based approaches to problem solving, such as Statistical Process Control, Design of Experiments, Six Sigma, or other statistical analysis frameworks.\n• Team oriented and collaborative working style.\n• Growth mindset, positive attitude and strong interest in solving business challenges and adapting to a changing work environment.\n• Ability to communicate effectively both orally and in writing; ability to communicate (and work) effectively with people from different technical and business backgrounds, acting as a liaison, understanding, and appreciating different perspectives and translating into terms necessary for any group or individual to understand.\n• Presentation skills: ability to present and discuss technical information in a manner that establishes rapport, persuades others, and establishes understanding for technical and non-technical audiences.\n• Strong organizational skills; attention to detail.\n• Must be able to interact well with others both directly (face-to-face) and remotely within a multi-discipline shared work environment.\n• Ability to influence through the use of data and insights while showing tactful discretion with difficult/sensitive information.\n• Travel required per project, estimated at 2 weeks per quarter.\n• Ability to manage multiple projects\n\nWhat Cencora offers\n\nWe provide compensation, benefits, and resources that enable a highly inclusive culture and support our team members’ ability to live with purpose every day. In addition to traditional offerings like medical, dental, and vision care, we also provide a comprehensive suite of benefits that focus on the physical, emotional, financial, and social aspects of wellness. This encompasses support for working families, which may include backup dependent care, adoption assistance, infertility coverage, family building support, behavioral health solutions, paid parental leave, and paid caregiver leave. To encourage your personal growth, we also offer a variety of training programs, professional development resources, and opportunities to participate in mentorship programs, employee resource groups, volunteer activities, and much more. For details, visit https://www.virtualfairhub.com/cencora\n\nFull time\n\nSalary Range*\n\n$88,700 - 126,940\n• This Salary Range reflects a National Average for this job. The actual range may vary based on your locale. Ranges in Colorado/California/Washington/New York/Hawaii/Vermont/Minnesota/Massachusetts/Illinois State-specific locations may be up to 10% lower than the minimum salary range, and 12% higher than the maximum salary range.\n\nEqual Employment Opportunity\n\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\n\nThe company’s continued success depends on the full and effective utilization of qualified individuals. Therefore, harassment is prohibited and all matters related to recruiting, training, compensation, benefits, promotions and transfers comply with equal opportunity principles and are non-discriminatory.\n\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com. We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned\n\n.\nAffiliated Companies:\nAffiliated Companies: AmerisourceBergen Drug Corporation",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "W CNSHOHOCKEN, PA",
    "job_city": "Conshohocken",
    "job_state": "Pennsylvania",
    "job_country": "US",
    "job_latitude": 40.0792766,
    "job_longitude": -75.3015714,
    "job_benefits": [
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4GHeUqb67eilko0QAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will be a skilled communicator with experience telling compelling stories through data-driven insights",
        "A Bachelors degree in Industrial Engineering, Operations Research, Quantitative Methods, Data Science or related field or equivalent professional qualification/workplace skills",
        "Requires five (5) to seven (7) years of directly related and progressively responsible experience",
        "Hands-on experience using simulation and analytical tools to solve operational problems",
        "Practical experience with Simulation frameworks (discrete-event, agent-based) and technologies such as AnyLogic/Arena/EnterpriseDynamics/FlexSim/etc",
        "Project Management / Agile hands-on experience",
        "Experience working in an Operations environment",
        "Experience with business intelligence tools (PowerBI, Tableau, Qlik)",
        "Experience pulling and cleaning data from Cloud Platforms (Google, Azure), or Supply Chain Tools (including ERP, WMS, TMS, Inventory & Forecasting Tools)",
        "Experience programming with data analytic tools such as SQL, Python, DataBricks, etc",
        "Experience transforming CAD data into simulation tools",
        "Experience building business cases and cost/benefit analysis to support strategic decision making",
        "Strong analytical and critical thinking skills, with the ability to identify and resolve complex issues quickly and innovative",
        "Experience leveraging analytic based approaches to problem solving, such as Statistical Process Control, Design of Experiments, Six Sigma, or other statistical analysis frameworks",
        "Growth mindset, positive attitude and strong interest in solving business challenges and adapting to a changing work environment",
        "Ability to communicate effectively both orally and in writing; ability to communicate (and work) effectively with people from different technical and business backgrounds, acting as a liaison, understanding, and appreciating different perspectives and translating into terms necessary for any group or individual to understand",
        "Presentation skills: ability to present and discuss technical information in a manner that establishes rapport, persuades others, and establishes understanding for technical and non-technical audiences",
        "Strong organizational skills; attention to detail",
        "Must be able to interact well with others both directly (face-to-face) and remotely within a multi-discipline shared work environment"
      ],
      "Benefits": [
        "We provide compensation, benefits, and resources that enable a highly inclusive culture and support our team members’ ability to live with purpose every day",
        "In addition to traditional offerings like medical, dental, and vision care, we also provide a comprehensive suite of benefits that focus on the physical, emotional, financial, and social aspects of wellness",
        "This encompasses support for working families, which may include backup dependent care, adoption assistance, infertility coverage, family building support, behavioral health solutions, paid parental leave, and paid caregiver leave",
        "To encourage your personal growth, we also offer a variety of training programs, professional development resources, and opportunities to participate in mentorship programs, employee resource groups, volunteer activities, and much more",
        "Salary Range*",
        "$88,700 - 126,940",
        "This Salary Range reflects a National Average for this job"
      ],
      "Responsibilities": [
        "This individual will provide guidance through advanced analytics modeling and optimizing facilities within the Supply Chain",
        "The Simulation Engineer should have a firm understanding of distribution operational and automation processes",
        "Create simulation models of our distribution centers that will help determine efficiency opportunities",
        "Understand and have experience with design and / or implementation of automation solutions such as conveyors, sorters, high speed picking and packaging lines, robotic work cells",
        "Work closely with other teams, such as Engineering and Operations, in the validation of the models and in the design of potential scenarios that will improve the total outcome",
        "Collaborate in developing a cost to serve model",
        "Implement and support the new inhouse simulation capabilities – process, tools, training",
        "Collaborate in the creation of business cases to close gaps and define capex requirements",
        "Provide advice and insights on improving existing simulation capabilities",
        "Design interactive business intelligence dashboards to share input and outputs",
        "Develop and train team to utilize self-service modeling functionality",
        "Stay up to date with modeling and simulations trends",
        "Perform other duties as assigned",
        "Team oriented and collaborative working style",
        "Ability to influence through the use of data and insights while showing tactful discretion with difficult/sensitive information",
        "Travel required per project, estimated at 2 weeks per quarter",
        "Ability to manage multiple projects"
      ]
    },
    "job_onet_soc": "13108100",
    "job_onet_job_zone": "4",
    "id": "careers-cencora-com-us-en-job-r2523242-simulation-engineer-supply-chain",
    "_source": "new_jobs"
  },
  {
    "job_id": "5FJGrJp-KLWXkkHBAAAAAA==",
    "job_title": "Staff Analytics Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=0df90abea72f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=0df90abea72f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-ny-syracuse-staff-analytics-engineer-nitrogen-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Staff Analytics Engineer to transform raw data into trusted business insights.\n\nKey Responsibilities\n\nArchitect the complete data lifecycle and own the gold layer, designing the semantic layer and optimizing data tables\n\nTranslate complex business questions into technical definitions and build consensus on enterprise-wide metric definitions\n\nChampion a product-oriented mindset by establishing standards for documentation, testing, and data model design\n\nRequired Qualifications\n\n10+ years of experience in Analytics Engineering, Software Engineering, or Data Engineering, with 4 years in large-scale data environments\n\nExpertise in dimensional modeling best practices and designing reusable data models\n\nProficiency in advanced SQL techniques and performance tuning\n\nStrong expertise in Python for scripting and data manipulation\n\nBachelor's degree in Computer Science, Software Engineering, Data Engineering, or a related field",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Syracuse, NY",
    "job_city": "Syracuse",
    "job_state": "New York",
    "job_country": "US",
    "job_latitude": 43.0494832,
    "job_longitude": -76.1473977,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D5FJGrJp-KLWXkkHBAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "10+ years of experience in Analytics Engineering, Software Engineering, or Data Engineering, with 4 years in large-scale data environments",
        "Expertise in dimensional modeling best practices and designing reusable data models",
        "Proficiency in advanced SQL techniques and performance tuning",
        "Strong expertise in Python for scripting and data manipulation",
        "Bachelor's degree in Computer Science, Software Engineering, Data Engineering, or a related field"
      ],
      "Responsibilities": [
        "Architect the complete data lifecycle and own the gold layer, designing the semantic layer and optimizing data tables",
        "Translate complex business questions into technical definitions and build consensus on enterprise-wide metric definitions",
        "Champion a product-oriented mindset by establishing standards for documentation, testing, and data model design"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "ObQrQ-xXvc7iMgweAAAAAA==",
    "job_title": "Staff Data Engineer – Cloud Data Platform",
    "employer_name": "01 Calix North America",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Workday",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://calix.wd1.myworkdayjobs.com/en-US/External/job/Staff-Data-Engineer---Cloud-Data-Platform_R-10714?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Workday",
        "apply_link": "https://calix.wd1.myworkdayjobs.com/en-US/External/job/Staff-Data-Engineer---Cloud-Data-Platform_R-10714?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Please note that all emails from Calix will come from a @calix.com email address. If you receive a communication that you think may not be from Calix, please report it to us at talentandculture@calix.com. This is a remote position that could be based anywhere in the United States. Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks. As part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, transformation and analytics. Responsibilities and Duties: Work closely with Cloud product owners to understand, analyze product requirements and provide feedback. Develop conceptual, logical, physical models and meta data solutions. Design and manage an array of data design deliverables including data models, data diagrams, data flows and corresponding data dictionary documentations. Determine database structural requirements by analyzing client operations, applications, and data from existing systems. Technical leadership of software design in meeting requirements of service stability, reliability, scalability, and security Guiding technical discussions within engineer group and making technical recommendations. Design review and code review with peer engineers Guiding testing architecture for large scale data ingestion and transformations. Customer facing engineering role in debugging and resolving field issues. Qualifications: This role may be required to travel and attend face-to-face meetings and Calix sponsored events. 10+ years of development experience performing Data modeling, master data management and building ETL/data pipeline implementations. Cloud Platforms: Proficiency in both Google Cloud Platform (GCP) services (BigQuery, Dataflow, Dataproc, PubSub/Kafka, Cloud Storage) and AWS Big Data Technologies: Knowledge of big data processing frameworks such as Apache Spark ,Flink . Programming Languages: Strong knowledge of SQL and at least one programming language (Python, Java, or Scala),DBT. Data Visualization: Experience with BI tools such as Google Data Studio, Looker, ThoughtSpot, and using BigQuery BI Engine for optimized reporting Problem Solving: Strong analytical and troubleshooting skills, particularly in complex data scenarios. Collaboration: Ability to work effectively in a team environment and engage with cross-functional teams. Communication: Proficient in conveying complex technical concepts to stakeholders. Knowledge of data governance, security best practices, and compliance regulations in both GCP and AWS environments. Bachelor’s degree in Computer Science, Information Technology or a related field. Location: Remote-based position located in the United States. #LI-Remote The base pay range for this position varies based on the geographic location. More information about the pay range specific to candidate location and other factors will be shared during the recruitment process. Individual pay is determined based on location of residence and multiple factors, including job-related knowledge, skills and experience. San Francisco Bay Area: 156,400 - 265,700 USD Annual All Other US Locations: 136,000 - 231,000 USD Annual As a part of the total compensation package, this role may be eligible for a bonus. For information on our benefits click here. PLEASE NOTE: All emails from Calix will come from a '@calix.com' email address. Please verify and confirm any communication from Calix prior to disclosing any personal or financial information. If you receive a communication that you think may not be from Calix, please report it to us at talentandculture@calix.com. Calix delivers a broadband platform and managed services that enable our customers to improve life one community at a time. We’re at the forefront of a once in a generational change in the broadband industry. Join us as we innovate, help our customers reach their potential, and connect underserved communities with unrivaled digital experiences. This is the Calix mission - to enable BSPs of all sizes to Simplify. Innovate. Grow. To learn more, visit the Calix web site at www.calix.com To learn more about our international job opportunities, please visit our International Careers Page If you are a person with a disability needing assistance with the application process please: Email us at calix.interview@calix.com; or Call us at +1 (408) 514-3000. Calix is a Drug Free Workplace. You may access a copy of Calix Candidate Privacy Policy HERE and other Calix Privacy Policies HERE.",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Remote, OR",
    "job_city": "Remote",
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 43.005945499999996,
    "job_longitude": -123.8925908,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DObQrQ-xXvc7iMgweAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "10+ years of development experience performing Data modeling, master data management and building ETL/data pipeline implementations",
        "Cloud Platforms: Proficiency in both Google Cloud Platform (GCP) services (BigQuery, Dataflow, Dataproc, PubSub/Kafka, Cloud Storage) and AWS Big Data Technologies: Knowledge of big data processing frameworks such as Apache Spark ,Flink ",
        "Programming Languages: Strong knowledge of SQL and at least one programming language (Python, Java, or Scala),DBT",
        "Data Visualization: Experience with BI tools such as Google Data Studio, Looker, ThoughtSpot, and using BigQuery BI Engine for optimized reporting Problem Solving: Strong analytical and troubleshooting skills, particularly in complex data scenarios",
        "Collaboration: Ability to work effectively in a team environment and engage with cross-functional teams",
        "Communication: Proficient in conveying complex technical concepts to stakeholders",
        "Knowledge of data governance, security best practices, and compliance regulations in both GCP and AWS environments",
        "Bachelor’s degree in Computer Science, Information Technology or a related field"
      ],
      "Benefits": [
        "#LI-Remote The base pay range for this position varies based on the geographic location",
        "More information about the pay range specific to candidate location and other factors will be shared during the recruitment process",
        "Individual pay is determined based on location of residence and multiple factors, including job-related knowledge, skills and experience",
        "San Francisco Bay Area: 156,400 - 265,700 USD Annual All Other US Locations: 136,000 - 231,000 USD Annual As a part of the total compensation package, this role may be eligible for a bonus"
      ],
      "Responsibilities": [
        "As part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, transformation and analytics",
        "Responsibilities and Duties: Work closely with Cloud product owners to understand, analyze product requirements and provide feedback",
        "Develop conceptual, logical, physical models and meta data solutions",
        "Design and manage an array of data design deliverables including data models, data diagrams, data flows and corresponding data dictionary documentations",
        "Determine database structural requirements by analyzing client operations, applications, and data from existing systems",
        "Technical leadership of software design in meeting requirements of service stability, reliability, scalability, and security Guiding technical discussions within engineer group and making technical recommendations",
        "Design review and code review with peer engineers Guiding testing architecture for large scale data ingestion and transformations",
        "Customer facing engineering role in debugging and resolving field issues",
        "Qualifications: This role may be required to travel and attend face-to-face meetings and Calix sponsored events"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "calix-wd1-myworkdayjobs-com-en-us-external-job-staff-data-engineer-cloud-data-platform_r-10714",
    "_source": "new_jobs"
  },
  {
    "job_id": "QMFI59Aeon5OiMxQAAAAAA==",
    "job_title": "Data Solutions Engineer",
    "employer_name": "Early Career",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Citi Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.citi.com/job/irving/data-solutions-engineer/287/91594717280?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Citi Careers",
        "apply_link": "https://jobs.citi.com/job/irving/data-solutions-engineer/287/91594717280?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=12272bb5413dda94&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/data-solutions-engineer-citi-JV_IC1140006_KO0,23_KE24,28.htm?jl=1010030029481&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/-ZBW4eiGrDb3Dr5czTYeI2QaJErXFTVKY7CiC8HcO-gC_AN4pjITwg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-solutions-engineer-at-citi-4370775693?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Experteer",
        "apply_link": "https://us.experteer.com/career/view-jobs/data-solutions-engineer-irving-tx-usa-56103968?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/18342d1b3a5230bb8ebc6b74cb10e255?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This Data Solutions Engineer (Applications Development Senior Programmer Analyst - C12) is responsible for building next-generation Data Engineering solutions. This intermediate-level position involves active participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team. A key aspect of this role is liaising between business users and technologists to facilitate the exchange of information regarding solutions, including requirements and usage.\n\nResponsibilities:\n• Serve as an integral team member of our Data Engineering team, responsible for the design and development of Big Data solutions.\n• Partner with domain experts, product managers, analysts, and data scientists to develop robust Big Data pipelines in Hadoop or Snowflake environments.\n• Responsible for delivering a data-as-a-service framework.\n• Responsible for moving all legacy workloads to cloud platform.\n• Lead the migration of all legacy workloads to cloud platforms.\n• Engage with key stakeholders to elicit and document requirements, including detailed data flow specifications.\n• Assess appropriate solutions and collaborate with relevant teams to drive optimal implementations.\n• Work with data scientists to build client pipelines using heterogeneous sources and provide essential engineering services for data science applications.\n• Research and evaluate open-source technologies and components, recommending and integrating them into design and implementation efforts.\n• Act as a technical expert, mentoring other team members on Big Data and Cloud technology stacks.\n• Define comprehensive requirements for maintainability, testability, performance, security, quality, and usability across the data platform.\n• Drive the implementation of consistent patterns, reusable components, and coding standards for all data engineering processes.\n• Convert SAS-based pipelines into modern languages like PySpark and Scala for execution on Hadoop and non-Hadoop ecosystems.\n• Optimize Big Data applications on both Hadoop and non-Hadoop platforms for peak performance.\n• Evaluate new IT developments and evolving business requirements, recommending appropriate system alternatives and/or enhancements to current systems through analysis of business processes, systems, and industry standards.\n• Appropriately assess risk when making business decisions, demonstrating consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets. This includes driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing, and reporting control issues with transparency.\n\nRequired Qualifications:\n• 5+ years of experience with Hadoop and Big Data technologies\n• Demonstrated proficiency in Python, PySpark, and Scala, including practical experience with fundamental machine learning libraries\n• Experience in developing robust data solutions leveraging Google Cloud or AWS platforms; relevant certifications are preferred\n• Experience with SAS\n• Experience with containerization and related technologies (e.g., Docker, Kubernetes)\n• Comprehensive understanding of software engineering and data analytics\n• In-depth knowledge and hands-on experience with the Hadoop ecosystem and Big Data technologies (e.g., HDFS, MapReduce, Hive, Pig, Impala, Kafka, Kudu, Solr)\n• Knowledge of Agile (Scrum) development methodologies.\n• Strong development and automation skills.\n• System-level understanding of data structures, algorithms, distributed storage, and compute.\n• A proactive approach to solving complex business problems, complemented by strong interpersonal and teamwork skills.\n\nDesired Qualifications\n• Familiarity with Hadoop administration and Snowflake.\n• Proficiency in Java or additional experience with Apache Beam.\n\nEducation:\n• Bachelor’s degree/University degree or equivalent experience\n\nApplicants must be authorized to work in the U.S for this position; Citi will not sponsor applicants for U.S. work authorization for this role. Candidate must be located within commuting distance or be willing to relocate to the area.\n\nThis job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.\n\n------------------------------------------------------\n\nJob Family Group:\nTechnology\n\n------------------------------------------------------\n\nJob Family:\nApplications Development\n\n------------------------------------------------------\n\nTime Type:\nFull time\n\n------------------------------------------------------\n\nPrimary Location:\nIrving Texas United States\n\n------------------------------------------------------\n\nPrimary Location Full Time Salary Range:\n$107,120.00 - $160,680.00\n\nIn addition to salary, Citi’s offerings may also include, for eligible employees, discretionary and formulaic incentive and retention awards. Citi offers competitive employee benefits, including: medical, dental & vision coverage; 401(k); life, accident, and disability insurance; and wellness programs. Citi also offers paid time off packages, including planned time off (vacation), unplanned time off (sick leave), and paid holidays. For additional information regarding Citi employee benefits, please visit citibenefits.com. Available offerings may vary by jurisdiction, job level, and date of hire.\n\n------------------------------------------------------\n\nMost Relevant Skills\nPlease see the requirements listed above.\n\n------------------------------------------------------\n\nOther Relevant Skills\nFor complementary skills, please see above and/or contact the recruiter.\n\n------------------------------------------------------\n\nAnticipated Posting Close Date:\nFeb 24, 2026\n\n------------------------------------------------------\n\nCiti is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law.\n\nIf you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.\n\nView Citi’s EEO Policy Statement and the Know Your Rights poster.",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Jacksonville, FL",
    "job_city": "Jacksonville",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 30.3297566,
    "job_longitude": -81.6591529,
    "job_benefits": [
      "health_insurance",
      "paid_time_off",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DQMFI59Aeon5OiMxQAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience with Hadoop and Big Data technologies",
        "Demonstrated proficiency in Python, PySpark, and Scala, including practical experience with fundamental machine learning libraries",
        "Experience with SAS",
        "Experience with containerization and related technologies (e.g., Docker, Kubernetes)",
        "Comprehensive understanding of software engineering and data analytics",
        "In-depth knowledge and hands-on experience with the Hadoop ecosystem and Big Data technologies (e.g., HDFS, MapReduce, Hive, Pig, Impala, Kafka, Kudu, Solr)",
        "Knowledge of Agile (Scrum) development methodologies",
        "Strong development and automation skills",
        "System-level understanding of data structures, algorithms, distributed storage, and compute",
        "A proactive approach to solving complex business problems, complemented by strong interpersonal and teamwork skills",
        "Bachelor’s degree/University degree or equivalent experience",
        "Applicants must be authorized to work in the U.S for this position; Citi will not sponsor applicants for U.S. work authorization for this role",
        "Candidate must be located within commuting distance or be willing to relocate to the area",
        "Please see the requirements listed above",
        "For complementary skills, please see above and/or contact the recruiter"
      ],
      "Benefits": [
        "$107,120.00 - $160,680.00",
        "In addition to salary, Citi’s offerings may also include, for eligible employees, discretionary and formulaic incentive and retention awards",
        "Citi offers competitive employee benefits, including: medical, dental & vision coverage; 401(k); life, accident, and disability insurance; and wellness programs",
        "Citi also offers paid time off packages, including planned time off (vacation), unplanned time off (sick leave), and paid holidays"
      ],
      "Responsibilities": [
        "This intermediate-level position involves active participation in the establishment and implementation of new or revised application systems and programs in coordination with the Technology team",
        "A key aspect of this role is liaising between business users and technologists to facilitate the exchange of information regarding solutions, including requirements and usage",
        "Serve as an integral team member of our Data Engineering team, responsible for the design and development of Big Data solutions",
        "Partner with domain experts, product managers, analysts, and data scientists to develop robust Big Data pipelines in Hadoop or Snowflake environments",
        "Responsible for delivering a data-as-a-service framework",
        "Responsible for moving all legacy workloads to cloud platform",
        "Lead the migration of all legacy workloads to cloud platforms",
        "Engage with key stakeholders to elicit and document requirements, including detailed data flow specifications",
        "Assess appropriate solutions and collaborate with relevant teams to drive optimal implementations",
        "Work with data scientists to build client pipelines using heterogeneous sources and provide essential engineering services for data science applications",
        "Research and evaluate open-source technologies and components, recommending and integrating them into design and implementation efforts",
        "Act as a technical expert, mentoring other team members on Big Data and Cloud technology stacks",
        "Define comprehensive requirements for maintainability, testability, performance, security, quality, and usability across the data platform",
        "Drive the implementation of consistent patterns, reusable components, and coding standards for all data engineering processes",
        "Convert SAS-based pipelines into modern languages like PySpark and Scala for execution on Hadoop and non-Hadoop ecosystems",
        "Optimize Big Data applications on both Hadoop and non-Hadoop platforms for peak performance",
        "Evaluate new IT developments and evolving business requirements, recommending appropriate system alternatives and/or enhancements to current systems through analysis of business processes, systems, and industry standards",
        "Appropriately assess risk when making business decisions, demonstrating consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets",
        "This includes driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing, and reporting control issues with transparency",
        "This job description provides a high-level review of the types of work performed"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-citi-com-job-irving-data-solutions-engineer-287-91594717280",
    "_source": "new_jobs"
  },
  {
    "job_id": "uuiihkAJSRtJByehAAAAAA==",
    "job_title": "Senior Data Analytics Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=04813c62bcb3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=04813c62bcb3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Senior Data Analytics Engineer to join their Data Solutions team.\n\nKey Responsibilities\n\nLead data analytics projects to build innovative solutions while ensuring adherence to budget, schedule, and scope\n\nMentor members in the Analytics Center of Excellence and assist with data catalog and visualization software oversight\n\nDrive data and analytics solutions from conception to deployment with clear ROI impact\n\nRequired Qualifications\n\nBachelor's degree in computer or information science preferred or relevant experience\n\n5+ years of relevant experience in a data-driven professional setting\n\nStrong command of databases and SQL, with proficiency in Python or R for data manipulation and analysis\n\nThorough understanding of project management principles and information technology practices\n\nStrong data analytics background, including experience with SQL, Snowflake Database, Azure cloud, and data visualization tools",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Southport, IN",
    "job_city": "Southport",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 39.6618814,
    "job_longitude": -86.1166506,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DuuiihkAJSRtJByehAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of relevant experience in a data-driven professional setting",
        "Strong command of databases and SQL, with proficiency in Python or R for data manipulation and analysis",
        "Thorough understanding of project management principles and information technology practices",
        "Strong data analytics background, including experience with SQL, Snowflake Database, Azure cloud, and data visualization tools"
      ],
      "Responsibilities": [
        "Lead data analytics projects to build innovative solutions while ensuring adherence to budget, schedule, and scope",
        "Mentor members in the Analytics Center of Excellence and assist with data catalog and visualization software oversight",
        "Drive data and analytics solutions from conception to deployment with clear ROI impact"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "8JZA4-i-u69bI9q_AAAAAA==",
    "job_title": "Senior Analytics Developer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=e89889122b9c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=e89889122b9c&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Senior Analytics Developer, Brokerage & Ledger Product Data Science.\n\nKey Responsibilities\n\nDesign, build, and maintain high-quality analytical data models for brokerage and ledger workflows\n\nOwn critical datasets that support client-facing metrics, operational decision-making, and regulatory reporting\n\nCollaborate with cross-functional teams to translate stakeholder needs into durable data solutions\n\nRequired Qualifications\n\nExperience working with large, complex analytical datasets in a production environment\n\nStrong SQL skills and experience building analytical models or data marts\n\nProduct-oriented mindset focused on data usability and long-term ownership\n\nFamiliarity with modern analytics tooling such as dbt, Airflow, Python, and cloud data warehouses\n\nExperience in financial services, trading, or accounting is a plus",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Davie, FL",
    "job_city": "Davie",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 26.076478299999998,
    "job_longitude": -80.25211569999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D8JZA4-i-u69bI9q_AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Experience working with large, complex analytical datasets in a production environment",
        "Strong SQL skills and experience building analytical models or data marts",
        "Product-oriented mindset focused on data usability and long-term ownership",
        "Familiarity with modern analytics tooling such as dbt, Airflow, Python, and cloud data warehouses"
      ],
      "Responsibilities": [
        "Design, build, and maintain high-quality analytical data models for brokerage and ledger workflows",
        "Own critical datasets that support client-facing metrics, operational decision-making, and regulatory reporting",
        "Collaborate with cross-functional teams to translate stakeholder needs into durable data solutions"
      ]
    },
    "job_onet_soc": "43911100",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "oiOUmKiJy99QiKAiAAAAAA==",
    "job_title": "Healthcare Data Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=3c476bccca43&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=3c476bccca43&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Healthcare Data Engineer.\n\nKey Responsibilities\n\nSupport the development and maintenance of secure, scalable data systems for analytics in a HIPAA regulated environment\n\nContribute to building and operating data pipelines from various healthcare sources\n\nAssist in documenting data processes and participate in data quality efforts\n\nRequired Qualifications, Training, and Education\n\nBachelor's degree in Computer Science, Software Engineering, Data Science, Statistics, or a related field\n\n3+ years of experience in a related technical role or relevant internships\n\nFamiliarity with at least one cloud platform, preferably Azure\n\nUnderstanding of data architecture principles and experience with data pipelines\n\nSolid foundational skills in Python and SQL",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1771113600,
    "job_posted_at_datetime_utc": "2026-02-15T00:00:00.000Z",
    "job_location": "Spring, TX",
    "job_city": "Spring",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 30.0799405,
    "job_longitude": -95.41716009999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DoiOUmKiJy99QiKAiAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree in Computer Science, Software Engineering, Data Science, Statistics, or a related field",
        "3+ years of experience in a related technical role or relevant internships",
        "Familiarity with at least one cloud platform, preferably Azure",
        "Understanding of data architecture principles and experience with data pipelines",
        "Solid foundational skills in Python and SQL"
      ],
      "Responsibilities": [
        "Support the development and maintenance of secure, scalable data systems for analytics in a HIPAA regulated environment",
        "Contribute to building and operating data pipelines from various healthcare sources",
        "Assist in documenting data processes and participate in data quality efforts"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "0ihmnDaGYARMqeWnAAAAAA==",
    "job_title": "Data Integration Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=dcaee1d2df8d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=dcaee1d2df8d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Structured Data Integration & Testing Engineer to enhance its member-facing AI assistant through reliable data integration and testing.\n\nKey Responsibilities\n\nLead the development and implementation of AI solutions for healthcare payer use cases\n\nArchitect and deploy scalable AI models using Python and cloud platforms\n\nCollaborate with product teams to identify and prioritize AI opportunities that address business challenges\n\nRequired Qualifications\n\nBachelor's degree in STEM or equivalent work experience required; Master's or PhD preferred\n\n5+ years of experience in analytics, technology, or software engineering, with 3+ years in machine learning\n\nProven experience with large datasets and data science pipelines\n\nDeep knowledge of machine learning frameworks and natural language processing\n\nExperience deploying ML models in cloud environments, preferably GCP Vertex AI",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "New Orleans, LA",
    "job_city": "New Orleans",
    "job_state": "Louisiana",
    "job_country": "US",
    "job_latitude": 29.9508941,
    "job_longitude": -90.07583559999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D0ihmnDaGYARMqeWnAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience in analytics, technology, or software engineering, with 3+ years in machine learning",
        "Proven experience with large datasets and data science pipelines",
        "Deep knowledge of machine learning frameworks and natural language processing",
        "Experience deploying ML models in cloud environments, preferably GCP Vertex AI"
      ],
      "Responsibilities": [
        "Lead the development and implementation of AI solutions for healthcare payer use cases",
        "Architect and deploy scalable AI models using Python and cloud platforms",
        "Collaborate with product teams to identify and prioritize AI opportunities that address business challenges"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "HxMHJpglCk9QoRUvAAAAAA==",
    "job_title": "Healthcare Data Engineer",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=011c6104be2f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=011c6104be2f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Healthcare Data Engineer.\n\nKey Responsibilities\n\nSupport the development and maintenance of secure, scalable data systems for analytics in a HIPAA regulated environment\n\nContribute to building and operating data pipelines from various healthcare sources\n\nAssist in documenting data processes and participate in data quality efforts\n\nRequired Qualifications, Training, and Education\n\nBachelor's degree in Computer Science, Software Engineering, Data Science, Statistics, or a related field\n\n3+ years of experience in a related technical role or relevant internships\n\nFamiliarity with at least one cloud platform, preferably Azure\n\nUnderstanding of data architecture principles and experience with data pipelines\n\nSolid foundational skills in Python and SQL",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Pensacola, FL",
    "job_city": "Pensacola",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 30.407664699999998,
    "job_longitude": -87.2189746,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHxMHJpglCk9QoRUvAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor's degree in Computer Science, Software Engineering, Data Science, Statistics, or a related field",
        "3+ years of experience in a related technical role or relevant internships",
        "Familiarity with at least one cloud platform, preferably Azure",
        "Understanding of data architecture principles and experience with data pipelines",
        "Solid foundational skills in Python and SQL"
      ],
      "Responsibilities": [
        "Support the development and maintenance of secure, scalable data systems for analytics in a HIPAA regulated environment",
        "Contribute to building and operating data pipelines from various healthcare sources",
        "Assist in documenting data processes and participate in data quality efforts"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  }
]