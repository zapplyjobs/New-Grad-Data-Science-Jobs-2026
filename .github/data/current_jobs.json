[
  {
    "job_id": "y9Ez-YEkLqpd9staAAAAAA==",
    "job_title": "DataEngineer",
    "employer_name": "Jobs via Dice",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR9CC1NlnLA7sshF1s1dqKvk8U495jsMwImnyPP&s=0",
    "employer_website": null,
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/dataengineer-at-jobs-via-dice-4372009753?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/dataengineer-at-jobs-via-dice-4372009753?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Dice",
        "apply_link": "https://www.dice.com/job-detail/8450f66f-d9c9-4a3f-9a3e-7e7ddf43420c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/185ad29d20a173269bde7c12ca5be9f0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/amp/job/us-texas-plano-dataengineer-jobs-via-dice-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Dice is the leading career destination for tech experts at every stage of their careers. Our client, Key Business Solutions, Inc., is seeking the following. Apply via Dice today!\n\nJob Description: Data Engineer\n\nPlano, TX - 4 Days Hybrid\n\n12+ Months\n\nDescription:\n\nCollecting data from internal and external sources, ensuring accuracy, and removing duplicates or errors.\n\nApplying statistical and analytical techniques to identify trends, correlations, and anomalies.\n\nCreating dashboards, charts, and reports using tools like Excel, SQL, Python, and Tableau to communicate findings to stakeholders.\n\nTranslating complex datasets into actionable recommendations for business strategy, marketing, or operations.\n\nWorking with cross-functional teams, including project managers, business analysts, and marketing, to understand data needs and develop solutions.\n\nEnsuring compliance with data management policies, maintaining data integrity, and supporting data architecture and modeling efforts.\n\nSkills And Qualifications\n\nProficiency in SQL, Excel, Python, R, and data visualization tools like Tableau or Power BI.\n\nAbility to interpret complex data, identify patterns, and provide actionable insights.\n\nTranslating technical findings into clear, business-friendly language for stakeholders.\n\nUnderstanding of business processes, KPIs, and regulatory requirements, especially in marketing.\n\nWillingness to adopt new tools, programming languages, and Big Data techniques.\n\nKnowledge of marketing data sets like Google Analytics, Social Media Platforms, Salesforce, Marketing Cloud and Customer Data Platforms.",
    "job_is_remote": false,
    "job_posted_at": "24 hours ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Plano, TX",
    "job_city": "Plano",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 33.0216577,
    "job_longitude": -96.6979973,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dy9Ez-YEkLqpd9staAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "12+ Months",
        "Proficiency in SQL, Excel, Python, R, and data visualization tools like Tableau or Power BI",
        "Ability to interpret complex data, identify patterns, and provide actionable insights",
        "Translating technical findings into clear, business-friendly language for stakeholders",
        "Understanding of business processes, KPIs, and regulatory requirements, especially in marketing",
        "Willingness to adopt new tools, programming languages, and Big Data techniques",
        "Knowledge of marketing data sets like Google Analytics, Social Media Platforms, Salesforce, Marketing Cloud and Customer Data Platforms"
      ],
      "Responsibilities": [
        "Collecting data from internal and external sources, ensuring accuracy, and removing duplicates or errors",
        "Applying statistical and analytical techniques to identify trends, correlations, and anomalies",
        "Creating dashboards, charts, and reports using tools like Excel, SQL, Python, and Tableau to communicate findings to stakeholders",
        "Translating complex datasets into actionable recommendations for business strategy, marketing, or operations",
        "Working with cross-functional teams, including project managers, business analysts, and marketing, to understand data needs and develop solutions",
        "Ensuring compliance with data management policies, maintaining data integrity, and supporting data architecture and modeling efforts"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-dataengineer-at-jobs-via-dice-4372009753",
    "_source": "new_jobs"
  },
  {
    "job_id": "DmKkjIHFGqwpZawzAAAAAA==",
    "job_title": "Associate Data Engineer",
    "employer_name": "THE DOW CHEMICAL COMPANY",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHE6Qv_UOqARLxrnwfpr-ucKbs6GEppbjL9zYs&s=0",
    "employer_website": null,
    "job_publisher": "THE DOW CHEMICAL COMPANY",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "THE DOW CHEMICAL COMPANY",
        "apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Dow, we believe in putting people first and we’re passionate about delivering integrity, respect and safety to our customers, our employees and the planet.\n\nOur people are at the heart of our solutions. They reflect the communities we live in and the world where we do business. Their diversity is our strength. We’re a community of relentless problem solvers that offers the daily opportunity to contribute with your perspective, transform industries and shape the future. Our purpose is simple - to deliver a sustainable future for the world through science and collaboration.If you’re looking for a challenge and meaningful role, you’re in the right place.\n\nAbout you and the role\n\nDow has an exciting and challenging opportunity for an Associate Data Engineer located in Midland, MI or Houston, TX. which will work on the Enterprise Data & Analytics - Data Analytics Platform team.\n\nAs an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow. TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects. They will work closely with amulti-disciplinary team to:\n• Createdata pipelines forprojectsin the Azure environment.\n• Write notebooks in Databricks\n• Develop Infrastructure as Code (IaC) code\n• Build logic apps\n• Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation.\n\nAssociate Data Engineer Responsibilities / Duties:\n• Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)\n• Develop and deploy data pipelines using Azure data services\n• Deployment of Azure services using Infrastructure as Code and Azure DevOps.\n• This entry level position is for an Independent Contributor and is not expected to be a people leader\n\nKnowledge, skills and abilities include:\n• Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives\n• Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’\n• Concepts of Data architecting - concepts\n• General understanding of digital industry trends\n\nOther Critical Skills:\n• Ability to thrive in challenging situations and solve complex problems\n• Ability to manage own work effort across multiple projects with little supervision\n• Analytical and problem-solving skills\n• Customer centricity\n• Good communication\n\nYour Skills\n• Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs). This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms.\n• Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems.\n• Business Processes: Understanding how business functions operate and how technology enables or improves them. This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs.\n• Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments.\n• Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards.\n\nRequired qualifications:\n• Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree.\n• A minimum requirement for this U.S. based position is the ability to work legally in the United States. No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process.\n\nYour preferred qualifications include:\n• A degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines\n• Experiencewith Azuredata services andPython (other programming language)\n• Experience developing intheAzureenvironment\n• Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling\n• Multi-application and cross-platform design experience\n\nNote: Relocation assistance is not available with this position.\n\nBenefits – What Dow offers you\n\nWe invest in you.\n\nDow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career. You bring your background, talent, and perspective to work every day. Dow rewards that commitment by investing in your total wellbeing.\n\nHere are just a few highlights of what you would be offered as a Dow employee:\n• Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives.\n• Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it.\n• Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals.\n• Employee stock purchase programs (availability varies depending on location).\n• Student Debt Retirement Savings Match Program (U.S. only).\n• Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match.\n• Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs. Travel insurance is also available in certain countries/locations.\n• Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building.\n• Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs.\n• Competitive yearly vacation allowance.\n• Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents).\n• Paid time off to care for family members who are sick or injured.\n• Paid time off to support volunteering and Employee Resource Group’s (ERG) participation.\n• Wellbeing Portal for all Dow employees, our one-stop shop to promote wellbeing, empowering employees to take ownership of their entire wellbeing journey.\n• On-site fitness facilities to help stay healthy and active (availability varies depending on location).\n• Employee discounts for online shopping, cinema tickets, gym memberships and more.\n• Additionally, some of our locations might offer:\n• Transportation allowance (availability varies depending on location)\n• Meal subsidiaries/vouchers (availability varies depending on location)\n• Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)\n\nJoin our team, we can make a difference together.\n\nAbout Dow\nDow (NYSE: DOW) is one of the world’s leading materials science companies, serving customers in high-growth markets such as packaging, infrastructure, mobility and consumer applications.Our global breadth, asset integration and scale, focused innovation, leading business positions and commitment to sustainability enable us to achieve profitable growth and help deliver a sustainable future. We operate manufacturing sites in 30countries and employ approximately36,000 people. Dow delivered sales of approximately$43 billionin 2024. References to Dow or the Company mean Dow Inc. and its subsidiaries. Learn more about us and our ambition to be the most innovative, customer-centric, inclusive and sustainable materials science company in the world by visitingwww.dow.comopens in a new tab.\n\nAs part of our dedication to inclusion, Dow is committed to equal opportunities in employment. We encourage every employee to bring their whole self to work each day to not only deliver more value, but also have a more fulfilling career. Further information regarding Dow's equal opportunities is available on www.dow.comopens in a new tab.\nDow is an Equal Employment Opportunity employer and is committed to providing opportunities without regard for race, color, religion, sex, including pregnancy, sexual orientation, or gender identity, national origin, age, disability and genetic information, including family medical history. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may call us at 1-833-My Dow HR (833-693-6947) and select option 8.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Richmond, TX",
    "job_city": "Richmond",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 29.5821811,
    "job_longitude": -95.76078319999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDmKkjIHFGqwpZawzAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This entry level position is for an Independent Contributor and is not expected to be a people leader",
        "Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives",
        "Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’",
        "Concepts of Data architecting - concepts",
        "General understanding of digital industry trends",
        "Ability to thrive in challenging situations and solve complex problems",
        "Ability to manage own work effort across multiple projects with little supervision",
        "Analytical and problem-solving skills",
        "Customer centricity",
        "Good communication",
        "Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs)",
        "Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree",
        "A minimum requirement for this U.S. based position is the ability to work legally in the United States",
        "No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process",
        "Python (other programming language)",
        "Experience developing intheAzureenvironment",
        "Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling",
        "Multi-application and cross-platform design experience"
      ],
      "Benefits": [
        "We invest in you",
        "Dow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career",
        "You bring your background, talent, and perspective to work every day",
        "Dow rewards that commitment by investing in your total wellbeing",
        "Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives",
        "Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it",
        "Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals",
        "Employee stock purchase programs (availability varies depending on location)",
        "Student Debt Retirement Savings Match Program (U.S. only)",
        "Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match",
        "Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs",
        "Travel insurance is also available in certain countries/locations",
        "Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building",
        "Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs",
        "Competitive yearly vacation allowance",
        "Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents)",
        "Paid time off to care for family members who are sick or injured",
        "Paid time off to support volunteering and Employee Resource Group’s (ERG) participation",
        "On-site fitness facilities to help stay healthy and active (availability varies depending on location)",
        "Employee discounts for online shopping, cinema tickets, gym memberships and more",
        "Additionally, some of our locations might offer:",
        "Transportation allowance (availability varies depending on location)",
        "Meal subsidiaries/vouchers (availability varies depending on location)",
        "Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)"
      ],
      "Responsibilities": [
        "which will work on the Enterprise Data & Analytics - Data Analytics Platform team",
        "As an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow",
        "TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects",
        "They will work closely with amulti-disciplinary team to:",
        "Createdata pipelines forprojectsin the Azure environment",
        "Write notebooks in Databricks",
        "Develop Infrastructure as Code (IaC) code",
        "Build logic apps",
        "Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation",
        "Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)",
        "Develop and deploy data pipelines using Azure data services",
        "Deployment of Azure services using Infrastructure as Code and Azure DevOps",
        "This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms",
        "Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems",
        "Business Processes: Understanding how business functions operate and how technology enables or improves them",
        "This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs",
        "Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments",
        "Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-dow-com-hiring-associate-data-engineer-richmond-tx-698d65cf94cbec0a687e0c22",
    "_source": "new_jobs"
  },
  {
    "job_id": "4jWnCYR8XCPyHreLAAAAAA==",
    "job_title": "Lead Data Engineer; Java, Python, Spark, AWS",
    "employer_name": "Capital One",
    "employer_logo": null,
    "employer_website": "https://www.capitalone.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/baltimore/maryland/software_development/4710755767/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Overview\n\nThe Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment. In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools. If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you.\n\nKey Responsibilities\n• Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools.\n• Work with developers experienced in machine learning, distributed microservices, and full stack systems.\n• Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake.\n• Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers.\n• Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans.\n• Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions.\n\nRequired Qualifications\n• Bachelor’s Degree\n• Minimum 4 years of experience in application development (internship experience does not apply)\n• At least 2 years of experience in big data technologies\n• At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)\n\nPreferred Qualifications\n• 7+ years of application development experience including Python, SQL, Scala, or Java\n• 4+ years of experience with a public cloud (AWS, Microsoft Azure, or Google Cloud)\n• 4+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n• 4+ years of experience with real-time data and streaming applications\n• 4+ years of experience with NoSQL implementations (e.g., Mongo or Cassandra)\n• 4+ years of data warehousing experience (Redshift or Snowflake)\n• 4+ years of Unix/Linux experience including basic commands and shell scripting\n• 2+ years of experience with Agile engineering practices\n\nBenefits & Perks\n• Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer\n• Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer\n• Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer\n• Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours.",
    "job_is_remote": false,
    "job_posted_at": "19 hours ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Baltimore, MD",
    "job_city": "Baltimore",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.2905023,
    "job_longitude": -76.6104072,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4jWnCYR8XCPyHreLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor’s Degree",
        "Minimum 4 years of experience in application development (internship experience does not apply)",
        "At least 2 years of experience in big data technologies",
        "At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)"
      ],
      "Benefits": [
        "Benefits & Perks",
        "Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer",
        "Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer",
        "Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer",
        "Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours"
      ],
      "Responsibilities": [
        "The Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment",
        "In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools",
        "If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you",
        "Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools",
        "Work with developers experienced in machine learning, distributed microservices, and full stack systems",
        "Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake",
        "Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers",
        "Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans",
        "Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-7obxlyk4atqenktrzypk5a",
    "_source": "new_jobs"
  },
  {
    "job_id": "LPMf3VOKqFx0iif1AAAAAA==",
    "job_title": "Salesforce Data Cloud Engineer (Software Engineer III)",
    "employer_name": "American Lebanese Syrian Associated Charities",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2O3VtFjogX0f0XMlHxYfcPM_n5WtCfVgXNUvu&s=0",
    "employer_website": null,
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/American-Lebanese-Syrian-Associated-Charities/Job/Salesforce-Data-Cloud-Engineer-(Software-Engineer-III)/-in-Memphis,TN?jid=ea00d7792eef4280&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/American-Lebanese-Syrian-Associated-Charities/Job/Salesforce-Data-Cloud-Engineer-(Software-Engineer-III)/-in-Memphis,TN?jid=ea00d7792eef4280&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/salesforce-data-coud-engineer-software-engineer-iii-at-st-jude-children-s-research-hospital-alsac-4371665068?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/salesforce-data-cloud-engineer-software-engineer-memphis-st-jude-alsac-330bd194804998465f6e3f08aec15287?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/salesforce-data-coud-engineer-software-engineer-iii/8473158?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/698d472cf64d441a164f4d81?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "J-O-B-Z",
        "apply_link": "https://j-o-b-z.com/seo/job/136483770/tn/memphis/salesforce-data-coud-engineer-software-engineer-iii?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/amp/job/us-tn-memphis-salesforce-data-coud-engineer-software-iii-st-jude-alsac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "AtALSACyou do more than make a living; you make a difference.\n\nWe like people who are different...because we're different, too. As one of the world's most iconic and respected nonprofits, we know what it's like to stand out. That's why we're looking at you. Your background, perspective, and desire to make an impact set you apart. As we work to help St. Jude cure childhood cancer, we're calling on the game-changers, innovators and visionaries to join our family. Not just for the kids of St. Jude, but also for you. Because at ALSAC, we develop and celebrate our employees. So, bring your whole, authentic self and become part of our shared mission: Finding cures. Saving children.\n\nJob Description\n\nThe Salesforce Data Cloud Engineer (Software Engineer III) provides technical leadership and hands-on engineering expertise in designing, developing, implementing, and supporting scalable enterprise data and application solutions. This role is a key contributor and mentor within the engineering team, serving as a subject matter expert across Salesforce Data Cloud, identity resolution, data governance, APIs, and multi-layered application development.\n\nThis position drives the architecture and delivery of Data Cloud pipelines, customer identity unification, and privacy-safe activation across enterprise platforms. The engineer continually evaluates emerging technologies, ensures adherence to secure development practices, and documents solutions through clear technical artifacts including diagrams, flowcharts, metadata definitions, and code documentation.\n\nPrimary Responsibilities\n\nSalesforce Data Cloud & Unified Profile Engineering\n• Design, develop, and optimize Data Cloud pipelines-including Data Streams, Data Model Objects (DMOs), Data Graphs, Calculated Insights-to build a scalable unified Profile System with high-quality identity resolution and golden records.\n• Define, tune, and maintain identity resolution rulesets (deterministic and probabilistic) for accuracy, precision/recall, and measurable profile completeness.\n• Build and optimize Segments and Activation across Marketing Cloud, Advertising platforms, Sales/Service Cloud, and Commerce using privacy-safe matching and deduplication.\n• Implement data governance, consent modeling, and privacy controls (GDPR/CCPA) across ingestion, unification, storage, and activation.\n• Partner with Marketing, Product, Analytics, and CRM teams to translate business use cases into Data Cloud data models, insights, and event-driven activations.\nData Engineering, Integrations & Platform Reliability\n• Establish and maintain CI/CD pipelines, automated data quality checks, operational monitoring, data lineage visibility, and SLA reporting across ingestion and activation flows.\n• Collaborate on Lakehouse Federation and Zero-Copy architecture patterns with Amazon Redshift, Databricks, and other enterprise data platforms for analytics, ML integration, and model operationalization (Einstein Studio or external).\n• Architect and implement ETL/ELT processes using SQL, streaming platforms (Kafka/Kinesis), APIs, and microservices aligned to enterprise engineering standards.\n• Analyze and resolve complex technical issues across interconnected systems; perform root cause analysis, implement fixes, and prevent recurrence.\nApplication Development & Engineering Leadership\n• Apply software engineering best practices across front-end, back-end, and data layers while conforming to internal coding standards and secure development practices.\n• Create, modify, test, debug, document, and maintain complex application programs supporting enterprise systems.\n• Work directly with users, business analysts, and cross-functional teams to understand requirements, validate user needs, and drive enhancements.\n• Mentor and support Software Engineer I and II team members through code reviews, technical guidance, and skill development.\nContinuous Learning & Technical Governance\n• Stay current with industry trends, emerging technologies, and Salesforce ecosystem advancements; participate in education, certifications, and learning opportunities.\n• Maintain thorough documentation (flowcharts, diagrams, layouts, data dictionaries, design specifications) to support system understanding and operational continuity.\nMinimum Qualifications\n• 5+ years of experience in data engineering, CRM, or Customer Data Platforms (CDP).\n• 2+ years hands-on with Salesforce Data Cloud or an equivalent enterprise CDP.\n• Bachelor's degree in Information Systems, Engineering, Mathematics, Computer Science, or related field; 5-8 years technical and programming experience.\n• Strong SQL proficiency and hands-on expertise with Data Cloud entities (DMOs, Data Streams, Data Graphs, Calculated Insights, Segments, Activation).\n• Proven experience activating audiences across Marketing Cloud, Advertising platforms, Sales/Service Cloud, or Commerce Cloud.\n• Experience with ETL/ELT pipelines, streaming technologies (Kafka/Kinesis), REST/GraphQL APIs, and Git-based CI/CD.\n• Strong understanding of consent management, data privacy, PII/PHI protection, and compliance frameworks (GDPR/CCPA).\n• Professional experience developing across multiple layers of the application stack (front-end, back-end, data, APIs).\n• Experience in structured, cross-functional environments with established roles and processes.\n• Preferred skills include experience with Python, dbt, MDM platforms, Reverse ETL, enterprise data warehouses (e.g., Amazon Redshift), Einstein Studio or ML model operationalization, and Copado or similar Salesforce DevOps tooling.\n\nWork Location:\nThis position is based at ALSAC's National Executive Office in Memphis, TN and is eligible for a hybrid work schedule.\n\nBenefits & PerksThe following Benefits & Perks apply to Full-Time Roles Only.We're dedicated to ensuring children and their families have every opportunity to enjoy life's special moments. We're also committed to giving our staff excellent benefits so they can do the same.\n• Core Medical Coverage: (low cost low deductible Medical, Dental, and Vison Insurance plans)\n• 401K Retirement Plan with 7% Employer Contribution\n• Exceptional Paid Time Off\n• Maternity / Paternity Leave\n• Infertility Treatment Program\n• Adoption Assistance\n• Education Assistance\n• Enterprise Learning and Development\n• And more\n\nALSAC is an equal employment opportunity employer.\n\nALSAC does not discriminate against any individual with regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, transgender status, disability, veteran status, genetic information or other protected status.\n\nNo Search Firms:\n\nALSAC does not accept unsolicited assistance from search firms for employment opportunities. All resumes submitted by search firms to any ALSAC employee or ALSAC representative via email, the internet or in any form and/or method without being contacted and approved by our Employee Experience team and without a valid written search agreement in place will result in no fee being paid if a referred candidate is hired by ALSAC.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Memphis, TN",
    "job_city": "Memphis",
    "job_state": "Tennessee",
    "job_country": "US",
    "job_latitude": 35.148581199999995,
    "job_longitude": -90.0518955,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DLPMf3VOKqFx0iif1AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience in data engineering, CRM, or Customer Data Platforms (CDP)",
        "2+ years hands-on with Salesforce Data Cloud or an equivalent enterprise CDP",
        "Bachelor's degree in Information Systems, Engineering, Mathematics, Computer Science, or related field; 5-8 years technical and programming experience",
        "Strong SQL proficiency and hands-on expertise with Data Cloud entities (DMOs, Data Streams, Data Graphs, Calculated Insights, Segments, Activation)",
        "Proven experience activating audiences across Marketing Cloud, Advertising platforms, Sales/Service Cloud, or Commerce Cloud",
        "Experience with ETL/ELT pipelines, streaming technologies (Kafka/Kinesis), REST/GraphQL APIs, and Git-based CI/CD",
        "Strong understanding of consent management, data privacy, PII/PHI protection, and compliance frameworks (GDPR/CCPA)",
        "Professional experience developing across multiple layers of the application stack (front-end, back-end, data, APIs)",
        "Experience in structured, cross-functional environments with established roles and processes"
      ],
      "Benefits": [
        "Benefits & PerksThe following Benefits & Perks apply to Full-Time Roles Only",
        "We're dedicated to ensuring children and their families have every opportunity to enjoy life's special moments",
        "We're also committed to giving our staff excellent benefits so they can do the same",
        "Core Medical Coverage: (low cost low deductible Medical, Dental, and Vison Insurance plans)",
        "401K Retirement Plan with 7% Employer Contribution",
        "Exceptional Paid Time Off",
        "Maternity / Paternity Leave",
        "Infertility Treatment Program",
        "Adoption Assistance",
        "Education Assistance",
        "Enterprise Learning and Development"
      ],
      "Responsibilities": [
        "The Salesforce Data Cloud Engineer (Software Engineer III) provides technical leadership and hands-on engineering expertise in designing, developing, implementing, and supporting scalable enterprise data and application solutions",
        "This role is a key contributor and mentor within the engineering team, serving as a subject matter expert across Salesforce Data Cloud, identity resolution, data governance, APIs, and multi-layered application development",
        "This position drives the architecture and delivery of Data Cloud pipelines, customer identity unification, and privacy-safe activation across enterprise platforms",
        "The engineer continually evaluates emerging technologies, ensures adherence to secure development practices, and documents solutions through clear technical artifacts including diagrams, flowcharts, metadata definitions, and code documentation",
        "Salesforce Data Cloud & Unified Profile Engineering",
        "Design, develop, and optimize Data Cloud pipelines-including Data Streams, Data Model Objects (DMOs), Data Graphs, Calculated Insights-to build a scalable unified Profile System with high-quality identity resolution and golden records",
        "Define, tune, and maintain identity resolution rulesets (deterministic and probabilistic) for accuracy, precision/recall, and measurable profile completeness",
        "Build and optimize Segments and Activation across Marketing Cloud, Advertising platforms, Sales/Service Cloud, and Commerce using privacy-safe matching and deduplication",
        "Implement data governance, consent modeling, and privacy controls (GDPR/CCPA) across ingestion, unification, storage, and activation",
        "Partner with Marketing, Product, Analytics, and CRM teams to translate business use cases into Data Cloud data models, insights, and event-driven activations",
        "Data Engineering, Integrations & Platform Reliability",
        "Establish and maintain CI/CD pipelines, automated data quality checks, operational monitoring, data lineage visibility, and SLA reporting across ingestion and activation flows",
        "Collaborate on Lakehouse Federation and Zero-Copy architecture patterns with Amazon Redshift, Databricks, and other enterprise data platforms for analytics, ML integration, and model operationalization (Einstein Studio or external)",
        "Architect and implement ETL/ELT processes using SQL, streaming platforms (Kafka/Kinesis), APIs, and microservices aligned to enterprise engineering standards",
        "Analyze and resolve complex technical issues across interconnected systems; perform root cause analysis, implement fixes, and prevent recurrence",
        "Application Development & Engineering Leadership",
        "Apply software engineering best practices across front-end, back-end, and data layers while conforming to internal coding standards and secure development practices",
        "Create, modify, test, debug, document, and maintain complex application programs supporting enterprise systems",
        "Work directly with users, business analysts, and cross-functional teams to understand requirements, validate user needs, and drive enhancements",
        "Mentor and support Software Engineer I and II team members through code reviews, technical guidance, and skill development",
        "Continuous Learning & Technical Governance",
        "Stay current with industry trends, emerging technologies, and Salesforce ecosystem advancements; participate in education, certifications, and learning opportunities",
        "Maintain thorough documentation (flowcharts, diagrams, layouts, data dictionaries, design specifications) to support system understanding and operational continuity"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-american-lebanese-syrian-associated-charities-job-salesforce-data-cloud-engineer-software-engineer-iii-in-memphis-tn",
    "_source": "new_jobs"
  },
  {
    "job_id": "aLEr5N2HS9ba0TU3AAAAAA==",
    "job_title": "Data Engineer - Remote at Staffing the Universe United States",
    "employer_name": "Staffing the Universe",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Ibfportal.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Ibfportal.com",
        "apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Remote job at Staffing the Universe. United States. Data Engineer\n\nData Engineer Hartford, CT or Remote Contract No third-party C2C\n\nJob Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team. On this team, you will be helping architect and deliver a wide variety of code artifacts. You will be working to build a scalable and secure ETL solutions for ope...",
    "job_is_remote": false,
    "job_posted_at": "9 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "New Haven, CT",
    "job_city": "New Haven",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.308274,
    "job_longitude": -72.9278835,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaLEr5N2HS9ba0TU3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Data Engineer Hartford, CT or Remote Contract No third-party C2C"
      ],
      "Responsibilities": [
        "Job Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team",
        "On this team, you will be helping architect and deliver a wide variety of code artifacts",
        "You will be working to build a scalable and secure ETL solutions for ope.."
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "ibfportal-com-office-job-data-engineer-remote-at-staffing-the-universe-united-states-ym9lbu9gtu5hnlh5ede0nys1utrrk2vvzmc9pq",
    "_source": "new_jobs"
  },
  {
    "job_id": "fKq613YVZoGaO3ZhAAAAAA==",
    "job_title": "Temporary  Student Data Engineer",
    "employer_name": "University of Notre Dame",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB_bk0V1eXoFSLkXbpRAL9w5OPDz55ACjhxHof&s=0",
    "employer_website": null,
    "job_publisher": "BMES Career Center",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BMES Career Center",
        "apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/698af9934db8972cec006c83?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/5e385fd85cc450ceae1fd6846215958e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/indiana/business/4868026961/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-notre-dame-temporary-student-data-engineer-university-part_time?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temporary Student Data Engineer\n\nNotre Dame, IN, United States\nContract\nProvost\nTemporary\n\nCompany Description\n\nJob Description\nWe are seeking a Student Data Engineer (SDE) to join a team of students creating a Roblox game as a data collection tool, gauging students' interest in STEM and health care careers. The Lucy Family Institute for Data & Society (LFIDS) leverages data science, AI & ML toward social good. LFIDS engages with the Notre Dame community & beyond through funded research projects & collaborations, educational workshops, and special events. SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana. In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed.\n\nKey Responsibilities:\n\nWithin assigned projects, this role requires completion of data processing and programming tasks related to:\n• Data collection, management, harvesting, processing, transformation, and visualization;\n• Prototype data processing solutions;\n• Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms.\n\nThe successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners. Other responsibilities may include data analysis and giving presentations to diverse audiences.\n\nAdditional Requirements\n\nIn addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested. For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience.\n\nAdditional Opportunities\n\nStudent data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects. Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately.\n\nThe Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests.\n\nCore Qualities & Expectations\n• Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving\n• Confidentiality: Maintaining confidentiality is required.\n• Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings).\n• Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly. Perfection is not expected-it's okay to take time to learn. What matters is trying your best in each unique circumstance. We're committed to supporting your growth and confidence in the role.\n• Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities. We aim to provide a structure that supports your success.\n• Attention to detail or the ability to follow a set of instructions that we'll co-create and adjust based on your preferred learning and working style.\n• Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer.\n\nQualifications\nWe are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:\n• Data science methods and tools,\n• Software design\n• User experience principles\n\nExperience in an LFIDS area of expertise, like:\n• R and/or Python\n• Events and communications support\n• Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites).\n\nAdditional Information\nCompensation: $17.00/hour\n\nApplications for this position will close on February 13, 2026.\n\nThe University of Notre Dame seeks to attract, develop, and retain the highest quality faculty, staff and administration. The University is an Equal Opportunity Employer, and does not discriminate on the basis of race, color, national or ethnic origin, sex, disability, veteran status, genetic information, or age in employment. Moreover, Notre Dame prohibits discrimination against veterans or disabled qualified individuals, and complies with 41 CFR 60-741.5(a) and 41 CFR 60-300.5(a). We strongly encourage applications from candidates attracted to a university with a Catholic identity.\n\nTo apply, visit https://jobs.smartrecruiters.com/UniversityOfNotreDame/3743990011597785-temporary-student-data-engineer\n\nCopyright 2025 Jobelephant.com Inc. All rights reserved.\n\nPosted by the FREE value-added recruitment advertising agency jeid-5691f5d28abbde4e8b71884761a96763",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Notre Dame, IN",
    "job_city": "Notre Dame",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.7001908,
    "job_longitude": -86.2379328,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DfKq613YVZoGaO3ZhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Student data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects",
        "Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving",
        "Confidentiality: Maintaining confidentiality is required",
        "Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings)",
        "Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly",
        "Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities",
        "We aim to provide a structure that supports your success",
        "Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer",
        "We are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:",
        "Data science methods and tools,",
        "Software design",
        "User experience principles",
        "Experience in an LFIDS area of expertise, like:",
        "R and/or Python",
        "Events and communications support",
        "Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites)"
      ],
      "Benefits": [
        "Compensation: $17.00/hour"
      ],
      "Responsibilities": [
        "SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana",
        "In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed",
        "Within assigned projects, this role requires completion of data processing and programming tasks related to:",
        "Data collection, management, harvesting, processing, transformation, and visualization;",
        "Prototype data processing solutions;",
        "Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms",
        "The successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners",
        "Other responsibilities may include data analysis and giving presentations to diverse audiences",
        "In addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested",
        "For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience",
        "Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately",
        "The Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobboard-bmes-org-jobs-22037211-temporary-student-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "vmP5DjcyLh4_B94fAAAAAA==",
    "job_title": "Senior Data Engineer (Remote)",
    "employer_name": "Parsons Corporation",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWoeHM7rt-U37K1_ab3N5lIK6QuMuDbepnStZr&s=0",
    "employer_website": "https://www.parsons.com",
    "job_publisher": "Parsons Careers - Parsons Corporation",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Parsons Careers - Parsons Corporation",
        "apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "In a world of possibilities, pursue one with endless opportunities. Imagine Next!\n\nAt Parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. Guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. Unleash your talent and redefine what’s possible.\n\nJob Description:\n\nParsons is looking for an amazingly talented Senior Data Engineer to join our team! In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization.\n\nWhat You'll Be Doing:\n• Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture.\n• Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake.\n• Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing.\n• Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing.\n• Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation.\n• Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms.\n• Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data.\n• Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement.\n\nWhat Required Skills You'll Bring:\n• Strong hands-on experience with T-SQL and Python.\n• Experience with comprehensive data conversion projects is preferred (ERP systems including Oracle Cloud ERP and/or SAP S4/HANA)\n• Experience with Relational Database systems\n• Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)\n• Familiar with multi-dimensional and tabular models\n• 5+ years of experience in data engineering, data architecture, or data platform development.\n• Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar).\n• Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines.\n• Deep understanding of lakehouse architecture and medallion design patterns.\n• Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines.\n• Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats.\n• Strong problem-solving skills and ability to work independently in a fast-paced environment.\n• US person\n\nWhat Desired Skills You'll Bring:\n• Experience with data governance, security, and compliance (e.g., SOX, HIPAA).\n• Snowflake, Azure Data Engineer, dbt, and/or Databricks certifications\n• Exposure to real-time data processing and streaming technologies (e.g., Kafka, Spark Streaming).\n• Familiarity with data observability tools and automated testing frameworks for pipelines.\n• Bachelor's or Master’s degree in Computer Science, Information Systems, or a related field\n\nSecurity Clearance Requirement:\nNone\n\nThis position is part of our Corporate team.\n\nFor over 80 years, Parsons Corporation, has shaped the future of the defense, intelligence, and critical infrastructure markets. Our employees work in a close-knit team environment to find new, innovative ways to deliver smart solutions that are used and valued by customers around the world. By combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, transportation, smart cities, and more, we're providing tomorrow's solutions today.\n\nSalary Range: $100,900.00 - $176,600.00\n\nWe value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!\n\nThis position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled.\n\nParsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status.\n\nWe truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! Imagine next and join the Parsons quest—APPLY TODAY!\n\nParsons is aware of fraudulent recruitment practices. To learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Virginia",
    "job_city": null,
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 37.4315734,
    "job_longitude": -78.6568942,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvmP5DjcyLh4_B94fAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 101000,
    "job_max_salary": 177000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Strong hands-on experience with T-SQL and Python",
        "Experience with Relational Database systems",
        "Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)",
        "Familiar with multi-dimensional and tabular models",
        "5+ years of experience in data engineering, data architecture, or data platform development",
        "Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar)",
        "Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines",
        "Deep understanding of lakehouse architecture and medallion design patterns",
        "Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines",
        "Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats",
        "Strong problem-solving skills and ability to work independently in a fast-paced environment",
        "US person"
      ],
      "Benefits": [
        "Salary Range: $100,900.00 - $176,600.00",
        "We value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!"
      ],
      "Responsibilities": [
        "In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization",
        "Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture",
        "Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake",
        "Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing",
        "Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing",
        "Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation",
        "Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms",
        "Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data",
        "Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-parsons-com-jobs-senior-data-engineer-remote-virtual-r-174702-jobs-information-technology",
    "_source": "new_jobs"
  },
  {
    "job_id": "1Eb8GME5F6h5wzAiAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Rural King",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSI99A-RNW3mNS-GYEPIGd0U4Qdfm6662lvJ7uw&s=0",
    "employer_website": "https://www.ruralking.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-rural-king-4371246007?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Rural-King-Supply/Job/Data-Engineer/-in-Mattoon,IL?jid=4c03fb6a091c6a1e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/data-engineer_7ea1a64fa7b5bb1f8b19872d37b34c2da2d7e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/data-engineer-mattoon-il--e8d0d5a5-70ea-4eed-87dd-58b13c57a77d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Job Listings - ICIMS",
        "apply_link": "https://careers-ruralking.icims.com/jobs/35283/data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Nexxt",
        "apply_link": "https://www.nexxt.com/jobs/data-engineer-mattoon-il-3161318692-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/_m6FAmu2Nyxj74VafN91ouQs45puOR74OyTLalSIYaTrYL6icAgj4w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About us\n\nRural King is America's Farm and Home Store, providing essentials to the communities we serve. With a wide array of necessities ranging from food and feed to farm and home products, Rural King serves over 130 locations across 13 states and is constantly expanding. Our annual sales exceed $2.5 Billion, and our heart beats in Mattoon, IL, home to our corporate office, distribution center, and flagship store.\n\nOne thing our customers appreciate is our unique shopping experience, complete with complimentary popcorn and coffee. It's just one way we show our appreciation for their support.\n\nAt Rural King, we value our associates and strive to create a positive, rewarding workplace. We offer growth opportunities, competitive benefits, and a people-first environment where dedicated individuals come together to serve rural communities passionately. Join us, and you'll find not just a job but a chance to grow professionally, contribute meaningfully, and make a difference in the lives of those we serve.\n\nHow we reward you\n\n401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%\n\nHealthcare plans to support your needs\n\nVirtual doctor visits\n\nAccess to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program\n\n15% Associate Discount\n\nDave Ramsey's SmartDollar Program\n\nAssociate Assistance Program\n\nRK Cares Associate Hardship Program\n\n24/7 Chaplaincy Services\n\nCompany paid YMCA Family Membership\n\nWhat You'll do\n\nAs a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization.\n• Design, build, maintain, and manage the systems that move data efficiently within the organization.\n• Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake.\n• Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs\n• Monitor the performance of data pipelines that are efficient and secure\n• Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism.\n• Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments.\n• Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively.\n• Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement.\n• Perform other duties as assigned.\n\nSupervisory Responsibilities\n\nNone\n\nEssential Qualities for Success\n• At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education.\n• Information Technology Engineering experience preferred.\n• Experience as a Data Engineer or similar software engineering role.\n• Proficient with git.\n• Good knowledge of SQL and PHP or similar languages.\n• Good knowledge of big data technologies.\n• Good knowledge of data modeling.\n• Familiar with Looker, Power BI or other BI tools.\n• Working knowledge of databases, MySQL/MariaDB, and SQL.\n• Strong understanding of retail business practices.\n• Excellent negotiation and conflict resolution skills.\n• Demonstrated ability to adapt in a fast-paced environment.\n• Strong analytical and problem-solving skills.\n• Excellent organizational skills and attention to detail.\n• Demonstrated behaviors must reflect integrity, professionalism, and confidentiality.\n\nPhysical Requirements\n• Ability to maintain a seated or standing position for extended durations.\n• Capability to lift 15 pounds periodically.\n• Able to navigate and access all facilities.\n• Skill to effectively communicate verbally with others, both in-person and via electronic devices.\n• Close vision for computer-related tasks.\n\nReasonable accommodations may be made to enable individuals with disabilities to perform essential job functions.\n\nThe pay range for this position is $55,000 - $65,000 annualized and is bonus eligible. Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs. To learn more about our benefits, review here https://acrobat.adobe.com/id/urn:aaid:sc:VA6C2:14539c15-191a-4b77-9c13-f6ccfce10094.\n\nResponsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization. - Design, build, maintain, and manage the systems that move data efficiently within the organization. - Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake. - Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism. - Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments. - Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively. - Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement. - Perform other duties as assigned. Supervisory Responsibilities None",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Mattoon, IL",
    "job_city": "Mattoon",
    "job_state": "Illinois",
    "job_country": "US",
    "job_latitude": 39.4830897,
    "job_longitude": -88.37282549999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D1Eb8GME5F6h5wzAiAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55000,
    "job_max_salary": 65000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education",
        "Experience as a Data Engineer or similar software engineering role",
        "Proficient with git",
        "Good knowledge of SQL and PHP or similar languages",
        "Good knowledge of big data technologies",
        "Good knowledge of data modeling",
        "Familiar with Looker, Power BI or other BI tools",
        "Working knowledge of databases, MySQL/MariaDB, and SQL",
        "Strong understanding of retail business practices",
        "Excellent negotiation and conflict resolution skills",
        "Demonstrated ability to adapt in a fast-paced environment",
        "Strong analytical and problem-solving skills",
        "Excellent organizational skills and attention to detail",
        "Demonstrated behaviors must reflect integrity, professionalism, and confidentiality",
        "Ability to maintain a seated or standing position for extended durations",
        "Capability to lift 15 pounds periodically",
        "Able to navigate and access all facilities",
        "Skill to effectively communicate verbally with others, both in-person and via electronic devices",
        "Close vision for computer-related tasks"
      ],
      "Benefits": [
        "401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%",
        "Healthcare plans to support your needs",
        "Access to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program",
        "15% Associate Discount",
        "Dave Ramsey's SmartDollar Program",
        "Associate Assistance Program",
        "RK Cares Associate Hardship Program",
        "24/7 Chaplaincy Services",
        "The pay range for this position is $55,000 - $65,000 annualized and is bonus eligible",
        "Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs"
      ],
      "Responsibilities": [
        "As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs",
        "Monitor the performance of data pipelines that are efficient and secure",
        "Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned",
        "Responsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d",
    "_source": "new_jobs"
  },
  {
    "job_id": "XhgeNP4cggggaxCtAAAAAA==",
    "job_title": "(USA) Senior Manager, Data Engineering",
    "employer_name": "Walmart",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0XcK_pPTIiFuiAYrGraOGjkX3xDSPgjHRhL3B&s=0",
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Senior-Manager,-Data-Engineering/-in-Cupertino,CA?jid=e874def2a1a78ecf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do...The Mission\nAt Sam’s Club, we are no longer just building dashboards or static pipelines; we are building the \"brain\" of the retail experience. As the Senior Engineering Manager for Agentic Data, you will be at the forefront of the AI revolution. Your mission is to evolve our massive data ecosystem into a Semantic & Contextual Layer—the foundational intelligence that allows AI Agents to reason, plan, and act autonomously for millions of members. You aren't just managing a team; you are architecting the bridge between raw data and autonomous action. You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time.\n________________________________________\nWhat You’ll Do\n• Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows.\n• Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents. You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state.\n• Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles. You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models.\n• Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations.\n• Strategic Leadership: Manage and mentor a high-performing team of engineers. You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals.\n• Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources.\n• Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows.\n________________________________________\nWhat You’ll Bring\n• Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role.\n• Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals. You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem).\n• Real-Time Expertise: Deep experience with Kafka and event-driven architectures. You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming).\n• Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI).\n• Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads.\n• Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality.\n• Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design.\n\nBenefits & Perks:\nBeyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWe believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.\n\nAbout Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.\n\nWe’re virtual\nWorking virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nThe annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years’ experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.\n4 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n1 year’s supervisory experience.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n809 11th Ave, Sunnyvale, CA 94089-4731, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "Cupertino, CA",
    "job_city": "Cupertino",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.322997799999996,
    "job_longitude": -122.03218229999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXhgeNP4cggggaxCtAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 143000,
    "job_max_salary": 286000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience: 7–10+ years in Data Engineering, building and operating large-scale, distributed, fault-tolerant data platforms, with at least 3+ years in a leadership role",
        "Data Engineering Mastery: Deep-rooted expertise in Data Fundamentals",
        "You must be an expert in data modeling (Star/Snowflake, Data Vault), storage formats (Parquet, Avro), and distributed computing concepts (sharding, replication, and CAP theorem)",
        "You understand how to manage state, schema evolution, and consistency in streaming environments (Kafka Streams, Flink, or Spark Streaming)",
        "Semantic & AI Mastery: Deep familiarity with the modern AI stack—specifically Vector Databases (Pinecone, Milvus), Knowledge Graphs, and framework orchestration (LangChain, LlamaIndex, CrewAI)",
        "Cloud Scale Mastery: Proven track record in GCP or Azure, utilizing BigQuery, Dataflow, and Pub/Sub to handle petabyte-scale workloads",
        "Engineering Rigor: Fluency in Python, Java, or Scala, with a deep engineering mindset around testability, maintainability, and high-performance system design",
        "Option 1: Bachelor’s degree in Computer Science and 5 years' experience in software engineering or related field",
        "Option 2: 7 years’ experience in software engineering or related field",
        "Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field",
        "4 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "1 year’s supervisory experience",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 5 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "Beyond competitive pay, you can receive incentive awards for your performance",
        "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "The annual salary range for this position is $143,000.00 - $286,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "You will lead the transition from traditional data structures to agent-ready environments, where data is not just stored, but understood in real-time",
        "Architect the Semantic Future: Define the technical vision for a unified semantic layer that transforms structured, semi-structured, and unstructured data into context-aware representations (Embeddings, Knowledge Graphs, and Metadata) consumable by LLMs and multi-agent workflows",
        "Real-Time Agentic Intelligence: Own the streaming architecture that feeds our agents",
        "You will leverage Kafka to build low-latency event-driven pipelines, ensuring agents have access to \"up-to-the-second\" member context and environmental state",
        "Champion Data Fundamentals: Ensure the bedrock of our AI strategy is built on elite Data Engineering principles",
        "You will oversee the design of scalable schemas, partitioning strategies, and high-performance indexing that make petabyte-scale data accessible to reasoning models",
        "Build Agent-Ready Ecosystems: Lead the design of \"Agentic Data\" pipelines—ensuring that data provided to AI agents is high-fidelity, optimized for retrieval, and enriched with the necessary business logic to prevent hallucinations",
        "Strategic Leadership: Manage and mentor a high-performing team of engineers",
        "You will bridge the gap between AI/ML Research, Product, and Platform Engineering to align data roadmaps with long-term autonomous decision-making goals",
        "Innovate Retrieval Systems: Own the strategy for Retrieval-Augmented Generation (RAG) at scale, optimizing how our agents discover and interact with Sam's Club’s vast proprietary knowledge base across both batch and streaming sources",
        "Establish Operational Excellence: Define standards for \"Agentic Observability\"—implementing telemetry, lineage, and auditability patterns specifically for autonomous and regulated AI workflows",
        "Real-Time Expertise: Deep experience with Kafka and event-driven architectures",
        "Data Modeling for LLMs: Expert-level understanding of how schema choices, latency, and context window management impact AI reasoning and retrieval quality"
      ]
    },
    "job_onet_soc": "11302100",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-walmart-job-usa-senior-manager-data-engineering-in-cupertino-ca",
    "_source": "new_jobs"
  },
  {
    "job_id": "Zl_lLJqOx3OHfJdpAAAAAA==",
    "job_title": "Tech Lead/Data Engineer III",
    "employer_name": "Medica",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_FelXxkN3248jJhI07DlC_ZOvbFg5RbXHeQzt&s=0",
    "employer_website": "https://www.medica.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/medica/hopkins-mn/lead-data-engineer/788bbbb1a1cac318a6dc43a1b175d33c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5613772574?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/hopkins/minnesota/info_technology/4860550653/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido",
        "apply_link": "https://us.jobrapido.com/jobpreview/6594542515557236736?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-mn-hopkins-tech-lead-data-engineer-iii-medica-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Medica is seeking an experienced Tech Lead/Data Engineer III. This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives. This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance.\n\nKey Responsibilities:\n\nWe're a team that owns our work with accountability, makes data-driven decisions, embraces continuous learning, and celebrates collaboration — because success is a team sport. It's our mission to be there in the moments that matter most for our members and employees. Join us in creating a community of connected care, where coordinated, quality service is the norm and every member feels valued.\n\nDesign, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources. Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data. Write complex SQL queries to support analytics needs. Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark.\n\nKey Accountabilities:\n• Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics.\n• Ensuring data quality, integrity, and security across all stages of the data lifecycle.\n• Collaborating with stakeholders to define data requirements and translate them into technical specifications.\n• Monitoring and optimizing performance of data systems and ETL processes.\n• Supporting the deployment and maintenance of data infrastructure in cloud environments.\n• Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting.\n• Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows.\n\nIn addition to engineering responsibilities, the Data Generalist component of this role includes:\n• Performing exploratory data analysis and generating actionable insights.\n• Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel.\n• Collaborating with cross-functional teams to align data efforts with business goals.\n• Automating data workflows using scripting languages such as Python or R.\n• Supporting business intelligence initiatives and translating data into strategic recommendations.\n• Documenting data processes and contributing to data governance standards.\n• Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets.\n• Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation.\n\nQualifications:\n• Bachelor's degree or equivalent experience in related field\n• 5 years of related work experience beyond the degree\n• Proficient use of SQL\n\nSkills And Abilities\n• Microsoft Certified: (or equivalent cloud) Azure Data Engineer\n• Microsoft Certified: Azure Fundamentals\n• Snowflake SnowPro Core Certification\n• Snowflake Advanced Architect Certification\n• Healthcare industry experience is desired\n\nThis position is an Office role, which requires an employee to work onsite at our Minnetonka, MN office, on average, 3 days per week.\n\nThe full salary grade for this position is $100,300 - $172,000. While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465. Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary. Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees.\n\nThe compensation and benefits information is provided as of the date of this posting. Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law.\n\nEligibility to work in the US: Medica does not offer work visa sponsorship for this role. All candidates must be legally authorized to work in the United States at the time of application. Employment is contingent on verification of identity and eligibility to work in the United States.\n\nWe are an Equal Opportunity employer, where all qualified candidates receive consideration for employment indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Hopkins, MN",
    "job_city": "Hopkins",
    "job_state": "Minnesota",
    "job_country": "US",
    "job_latitude": 44.9244005,
    "job_longitude": -93.41143989999999,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DZl_lLJqOx3OHfJdpAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 100000,
    "job_max_salary": 172000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives",
        "This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance",
        "Bachelor's degree or equivalent experience in related field",
        "5 years of related work experience beyond the degree",
        "Proficient use of SQL",
        "Microsoft Certified: (or equivalent cloud) Azure Data Engineer",
        "Microsoft Certified: Azure Fundamentals",
        "Snowflake SnowPro Core Certification",
        "Snowflake Advanced Architect Certification",
        "All candidates must be legally authorized to work in the United States at the time of application"
      ],
      "Benefits": [
        "The full salary grade for this position is $100,300 - $172,000",
        "While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465",
        "Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary",
        "Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees",
        "The compensation and benefits information is provided as of the date of this posting",
        "Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law"
      ],
      "Responsibilities": [
        "Design, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources",
        "Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data",
        "Write complex SQL queries to support analytics needs",
        "Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark",
        "Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics",
        "Ensuring data quality, integrity, and security across all stages of the data lifecycle",
        "Collaborating with stakeholders to define data requirements and translate them into technical specifications",
        "Monitoring and optimizing performance of data systems and ETL processes",
        "Supporting the deployment and maintenance of data infrastructure in cloud environments",
        "Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting",
        "Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows",
        "In addition to engineering responsibilities, the Data Generalist component of this role includes:",
        "Performing exploratory data analysis and generating actionable insights",
        "Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel",
        "Collaborating with cross-functional teams to align data efforts with business goals",
        "Automating data workflows using scripting languages such as Python or R",
        "Supporting business intelligence initiatives and translating data into strategic recommendations",
        "Documenting data processes and contributing to data governance standards",
        "Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets",
        "Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-tech-lead-data-engineer-iii-at-medica-4368808744",
    "_source": "new_jobs"
  },
  {
    "job_id": "Oh9syzGwM8nradswAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "The Walt Disney Company (Corporate)",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrdX9TX6JOKS6Ige-voB65FoPaFV54bDCcnlw2&s=0",
    "employer_website": null,
    "job_publisher": "Women For Hire- Job Board",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.womenforhire.com/job/usa/clermont-fl/lead-data-engineer-213217/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Women For Hire- Job Board",
        "apply_link": "https://jobs.womenforhire.com/job/usa/clermont-fl/lead-data-engineer-213217/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Disney Decision Science + Integration (DDSI) is a consulting team that supports clients across The Walt Disney Company, including Disney Experiences (Parks & Resorts worldwide, Cruise Line, Consumer Products, etc.), Disney Entertainment (ABC, The Walt Disney Studios, Disney Theatrical, Disney Streaming Services, etc.), ESPN, and Corporate Finance. Key partners to the DDSI organization include Marketing, Finance, Business Development, Research, and Operations. We develop, analyze, and execute strategies and improve the value proposition for our Guests, Cast Members, and Shareholders. The team leverages technology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business decisions and drive business value.\n\nWhat You Will Do\n\nYou will be responsible for planning and leading research and development related to advanced analytic data solutions, leveraging GenAI. You will work with business and technology leaders to understand scope and requirements, business needs, and data from across the Disney company in order to design and deliver the data pipelines necessary for our solutions. You will partner with the Decision Science Products, Decision Science, and client teams on critical projects. Other activities include planning, estimating, design, development, testing, production rollout and sustainment activities. You will also need to consult and collaborate with project team members, lead design reviews, do hands-on development, and communicate with colleagues and leaders.\n\nRequired Qualifications & Skills\n• 7+ years overall experience in a data engineering development capacity using a multiple environments (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion\n• Experience with a variety of GenAI models, tools, and concepts\n• 3+ years using, designing and building relational databases (preferably Snowflake or PostgreSQL)\n• 3+ years of experience leading and deploying code using a source control product such as GitLab/GitHub\n• 2+ years of experience with job scheduling software like Apache Airflow, Amazon MWAA, GitLab Runners or UC4\n• Multiple years of experience with ELT/ETL data pipeline development and maintenance\n• Multiple years of shown experience and expertise using SQL and Python\n• Experience using containerization technologies such as Docker or Kubernetes\n• Knowledgeable on cloud architecture and product offerings, preferably AWS\n• Understanding of Knowledge Graphs, Data Mesh, and other data sharing platforms\n• Experience translating project scope and high-level requirements into technical data engineering tasks\n• Experience defining solutions to sophisticated data engineering problems in support of advanced analytic processes\n• Experience collaborating with multiple project teams in a fast-paced environment\n• Experience with defining and estimating level-of-effort data engineering activities\n• Experience with project and sprint planning\n• Ability to communicate technical concepts and solutions to non-technical team members\n• Experience designing and building data structures to support requirements\n• Preferred Qualifications\n• Experience leading development of GenAI based systems including model selection, pipeline orchestration and deployment strategies\n• Experience defining GenAI architectures\n• Knowledgeable with Disney Parks attendance, reservations and/or products\n• Experience with cloud based technologies, preferably AWS EMR, EC2, and S3\n• Experience with advanced Snowflake offerings such as Snowpark, Data Exchange, Data Marketplace and Snowpipe\n\nEducation\n• Bachelor’s degree in computer science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study and/or equivalent work experience\n• Master’s degree preferred in computer science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study and/or equivalent work experience\n\n#DisneyTech\n\n#DisneyAnalytics\n\nwww.disneydatajobs.com",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Clermont, FL",
    "job_city": "Clermont",
    "job_state": "Florida",
    "job_country": "US",
    "job_latitude": 28.5549188,
    "job_longitude": -81.76620799999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DOh9syzGwM8nradswAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "7+ years overall experience in a data engineering development capacity using a multiple environments (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion",
        "Experience with a variety of GenAI models, tools, and concepts",
        "3+ years using, designing and building relational databases (preferably Snowflake or PostgreSQL)",
        "3+ years of experience leading and deploying code using a source control product such as GitLab/GitHub",
        "2+ years of experience with job scheduling software like Apache Airflow, Amazon MWAA, GitLab Runners or UC4",
        "Multiple years of experience with ELT/ETL data pipeline development and maintenance",
        "Multiple years of shown experience and expertise using SQL and Python",
        "Experience using containerization technologies such as Docker or Kubernetes",
        "Knowledgeable on cloud architecture and product offerings, preferably AWS",
        "Understanding of Knowledge Graphs, Data Mesh, and other data sharing platforms",
        "Experience translating project scope and high-level requirements into technical data engineering tasks",
        "Experience defining solutions to sophisticated data engineering problems in support of advanced analytic processes",
        "Experience collaborating with multiple project teams in a fast-paced environment",
        "Experience with defining and estimating level-of-effort data engineering activities",
        "Experience with project and sprint planning",
        "Ability to communicate technical concepts and solutions to non-technical team members",
        "Experience designing and building data structures to support requirements",
        "Experience leading development of GenAI based systems including model selection, pipeline orchestration and deployment strategies",
        "Experience defining GenAI architectures",
        "Knowledgeable with Disney Parks attendance, reservations and/or products",
        "Experience with cloud based technologies, preferably AWS EMR, EC2, and S3",
        "Experience with advanced Snowflake offerings such as Snowpark, Data Exchange, Data Marketplace and Snowpipe",
        "Bachelor’s degree in computer science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study and/or equivalent work experience"
      ],
      "Responsibilities": [
        "We develop, analyze, and execute strategies and improve the value proposition for our Guests, Cast Members, and Shareholders",
        "You will be responsible for planning and leading research and development related to advanced analytic data solutions, leveraging GenAI",
        "You will work with business and technology leaders to understand scope and requirements, business needs, and data from across the Disney company in order to design and deliver the data pipelines necessary for our solutions",
        "You will partner with the Decision Science Products, Decision Science, and client teams on critical projects",
        "Other activities include planning, estimating, design, development, testing, production rollout and sustainment activities",
        "You will also need to consult and collaborate with project team members, lead design reviews, do hands-on development, and communicate with colleagues and leaders"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-womenforhire-com-job-usa-clermont-fl-lead-data-engineer-213217",
    "_source": "new_jobs"
  },
  {
    "job_id": "19HGoOhNy4sBttccAAAAAA==",
    "job_title": "Data Engineer (Databricks)",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=7c1e5ae1ac6f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=7c1e5ae1ac6f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Data Consultant (Databricks).\n\nKey Responsibilities\n\nDesign and implement scalable data pipelines using Lakeflow and Databricks Asset Bundles\n\nDevelop high-volume ingestion pipelines and ensure data governance and security\n\nUtilize Terraform for managing Databricks workspace resources and support AWS deployment patterns\n\nRequired Qualifications\n\nHands-on experience with Databricks in production environments\n\nExpertise in PySpark and advanced SQL\n\nExperience with Delta Lake and data transformation frameworks\n\nFamiliarity with AWS infrastructure, including S3 and IAM\n\nTerraform experience specifically with Databricks and AWS resources",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Manchester, NH",
    "job_city": "Manchester",
    "job_state": "New Hampshire",
    "job_country": "US",
    "job_latitude": 42.9956397,
    "job_longitude": -71.4547891,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D19HGoOhNy4sBttccAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Hands-on experience with Databricks in production environments",
        "Expertise in PySpark and advanced SQL",
        "Experience with Delta Lake and data transformation frameworks",
        "Familiarity with AWS infrastructure, including S3 and IAM",
        "Terraform experience specifically with Databricks and AWS resources"
      ],
      "Responsibilities": [
        "Design and implement scalable data pipelines using Lakeflow and Databricks Asset Bundles",
        "Develop high-volume ingestion pipelines and ensure data governance and security",
        "Utilize Terraform for managing Databricks workspace resources and support AWS deployment patterns"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "q4w1sbGeUlXZqnSwAAAAAA==",
    "job_title": "(USA) Senior, Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": null,
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Talentify",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do... Role summary:\n\nAs a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights. You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets. Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance. This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment.\nAbout the team:\n\nWe’re a small but high‑impact Data Engineering team building the data foundations that power measurement and feature innovation for our advertising products. Operating at the intersection of engineering, analytics, and product strategy, we deliver scalable pipelines and accurate performance insights that drive data‑informed decisions. Our agility and end‑to‑end ownership set us apart—we move quickly, collaborate closely across functions, and build systems that directly influence how advertisers measure success and how new ad features evolve. If you want to build reliable, scalable data systems with visible impact, this is the team for you.\nWhat you'll do:\n• Design and build scalable data pipelines and models using Databricks, PySpark, and SQL.\n• Integrate and transform data from multiple sources with strong quality, observability, and governance.\n• Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions.\n• Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management.\n• Optimize pipelines for reliability, performance, and cost across cloud environments.\n• Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions.\n\nWhat you'll bring:\n• Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling.\n• Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow.\n• Solid understanding of cloud platforms, especially Google Cloud Platform.\n• Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform.\n• Strong grounding in data governance, data quality, and compliance best practices.\n• Ability to design resilient data architectures across warehouses, lakes, and streaming systems.\n• Proven skill in translating complex business needs into effective data solutions.\n• Familiarity with machine learning concepts and how they integrate with data engineering workflows.\n• A passion for finding ways to integrate AI and LLMs into daily engineering\nactivities and products, while not compromising simplicity and code\nmaintainability.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nDallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00\nDenver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years’ experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related\nfield.\n2 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n14901 Quorum Dr, Dallas, TX 75254-7521, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Rangeley, ME",
    "job_city": "Rangeley",
    "job_state": "Maine",
    "job_country": "US",
    "job_latitude": 44.965682099999995,
    "job_longitude": -70.6427102,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dq4w1sbGeUlXZqnSwAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment",
        "Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling",
        "Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow",
        "Solid understanding of cloud platforms, especially Google Cloud Platform",
        "Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform",
        "Strong grounding in data governance, data quality, and compliance best practices",
        "Ability to design resilient data architectures across warehouses, lakes, and streaming systems",
        "Proven skill in translating complex business needs into effective data solutions",
        "Familiarity with machine learning concepts and how they integrate with data engineering workflows",
        "A passion for finding ways to integrate AI and LLMs into daily engineering",
        "activities and products, while not compromising simplicity and code",
        "Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field",
        "Option 2: 5 years’ experience in",
        "software engineering or related field",
        "Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related",
        "2 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "Dallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00",
        "Denver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "As a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights",
        "You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets",
        "Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance",
        "Design and build scalable data pipelines and models using Databricks, PySpark, and SQL",
        "Integrate and transform data from multiple sources with strong quality, observability, and governance",
        "Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions",
        "Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management",
        "Optimize pipelines for reliability, performance, and cost across cloud environments",
        "Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talentify-io-job-usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845",
    "_source": "new_jobs"
  },
  {
    "job_id": "mLWbCE2BsJ1026TLAAAAAA==",
    "job_title": "Slalom Flex (Project Based)- Federal GCP Data Engineer",
    "employer_name": "Slalom",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTz83P7TkZM86wEdE65TUA9A-yqu267uumzStA3&s=0",
    "employer_website": "https://www.slalom.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "GCP Data Engineer (U.S. Citizenship Required)\n\nAbout Us\n\nSlalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six+ countries and 43+ markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 10,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.\n\nAbout The Role\n\nWe are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA. This role is a hands‑on engineering position working within a secure Google Cloud Platform environment to design, build, and optimize scalable data pipelines and analytics capabilities that enable high‑quality insights and operational excellence for our federal client.\n\nThis position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance.\n\nWhat You Will Do\n\nData Engineering & Cloud Development\n• Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage.\n• Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs.\n• Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation.\n\nClient Leadership & Delivery\n• Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements.\n• Translate technical designs and delivery timelines for both technical and non‑technical audiences.\n• Support modernization of legacy data environments into scalable GCP‑based architectures.\n• Ensure all solutions align with federal data governance, security, and performance standards.\n\nSolution Optimization & Innovation\n• Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling.\n• Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines.\n• Develop and maintain data lineage, metadata documentation, and enterprise data models.\n• Identify linkages across disparate datasets to build unified, interoperable data architectures.\n• Perform cleanup of existing datasets and transformation logic where needed.\n\nCollaboration & Team Leadership\n• Work closely with data architects, BI developers, cloud engineers, and data scientists.\n• Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning.\n• Track work in Jira and maintain documentation in Confluence.\n• Support testing, deployment, and quality assurance of data products.\n• Mentor junior data engineering team members and contribute to best-practice frameworks.\n\nMust-Have Qualifications\n• U.S. citizenship\n• Ability to obtain and maintain a federal Public Trust clearance\n• 3+ years of experience in cloud-based data engineering\n• Strong hands-on expertise with Google BigQuery\n• Proficiency in Python for pipeline development, automation, and cloud integration\n• Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar\n• Strong SQL skills, including data modeling and data quality testing\n• Experience with Git-based version control and CI/CD concepts\n• Familiarity with data governance, metadata management, and compliance considerations\n• Strong communication and stakeholder engagement skills\n\nNice-to-Have Skills\n• Experience supporting federal or regulated environments\n• Familiarity with Looker and downstream BI enablement\n• Understanding of ML workloads or data structures optimized for modeling\n• Experience with Agile/Scrum or SAFe\n• Knowledge of data quality frameworks and testing strategies\n• Exposure to GCP data governance tools such as Dataplex\n• Experience with serverless architectures (Cloud Functions, Cloud Run)\n• Familiarity with JavaScript for Dataform SQLX extensibility\n• Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)\n• Experience with dbt, Dataform, Databricks, or other analytics engineering tooling\n• Relevant GCP certifications\n\nCompensation And Benefits\n\nSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses.\n\nSlalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $55/hr to $75/hr. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.\n\nEEO and Accommodations\n\nSlalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration\n\nfor employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements.\n\nSlalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the\n\nselection process. Please advise the talent acquisition team if you require accommodations during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1769990400,
    "job_posted_at_datetime_utc": "2026-02-02T00:00:00.000Z",
    "job_location": "Oak Grove, NC",
    "job_city": "Oak Grove",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.9815344,
    "job_longitude": -78.8205619,
    "job_benefits": [
      "dental_coverage",
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmLWbCE2BsJ1026TLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55,
    "job_max_salary": 75,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "We are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA",
        "This position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance",
        "U.S. citizenship",
        "Ability to obtain and maintain a federal Public Trust clearance",
        "3+ years of experience in cloud-based data engineering",
        "Strong hands-on expertise with Google BigQuery",
        "Proficiency in Python for pipeline development, automation, and cloud integration",
        "Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar",
        "Strong SQL skills, including data modeling and data quality testing",
        "Experience with Git-based version control and CI/CD concepts",
        "Familiarity with data governance, metadata management, and compliance considerations",
        "Strong communication and stakeholder engagement skills",
        "Experience supporting federal or regulated environments",
        "Familiarity with Looker and downstream BI enablement",
        "Understanding of ML workloads or data structures optimized for modeling",
        "Experience with Agile/Scrum or SAFe",
        "Knowledge of data quality frameworks and testing strategies",
        "Exposure to GCP data governance tools such as Dataplex",
        "Experience with serverless architectures (Cloud Functions, Cloud Run)",
        "Familiarity with JavaScript for Dataform SQLX extensibility",
        "Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)",
        "Experience with dbt, Dataform, Databricks, or other analytics engineering tooling",
        "Relevant GCP certifications",
        "Slalom will also consider qualified applications with criminal histories, consistent with legal requirements"
      ],
      "Benefits": [
        "Slalom prides itself on helping team members thrive in their work and life",
        "As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability",
        "We also offer yearly $350 reimbursement account for any well-being-related expenses",
        "Slalom is committed to fair and equitable compensation practices",
        "For this position, the base salary pay range is $55/hr to $75/hr",
        "Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors",
        "The salary pay range is subject to change and may be modified at any time"
      ],
      "Responsibilities": [
        "Data Engineering & Cloud Development",
        "Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage",
        "Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs",
        "Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation",
        "Client Leadership & Delivery",
        "Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements",
        "Translate technical designs and delivery timelines for both technical and non‑technical audiences",
        "Support modernization of legacy data environments into scalable GCP‑based architectures",
        "Ensure all solutions align with federal data governance, security, and performance standards",
        "Solution Optimization & Innovation",
        "Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling",
        "Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines",
        "Develop and maintain data lineage, metadata documentation, and enterprise data models",
        "Identify linkages across disparate datasets to build unified, interoperable data architectures",
        "Perform cleanup of existing datasets and transformation logic where needed",
        "Collaboration & Team Leadership",
        "Work closely with data architects, BI developers, cloud engineers, and data scientists",
        "Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning",
        "Track work in Jira and maintain documentation in Confluence",
        "Support testing, deployment, and quality assurance of data products",
        "Mentor junior data engineering team members and contribute to best-practice frameworks",
        "Slalom welcomes and encourages applications from individuals with disabilities"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712",
    "_source": "new_jobs"
  },
  {
    "job_id": "06v4fScOsJJeq7PuAAAAAA==",
    "job_title": "Fall 2026 Co-op – Data Engineering - Full-time",
    "employer_name": "Keurig Dr Pepper",
    "employer_logo": null,
    "employer_website": "https://www.keurigdrpepper.com",
    "job_publisher": "Snagajob",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.snagajob.com/jobs/1159701017?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Snagajob",
        "apply_link": "https://www.snagajob.com/jobs/1159701017?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "**Job Overview:**\n• *Fall 2026 Co-op – Data Engineering**\n\nAs a **Fall 2026 Co-op – Data Engineering** in **Burlington, MA** at Keurig Dr Pepper (KDP), you will be a part of the beverage revolution. You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry. You will have real responsibilities and will be provided opportunities to grow professionally. Come learn what it takes to succeed at an industry-leading company and help contribute to our ongoing success. We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements. Candidate should have sound knowledge in SQL scripting.\n• *Shift/Schedule:**\n\n+ The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026\n\n+ Location: Burlington, MA\n\n+ Full-time; 40 hours per week\n\n+ Monday-Friday\n\n+ 8:00am until 5:00pm\n\n+ Hybrid; In office Tues- Thurs\n• *As a Data Engineering Co-op you will:**\n\n+ Create and develop optimal data pipeline using ETL tools\n\n+ Develop complex sql scripts to process and retrieve the data from data warehouse\n\n+ Assemble large, complex data sets that meet functional / non-functional business requirements\n\n+ Optimize the data delivery and date pipelines to process big data\n\n+ Work with stakeholders including Engineering & Product Management to assist with data-related needs\n• *Elements of the KDP 2026 Co-op Program include:**\n\n+ Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment\n\n+ Receive mentor support for your professional development\n\n+ Participate in meet & greets and lunch & learns with KDP executives and other organization leaders\n\n+ Receive professional development training such as networking, professional skills development and presenting\n\n+ Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business\n• *Total Rewards:**\n\n+ $31.00/ hour\n\n+ Paid bi-weekly\n\n+ $5,000.00 Sign-on Bonus, paid within first 30 days of employment\n• *Requirements:**\n\n+ Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields\n\n+ Available to work 40 hours per week (M-F, 8-5) in Burlington, MA\n\n+ Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines\n\n+ Strong skills in Microsoft Excel and PowerPoint\n\n+ Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database\n\n+ Build and optimize big data pipelines, architectures and data sets\n\n+ Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement\n\n+ A successful history of transforming, processing and extracting data from large data warehouse\n\n+ Strong communication skills including excellent listening, written, and verbal abilities\n\n+ Ability to work cross-functionally, be independently driven, and adapt to changes\n\nFollowing skills would be considered as a plus:\n\n+ Ability to create reports using Microsoft Power BI\n\n+ Knowledge in developing machine learning data models\n• *Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future. The Company cannot offer employment to F-1 (student) visa holders who require sponsorship in the future or cannot work now on a full time basis.\n\nWe incorporate HireVue, an automated interview tool, into our campus recruitment process. Please visit this link (https://www.hirevue.com/candidates/interview-tips) to learn more about HireVue and how to prepare. *We recommend checking with your campus career center for additional preparation resources such as InterviewStream, Big Interview and more\n• *Company Overview:**\n\nKeurig Dr Pepper (NASDAQ: KDP) is a leading beverage company in North America, with a portfolio of more than 125 owned, licensed and partners brands and powerful distribution capabilities to provide a beverage for every need, anytime, anywhere. We operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. We work with big, exciting beverage brands and the #1 single-serve coffee brewing system in North America at KDP, and we have fun doing it!\n\nTogether, we have built a leading beverage company in North America offering hot and cold beverages together at scale. Whatever your area of expertise, at KDP you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. Will you join us?\n\nWe strive to be an employer of choice, providing a culture and opportunities that empower our team of ~29,000 employees to grow and develop. We offer robust benefits to support your health and wellness as well as your personal and financial well-being. We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work.\n\nKeurig Dr Pepper is an equal opportunity employer and recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *A.I. Disclosure:**\n\nKDP uses artificial intelligence to assist with initial resume screening and candidate matching. This technology helps us efficiently identify candidates whose qualifications align with our open roles. If you prefer not to have your application processed using artificial intelligence, you may opt out by emailing your resume and qualifications directly to kdpjobs@kdrp.com .\n\nKeurig Dr Pepper is an equal opportunity employer and affirmatively seeks diversity in its workforce. Keurig Dr Pepper recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *Job Overview:**\n• *Fall 2026 Co-op – Data Engineering**\n\nAs a **Fall 2026 Co-op – Data Engineering** in **Burlington, MA** at Keurig Dr Pepper (KDP), you will be a part of the beverage revolution. You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry. You will have real responsibilities and will be provided opportunities to grow professionally. Come learn what it takes to succeed at an industry-leading company and help contribute to our ongoing success. We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements. Candidate should have sound knowledge in SQL scripting.\n• *Shift/Schedule:**\n\n+ The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026\n\n+ Location: Burlington, MA\n\n+ Full-time; 40 hours per week\n\n+ Monday-Friday\n\n+ 8:00am until 5:00pm\n\n+ Hybrid; In office Tues- Thurs\n• *As a Data Engineering Co-op you will:**\n\n+ Create and develop optimal data pipeline using ETL tools\n\n+ Develop complex sql scripts to process and retrieve the data from data warehouse\n\n+ Assemble large, complex data sets that meet functional / non-functional business requirements\n\n+ Optimize the data delivery and date pipelines to process big data\n\n+ Work with stakeholders including Engineering & Product Management to assist with data-related needs\n• *Elements of the KDP 2026 Co-op Program include:**\n\n+ Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment\n\n+ Receive mentor support for your professional development\n\n+ Participate in meet & greets and lunch & learns with KDP executives and other organization leaders\n\n+ Receive professional development training such as networking, professional skills development and presenting\n\n+ Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business\n• *Total Rewards:**\n\n+ $31.00/ hour\n\n+ Paid bi-weekly\n\n+ $5,000.00 Sign-on Bonus, paid within first 30 days of employment\n• *Requirements:**\n\n+ Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields\n\n+ Available to work 40 hours per week (M-F, 8-5) in Burlington, MA\n\n+ Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines\n\n+ Strong skills in Microsoft Excel and PowerPoint\n\n+ Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database\n\n+ Build and optimize big data pipelines, architectures and data sets\n\n+ Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement\n\n+ A successful history of transforming, processing and extracting data from large data warehouse\n\n+ Strong communication skills including excellent listening, written, and verbal abilities\n\n+ Ability to work cross-functionally, be independently driven, and adapt to changes\n\nFollowing skills would be considered as a plus:\n\n+ Ability to create reports using Microsoft Power BI\n\n+ Knowledge in developing machine learning data models\n• *Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future. The Company cannot offer employment to F-1 (student) visa holders who require sponsorship in the future or cannot work now on a full time basis.\n\nWe incorporate HireVue, an automated interview tool, into our campus recruitment process. Please visit this link (https://www.hirevue.com/candidates/interview-tips) to learn more about HireVue and how to prepare. *We recommend checking with your campus career center for additional preparation resources such as InterviewStream, Big Interview and more\n• *Company Overview:**\n\nKeurig Dr Pepper (NASDAQ: KDP) is a leading beverage company in North America, with a portfolio of more than 125 owned, licensed and partners brands and powerful distribution capabilities to provide a beverage for every need, anytime, anywhere. We operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. We work with big, exciting beverage brands and the #1 single-serve coffee brewing system in North America at KDP, and we have fun doing it!\n\nTogether, we have built a leading beverage company in North America offering hot and cold beverages together at scale. Whatever your area of expertise, at KDP you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. Will you join us?\n\nWe strive to be an employer of choice, providing a culture and opportunities that empower our team of ~29,000 employees to grow and develop. We offer robust benefits to support your health and wellness as well as your personal and financial well-being. We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work.\n\nKeurig Dr Pepper is an equal opportunity employer and recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *A.I. Disclosure:**\n\nKDP uses artificial intelligence to assist with initial resume screening and candidate matching. This technology helps us efficiently identify candidates whose qualifications align with our open roles. If you prefer not to have your application processed using artificial intelligence, you may opt out by emailing your resume and qualifications directly to kdpjobs@kdrp.com .\n\nKeurig Dr Pepper is an equal opportunity employer and affirmatively seeks diversity in its workforce. Keurig Dr Pepper recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.",
    "job_is_remote": false,
    "job_posted_at": "8 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "Burlington, MA",
    "job_city": "Burlington",
    "job_state": "Massachusetts",
    "job_country": "US",
    "job_latitude": 42.504716099999996,
    "job_longitude": -71.19562049999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D06v4fScOsJJeq7PuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements",
        "Candidate should have sound knowledge in SQL scripting",
        "*As a Data Engineering Co-op you will:*",
        "Create and develop optimal data pipeline using ETL tools",
        "Develop complex sql scripts to process and retrieve the data from data warehouse",
        "Assemble large, complex data sets that meet functional / non-functional business requirements",
        "Optimize the data delivery and date pipelines to process big data",
        "*Elements of the KDP 2026 Co-op Program include:**",
        "Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment",
        "Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields",
        "Available to work 40 hours per week (M-F, 8-5) in Burlington, MA",
        "Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines",
        "Strong skills in Microsoft Excel and PowerPoint",
        "Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database",
        "Build and optimize big data pipelines, architectures and data sets",
        "A successful history of transforming, processing and extracting data from large data warehouse",
        "Strong communication skills including excellent listening, written, and verbal abilities",
        "Ability to work cross-functionally, be independently driven, and adapt to changes",
        "Ability to create reports using Microsoft Power BI",
        "Knowledge in developing machine learning data models",
        "*Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future",
        "*Fall 2026 Co-op – Data Engineering**",
        "We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements",
        "Candidate should have sound knowledge in SQL scripting",
        "The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026",
        "Full-time; 40 hours per week",
        "Hybrid; In office Tues- Thurs",
        "*As a Data Engineering Co-op you will:*",
        "Create and develop optimal data pipeline using ETL tools",
        "Develop complex sql scripts to process and retrieve the data from data warehouse",
        "Assemble large, complex data sets that meet functional / non-functional business requirements",
        "Optimize the data delivery and date pipelines to process big data",
        "*Elements of the KDP 2026 Co-op Program include:**",
        "Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment",
        "Receive professional development training such as networking, professional skills development and presenting",
        "Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business",
        "Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields",
        "Available to work 40 hours per week (M-F, 8-5) in Burlington, MA",
        "Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines",
        "Strong skills in Microsoft Excel and PowerPoint",
        "Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database",
        "Build and optimize big data pipelines, architectures and data sets",
        "Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement",
        "A successful history of transforming, processing and extracting data from large data warehouse",
        "Strong communication skills including excellent listening, written, and verbal abilities",
        "Ability to work cross-functionally, be independently driven, and adapt to changes",
        "Ability to create reports using Microsoft Power BI",
        "Knowledge in developing machine learning data models",
        "*Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future"
      ],
      "Benefits": [
        "$31.00/ hour",
        "Paid bi-weekly",
        "$5,000.00 Sign-on Bonus, paid within first 30 days of employment",
        "We offer robust benefits to support your health and wellness as well as your personal and financial well-being",
        "We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work",
        "*Total Rewards:**",
        "$31.00/ hour",
        "Paid bi-weekly",
        "$5,000.00 Sign-on Bonus, paid within first 30 days of employment",
        "We offer robust benefits to support your health and wellness as well as your personal and financial well-being",
        "We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work"
      ],
      "Responsibilities": [
        "You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry",
        "You will have real responsibilities and will be provided opportunities to grow professionally",
        "Full-time; 40 hours per week",
        "8:00am until 5:00pm",
        "Work with stakeholders including Engineering & Product Management to assist with data-related needs",
        "Receive mentor support for your professional development",
        "Participate in meet & greets and lunch & learns with KDP executives and other organization leaders",
        "Receive professional development training such as networking, professional skills development and presenting",
        "Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business",
        "Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement",
        "You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry",
        "You will have real responsibilities and will be provided opportunities to grow professionally",
        "8:00am until 5:00pm",
        "Work with stakeholders including Engineering & Product Management to assist with data-related needs",
        "Receive mentor support for your professional development",
        "Participate in meet & greets and lunch & learns with KDP executives and other organization leaders"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "www-snagajob-com-jobs-1159701017",
    "_source": "new_jobs"
  },
  {
    "job_id": "xu-VGehYTuGfUSZxAAAAAA==",
    "job_title": "Data Engineer - Python",
    "employer_name": "Apptad Inc",
    "employer_logo": null,
    "employer_website": "https://apptad.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Overview\n\nWe are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions. This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment.\n\nKey Responsibilities\n• Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory.\n• Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions.\n• Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing.\n• Implement data migration and transformation workflows leveraging Azure cloud services.\n• Ensure data quality, performance tuning, and optimization of data pipelines.\n• Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects.\n• Apply best practices for data governance, privacy, and security within Azure environments.\n• Lead the design and implementation of both batch and streaming data pipelines.\n• Provide technical leadership and mentorship to team members.\n• Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions.\n• Troubleshoot and resolve issues to ensure high availability and reliability.\n• Participate in architectural discussions and develop coding standards and security guidelines.\n• Drive automation of data workflows and improve operational efficiency.\n• Support production environments with timely incident response and root cause analysis.\n\nRequired Qualifications\n• 5 to 7 years of relevant experience in data engineering or related fields.\n• Expertise in Python programming.\n• Proficiency in Azure Data Factory and its ecosystem.\n\nPreferred Qualifications\n• Familiarity with Azure Data Lake and other Azure cloud services.",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "Cockrell Hill, TX",
    "job_city": "Cockrell Hill",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7362421,
    "job_longitude": -96.8869481,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dxu-VGehYTuGfUSZxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions",
        "5 to 7 years of relevant experience in data engineering or related fields",
        "Expertise in Python programming",
        "Proficiency in Azure Data Factory and its ecosystem"
      ],
      "Responsibilities": [
        "This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment",
        "Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory",
        "Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions",
        "Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing",
        "Implement data migration and transformation workflows leveraging Azure cloud services",
        "Ensure data quality, performance tuning, and optimization of data pipelines",
        "Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects",
        "Apply best practices for data governance, privacy, and security within Azure environments",
        "Lead the design and implementation of both batch and streaming data pipelines",
        "Provide technical leadership and mentorship to team members",
        "Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions",
        "Troubleshoot and resolve issues to ensure high availability and reliability",
        "Participate in architectural discussions and develop coding standards and security guidelines",
        "Drive automation of data workflows and improve operational efficiency",
        "Support production environments with timely incident response and root cause analysis"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-2vqttgf6jgfthenvnt8bjz",
    "_source": "new_jobs"
  },
  {
    "job_id": "UBaAiKRJzBskW0c3AAAAAA==",
    "job_title": "Distinguished Data Engineer- Card",
    "employer_name": "Capital One",
    "employer_logo": null,
    "employer_website": "https://www.capitalone.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time and Part-time",
    "job_employment_types": [
      "FULLTIME",
      "PARTTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/data-management?id=2455126404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/data-management?id=2455126404&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/mclean/virginia/info_technology/4669622749/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Join to apply for the Distinguished Data Engineer- Card role at Capital One\n\nDistinguished Data Engineers are individual contributors who strive to be diverse in thought so we visualize the problem space. At Capital One, we believe diversity of thought strengthens our ability to influence, collaborate and provide the most innovative solutions across organizational boundaries. Distinguished Engineers will significantly impact our trajectory and devise clear roadmaps to deliver next generation technology solutions.\nDeep technical experts and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices Visionaries, collaborating on Capital One’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates Role models and mentors, helping to coach and strengthen the technical expertise and know-how of our engineering and product community Evangelists, both internally and externally, helping to elevate the Distinguished Engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities\n\nAbout the team: Capital One’s Card Core Architecture team is responsible for Card modernization’s data strategy, including data abstraction, data standardization, data products, GraphQL and API services to deliver reads and writes to clients, and analytical data strategy to enable our business to drive value across the customer lifecycle with real time intelligence and agility.\n\nIn this role, you will work on delivering Proof-of-concepts, actionable architectures and designs with hands-on deep technical expertise. You will act as a bridge between technical delivery and product management. You will work with other Distinguished Engineers to detail high level designs to help software engineers develop the products as envisioned. You will provide mentorship and strategic guidance to our engineer, product, and leadership groups.\n\nResponsibilities:\nBuild awareness, increase knowledge and drive adoption of modern technologies, sharing consumer and engineering benefits to gain buy-in Strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader Capital One team Promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in a unified manner Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent\n\nBasic Qualifications:\nBachelor’s Degree At least 7 years of experience in data engineering At least 3 years of experience in data architecture At least 2 years of experience building applications in AWS\n\nPreferred Qualifications:\nMasters’ Degree 9+ years of experience in data architecture, engineering and delivery 5+ years of experience developing in Python, Java, Scala or SQL 3+ years of architecting experience with AWS services such as S3, Glue, Aurora Global, DynamoDB, Kinesis, Lambda 3+ years of hands-on experience with data lakes including AVRO, Parquet, Iceberg, Delta, Spark, Kafka, Flink technologies 3+ years of data modeling experience 2+ years of experience with ontology standards for defining a domain 1+ year of experience deploying machine learning models 3+ years of experience implementing big data processing solutions on AWS 3+ years of experience in building highly resilient distributed data systems\n\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Salaries for part-time roles will be prorated based upon hours worked. Locations include Chicago, IL; McLean, VA; New York, NY; Plano, TX; Richmond, VA; San Francisco, CA; Wilmington, DE; and other locations with pay ranges adjusted accordingly. The actual offer will be reflected in the candidate’s offer letter. This role is eligible for performance-based incentive compensation.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits. Eligibility varies by status. This role is expected to accept applications for a minimum of 5 business days.\n\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable laws. Capital One promotes a drug-free workplace. Details on accommodations for applicants with disabilities are available from Capital One Recruiting.\n\nSeniority level: Mid-Senior level\n\nEmployment type: Full-time\n\nJob function: Information Technology\n\nReferrals increase your chances of interviewing at Capital One by 2x\n\nGet notified about new Data Engineer jobs in McLean, VA.\n#J-18808-Ljbffr",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770508800,
    "job_posted_at_datetime_utc": "2026-02-08T00:00:00.000Z",
    "job_location": "McLean, VA",
    "job_city": "McLean",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 38.9338676,
    "job_longitude": -77.1772604,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DUBaAiKRJzBskW0c3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-data-management",
    "_source": "new_jobs"
  },
  {
    "job_id": "ybwkukVOvvN7CZ4IAAAAAA==",
    "job_title": "Data Engineer Prin",
    "employer_name": "American Electric Power",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0RrFTOdrhzngcyQL-2zVSyVH1kAYBiD2Gj2L_&s=0",
    "employer_website": null,
    "job_publisher": "Tallo",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Tallo",
        "apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This job listing in Franklin - OH has been recently added. Tallo will add a summary here for this job shortly.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Franklin, OH",
    "job_city": "Franklin",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.5589474,
    "job_longitude": -84.30410739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DybwkukVOvvN7CZ4IAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "tallo-com-jobs-technology-data-engineer-oh-franklin-data-engineer-prin-5d6c21303973",
    "_source": "new_jobs"
  },
  {
    "job_id": "Po2HBu6hdvNEmq0-AAAAAA==",
    "job_title": "Senior Data Engineer",
    "employer_name": "Visium SA",
    "employer_logo": null,
    "employer_website": "https://www.visium.com",
    "job_publisher": "Snagajob",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.snagajob.com/jobs/1066921924?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Snagajob",
        "apply_link": "https://www.snagajob.com/jobs/1066921924?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Title: Senior Data Engineer\n\nType: Full-time\n\nLocation: Valencia or Barcelona\n\nAbout us\n\nAt Visium, we enable enterprise executives in defining their AI & Data strategy, execute large scale transformations and implement AI across operations, ensuring their organization becomes future-proof.\n\nWith expertise in strategy, architecture, cloud engineering, analytics, artificial intelligence and machine learning, we empower our clients to unleash and scale the power of their data.\n\nWe’re on a mission to pioneer a bright future and build future-proof and ethical organizations . Join the curious, the ambitious, the doers, the good-hearted, the ones who build a world we’re all in awe of – our Visiumees.\n\nReady to become one?\n\nRole\n\nAs a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets. Starting from raw data, you will help build robust systems to make this data usable and accessible.\n\nAs a Senior Data Engineer, you will be responsible for:\n• Assess and understand client's data landscape and assess data quality\n• Work closely with business and IT stakeholders to understand business requirements\n• Participating in technical sales activities\n• Work with high volume of heterogeneous data with distributed systems\n• Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms\n• Build and manage data infrastructure, including data storage, processing, and access\n• Implement data models and schema designs to support data analysis and reporting needs\n• Develop and maintain data quality and data governance frameworks\n• Create data visualizations and reports to communicate insights to stakeholders\n• Stay up to date with state-of-the-art data processing and analysis technologies.\n• Troubleshoot and debug data-related issues\n• Contribute to an exciting environment with a challenging team\n\nRequirements\n\nWe are looking for\n• 5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake.\n• Proficient in cloud technologies (Azure, AWS) and related data services.\n• Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight.\n• Experienced with distributed data processing frameworks like Spark.\n• Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes.\n• Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns.\n• Possesses a growth mindset, excels in problem-solving, and\n• Fluent communication both in English.\n\nThe following are a plus\n• You own one of the main Data and AI platforms certification (Databricks, Snowflake)\n• Familiarity with dbt and how to use it effectively\n• Experience in consulting is a plus\n\nBenefits\n\nWhat we offer\n• A competitive compensation package\n• A yearly education budget to steep your learning curve\n• A yearly sport budget because a fit body leads to a fit mind\n• A flexible working culture because your work-life balance matters to us\n• A position that enables you to have an impact on 1’000s of people, and the whole company's growth.\n• An international, knowledgeable, and passionate team with a strong collaborative mindset\n\nCheck our and to learn more about us & don’t hesitate to contact us if you have any questions.\n\nTitle: Senior Data Engineer\n\nType: Full-time\n\nLocation: Valencia or Barcelona\n\nAbout us\n\nAt Visium, we enable enterprise executives in defining their AI & Data strategy, execute large scale transformations and implement AI across operations, ensuring their organization becomes future-proof.\n\nWith expertise in strategy, architecture, cloud engineering, analytics, artificial intelligence and machine learning, we empower our clients to unleash and scale the power of their data.\n\nWe’re on a mission to pioneer a bright future and build future-proof and ethical organizations . Join the curious, the ambitious, the doers, the good-hearted, the ones who build a world we’re all in awe of – our Visiumees.\n\nReady to become one?\n\nRole\n\nAs a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets. Starting from raw data, you will help build robust systems to make this data usable and accessible.\n\nAs a Senior Data Engineer, you will be responsible for:\n• Assess and understand client's data landscape and assess data quality\n• Work closely with business and IT stakeholders to understand business requirements\n• Participating in technical sales activities\n• Work with high volume of heterogeneous data with distributed systems\n• Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms\n• Build and manage data infrastructure, including data storage, processing, and access\n• Implement data models and schema designs to support data analysis and reporting needs\n• Develop and maintain data quality and data governance frameworks\n• Create data visualizations and reports to communicate insights to stakeholders\n• Stay up to date with state-of-the-art data processing and analysis technologies.\n• Troubleshoot and debug data-related issues\n• Contribute to an exciting environment with a challenging team\n\nRequirements\n\nWe are looking for\n• 5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake.\n• Proficient in cloud technologies (Azure, AWS) and related data services.\n• Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight.\n• Experienced with distributed data processing frameworks like Spark.\n• Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes.\n• Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns.\n• Possesses a growth mindset, excels in problem-solving, and\n• Fluent communication both in English.\n\nThe following are a plus\n• You own one of the main Data and AI platforms certification (Databricks, Snowflake)\n• Familiarity with dbt and how to use it effectively\n• Experience in consulting is a plus\n\nBenefits\n\nWhat we offer\n• A competitive compensation package\n• A yearly education budget to steep your learning curve\n• A yearly sport budget because a fit body leads to a fit mind\n• A flexible working culture because your work-life balance matters to us\n• A position that enables you to have an impact on 1’000s of people, and the whole company's growth.\n• An international, knowledgeable, and passionate team with a strong collaborative mindset\n\nCheck our and to learn more about us & don’t hesitate to contact us if you have any questions.",
    "job_is_remote": false,
    "job_posted_at": "17 hours ago",
    "job_posted_at_timestamp": 1771034400,
    "job_posted_at_datetime_utc": "2026-02-14T02:00:00.000Z",
    "job_location": "Tome-Adelino, NM",
    "job_city": "Tome-Adelino",
    "job_state": "New Mexico",
    "job_country": "US",
    "job_latitude": 34.7318311,
    "job_longitude": -106.7175669,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DPo2HBu6hdvNEmq0-AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake",
        "Proficient in cloud technologies (Azure, AWS) and related data services",
        "Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight",
        "Experienced with distributed data processing frameworks like Spark",
        "Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes",
        "Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns",
        "Possesses a growth mindset, excels in problem-solving, and",
        "Fluent communication both in English",
        "You own one of the main Data and AI platforms certification (Databricks, Snowflake)",
        "Familiarity with dbt and how to use it effectively",
        "We are looking for",
        "5+ years of experience building data assets and pipelines using platforms like Databricks and Snowflake",
        "Proficient in cloud technologies (Azure, AWS) and related data services",
        "Skilled in data visualization tools such as Tableau, PowerBI, and Quicksight",
        "Experienced with distributed data processing frameworks like Spark",
        "Knowledgeable in designing and maintaining production-level data warehouses and provisioning data lakes",
        "Familiar with software development principles, proficient in Python, and well-versed in data modeling and ETL design patterns",
        "Possesses a growth mindset, excels in problem-solving, and",
        "Fluent communication both in English",
        "You own one of the main Data and AI platforms certification (Databricks, Snowflake)",
        "Familiarity with dbt and how to use it effectively"
      ],
      "Benefits": [
        "A competitive compensation package",
        "A yearly education budget to steep your learning curve",
        "A yearly sport budget because a fit body leads to a fit mind",
        "A flexible working culture because your work-life balance matters to us",
        "A position that enables you to have an impact on 1’000s of people, and the whole company's growth",
        "An international, knowledgeable, and passionate team with a strong collaborative mindset",
        "A competitive compensation package",
        "A yearly education budget to steep your learning curve",
        "A yearly sport budget because a fit body leads to a fit mind",
        "A flexible working culture because your work-life balance matters to us",
        "A position that enables you to have an impact on 1’000s of people, and the whole company's growth",
        "An international, knowledgeable, and passionate team with a strong collaborative mindset"
      ],
      "Responsibilities": [
        "As a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets",
        "Starting from raw data, you will help build robust systems to make this data usable and accessible",
        "As a Senior Data Engineer, you will be responsible for:",
        "Assess and understand client's data landscape and assess data quality",
        "Work closely with business and IT stakeholders to understand business requirements",
        "Participating in technical sales activities",
        "Work with high volume of heterogeneous data with distributed systems",
        "Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms",
        "Build and manage data infrastructure, including data storage, processing, and access",
        "Implement data models and schema designs to support data analysis and reporting needs",
        "Develop and maintain data quality and data governance frameworks",
        "Create data visualizations and reports to communicate insights to stakeholders",
        "Stay up to date with state-of-the-art data processing and analysis technologies",
        "Troubleshoot and debug data-related issues",
        "Contribute to an exciting environment with a challenging team",
        "As a Senior Data Engineer, you will work with various stakeholders to provide business leaders with valuable insights from large and complex datasets",
        "Starting from raw data, you will help build robust systems to make this data usable and accessible",
        "Assess and understand client's data landscape and assess data quality",
        "Work closely with business and IT stakeholders to understand business requirements",
        "Participating in technical sales activities",
        "Work with high volume of heterogeneous data with distributed systems",
        "Design, develop, and maintain scalable and efficient data assets, including data pipelines, data lakes, data warehouses, and ETL processes based on well-known data platforms",
        "Build and manage data infrastructure, including data storage, processing, and access",
        "Implement data models and schema designs to support data analysis and reporting needs",
        "Develop and maintain data quality and data governance frameworks",
        "Create data visualizations and reports to communicate insights to stakeholders",
        "Stay up to date with state-of-the-art data processing and analysis technologies",
        "Troubleshoot and debug data-related issues",
        "Contribute to an exciting environment with a challenging team"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-snagajob-com-jobs-1066921924",
    "_source": "new_jobs"
  },
  {
    "job_id": "mfrhp-8GQszS1bhlAAAAAA==",
    "job_title": "GCP Data Engineer",
    "employer_name": "Infosys",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSLqb-rFsLhWFLTkYho0FBp_JqLcerpRgWA8ZU_&s=0",
    "employer_website": "https://www.infosys.com",
    "job_publisher": "WhatJobs",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.whatjobs.com/jobs/gcp-data-engineer?id=2434505081&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "WhatJobs",
        "apply_link": "https://www.whatjobs.com/jobs/gcp-data-engineer?id=2434505081&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Job Description\n\nInfosys is seeking a GCP data engineer . In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards. You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.\n\nRequired Qualifications: Candidate must be located within commuting distance of Richardson, TX or be willing to relocate to the area. This position may require travel in the US Bachelor's degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply. Infosys is unable to provide immigration sponsorship for this role at this time At least 2 years of Information Technology experience. Experience working with technologies like - GCP with data engineering - data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query. ETL development experience with strong SQL background such as Python/R, Scala, Java, Hive, Spark, Kafka Strong knowledge on Python Program development to build reusable frameworks, enhance existing frameworks. Application build experience with core GCP Services like Dataproc, GKE, Composer, Preferred Qualifications: Good knowledge on Google Big Query, using advance SQL programing techniques to build Big Query Data sets in Ingestion and Transformation layer. Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring. Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams. Experience and desire to work in a global delivery environment. Along with competitive pay, as a full-time Infosys employee you are also eligible for the following benefits: - Medical/Dental/Vision/Life Insurance Long-term/Short-term Disability Health and Dependent Care Reimbursement Accounts Insurance (Accident, Critical Illness, Hospital Indemnity, Legal) 401(k) plan and contributions dependent on salary level Paid holidays plus Paid Time Off\nThe job entails sitting as well as working at a computer for extended periods of time. Should be able to communicate by telephone, email or face to face.\n\nEEO/About Us\n\nAbout Us\nInfosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.\n\nInfosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse/family member of a protected veteran; or disability.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1769990400,
    "job_posted_at_datetime_utc": "2026-02-02T00:00:00.000Z",
    "job_location": "Blue Ridge, TX",
    "job_city": "Blue Ridge",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 33.2978909,
    "job_longitude": -96.4016498,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dmfrhp-8GQszS1bhlAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Required Qualifications: Candidate must be located within commuting distance of Richardson, TX or be willing to relocate to the area",
        "This position may require travel in the US Bachelor's degree or foreign equivalent required from an accredited institution",
        "Will also consider three years of progressive experience in the specialty in lieu of every year of education",
        "Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply",
        "Infosys is unable to provide immigration sponsorship for this role at this time At least 2 years of Information Technology experience",
        "Experience working with technologies like - GCP with data engineering - data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query",
        "ETL development experience with strong SQL background such as Python/R, Scala, Java, Hive, Spark, Kafka Strong knowledge on Python Program development to build reusable frameworks, enhance existing frameworks",
        "Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring",
        "Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams",
        "Experience and desire to work in a global delivery environment",
        "Should be able to communicate by telephone, email or face to face"
      ],
      "Benefits": [
        "Along with competitive pay, as a full-time Infosys employee you are also eligible for the following benefits: - Medical/Dental/Vision/Life Insurance Long-term/Short-term Disability Health and Dependent Care Reimbursement Accounts Insurance (Accident, Critical Illness, Hospital Indemnity, Legal) 401(k) plan and contributions dependent on salary level Paid holidays plus Paid Time Off"
      ],
      "Responsibilities": [
        "In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards",
        "You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle",
        "You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued",
        "The job entails sitting as well as working at a computer for extended periods of time"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-whatjobs-com-jobs-gcp-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "_fJ4esw2CSBo0axNAAAAAA==",
    "job_title": "Google Cloud Platform Data Engineer (Locals Only)",
    "employer_name": "Jobs via Dice",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/4RAiIOB4Wvohn4qafdPor?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/4RAiIOB4Wvohn4qafdPor?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Overview\n\nDice is the leading career destination for tech experts. Our client, VeridianTech, is seeking a Google Cloud Platform Data Engineer to join their team in Mountain View, CA (Onsite) for a duration of 12+ months. In this role, you’ll develop and enhance Python frameworks and design robust data pipelines that power advanced data processing, quality, and machine learning operations. Apply via Dice today!\n\nKey Responsibilities\n• Develop and enhance Python frameworks and libraries to support data processing, quality, lineage, governance, analysis, and machine learning operations.\n• Design, build, and maintain scalable and efficient data pipelines on Google Cloud Platform.\n• Implement robust monitoring, logging, and alerting systems to ensure the reliability and stability of data infrastructure.\n• Build scalable batch pipelines leveraging BigQuery, Dataflow and Airflow/Composer scheduler/executor framework on Google Cloud Platform.\n• Build data pipelines leveraging Scala, Pub/Sub, Akka, and Dataflow on Google Cloud Platform.\n• Design data models for optimal storage and retrieval to support machine learning modeling using technologies like Bigtable and Vertex Feature Store.\n• Contribute to shared Data Engineering tooling and standards to improve productivity and quality for the team.\n\nRequired Qualifications\n• Python Expertise: Write and maintain Python frameworks and libraries to support data processing and integration tasks.\n• Code Management: Use Git and GitHub for source control, code reviews, and version management.\n• Google Cloud Platform Proficiency: Extensive experience working with GCP services (e.g., BigQuery, Cloud Dataflow, Pub/Sub, Cloud Storage).\n• Python Mastery: Proficient in Python with experience in optimizing data processing frameworks and libraries.\n• Software Engineering: Strong understanding of best practices including version control, collaborative development, code reviews, and CI/CD.\n• Data Management: Deep knowledge of data modeling, ETL/ELT, and data warehousing concepts.\n• Problem-Solving: Excellent problem-solving skills with the ability to tackle complex data engineering challenges.\n• Communication: Ability to explain complex technical details to non-technical stakeholders.\n• Data Science Stack: Proficiency in data analysis with tools such as Jupyter Notebook, pandas, and NumPy.\n• Frameworks/Tools: Familiarity with machine learning and data processing tools such as TensorFlow, Apache Spark, and scikit-learn.\n• Education: Bachelor’s or master’s degree in Computer Science, Engineering, Computer Information Systems, Mathematics, Physics, or a related field, or equivalent software development training.",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Mountain View, CA",
    "job_city": "Mountain View",
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 37.390026399999996,
    "job_longitude": -122.0812304,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D_fJ4esw2CSBo0axNAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Python Expertise: Write and maintain Python frameworks and libraries to support data processing and integration tasks",
        "Code Management: Use Git and GitHub for source control, code reviews, and version management",
        "Google Cloud Platform Proficiency: Extensive experience working with GCP services (e.g., BigQuery, Cloud Dataflow, Pub/Sub, Cloud Storage)",
        "Python Mastery: Proficient in Python with experience in optimizing data processing frameworks and libraries",
        "Software Engineering: Strong understanding of best practices including version control, collaborative development, code reviews, and CI/CD",
        "Data Management: Deep knowledge of data modeling, ETL/ELT, and data warehousing concepts",
        "Problem-Solving: Excellent problem-solving skills with the ability to tackle complex data engineering challenges",
        "Communication: Ability to explain complex technical details to non-technical stakeholders",
        "Data Science Stack: Proficiency in data analysis with tools such as Jupyter Notebook, pandas, and NumPy",
        "Frameworks/Tools: Familiarity with machine learning and data processing tools such as TensorFlow, Apache Spark, and scikit-learn",
        "Education: Bachelor’s or master’s degree in Computer Science, Engineering, Computer Information Systems, Mathematics, Physics, or a related field, or equivalent software development training"
      ],
      "Responsibilities": [
        "In this role, you’ll develop and enhance Python frameworks and design robust data pipelines that power advanced data processing, quality, and machine learning operations",
        "Develop and enhance Python frameworks and libraries to support data processing, quality, lineage, governance, analysis, and machine learning operations",
        "Design, build, and maintain scalable and efficient data pipelines on Google Cloud Platform",
        "Implement robust monitoring, logging, and alerting systems to ensure the reliability and stability of data infrastructure",
        "Build scalable batch pipelines leveraging BigQuery, Dataflow and Airflow/Composer scheduler/executor framework on Google Cloud Platform",
        "Build data pipelines leveraging Scala, Pub/Sub, Akka, and Dataflow on Google Cloud Platform",
        "Design data models for optimal storage and retrieval to support machine learning modeling using technologies like Bigtable and Vertex Feature Store",
        "Contribute to shared Data Engineering tooling and standards to improve productivity and quality for the team"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-4raiiob4wvohn4qafdpor",
    "_source": "new_jobs"
  }
]