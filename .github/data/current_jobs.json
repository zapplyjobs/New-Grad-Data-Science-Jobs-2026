[
  {
    "job_id": "DI_gmnxno-NVwm8wAAAAAA==",
    "job_title": "Senior Data Engineer (Informatica/Databricks)",
    "employer_name": "CACI",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSEIAi6XHhq4EhegiHCD5u672hDATM_CMWOaqCb&s=0",
    "employer_website": null,
    "job_publisher": "CACI Careers",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "CACI Careers",
        "apply_link": "https://careers.caci.com/global/en/job/CACIGLOBAL322083EXTERNALENGLOBAL?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Indeed",
        "apply_link": "https://www.indeed.com/viewjob?jk=9abc0f815d739892&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Infinitive-Inc/Job/Senior-Data-Engineer-(Python-PySpark-AWS)/-in-Ashburn,VA?jid=80f99d04bc706cc2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "TekSynap Job Openings - ICIMS",
        "apply_link": "https://careers-teksynap.icims.com/jobs/8146/senior-data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Built In",
        "apply_link": "https://builtin.com/job/senior-data-engineer-pythonpysparkaws/4210594?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/senior-data-engineer-databricks-ashburn-va--5f5b01bf-92fb-430c-bb16-c97cfe25d721?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobs - JazzHR",
        "apply_link": "https://infinitive.applytojob.com/apply/5vQYkVGXC4/Senior-Data-Engineer-Databricks?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-python-pyspark-aws-infinitive-inc-JV_IC1130338_KO0,39_KE40,54.htm?jl=1009925271783&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Job Title: Senior Data Engineer (Informatica/Databricks)\n\nJob Category: Information Technology\n\nTime Type: Full time\n\nMinimum Clearance Required to Start: None\n\nEmployee Type: Regular\n\nPercentage of Travel Required: Up to 10%\n\nType of Travel: Local\n• * *\n\nThe Opportunity:\n\nCACI is currently looking for a highly skilled and experienced Senior Data Engineer (Informatica/Databricks) with agile methodology experience to join our BEAGLE (Border Enforcement Applications for Government Leading-Edge Information Technology) Agile Solution Factory (ASF) Team supporting Customs and Border Protection (CBP) client located in Northern Virginia! Join this passionate team of industry-leading individuals supporting the best practices in Agile Software Development for the Department of Homeland Security (DHS).\n\nAs a member of the BEAGLE ASF Team, you will support the men and women charged with safeguarding the American people and enhancing the Nation’s safety, security, and prosperity. CBP agents and officers are on the front lines, every day, protecting our national security by combining customs, immigration, border security, and agricultural protection into one coordinated and supportive activity.\n\nASF programs thrive in a culture of innovation and are constantly seeking individuals who can bring creative ideas to solve complex problems, both technical and procedural at the team and portfolio levels. The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial. You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints\n\nResponsibilities:\n• Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform.\n• Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing.\n• Work with database developers and administrators across multiple product teams.\n• Serve as a data and technology expert across a broad and diverse set of mission critical applications\n• Modernize the data warehouse environment by migrating the platform to Databricks\n• Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes.\n• Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components.\n• Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets.\n• Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases\n\nQualifications:\n\n· Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:\n\no 3 year check for felony convictions\n\no 1 year check for illegal drug use\n\no 1 year check for misconduct such as theft or fraud\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· 7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering\n\n· Proven 7+ years of experience automating ELT data pipelines using Informatica.\n\n· Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS).\n\n· Experience building and optimizing data pipelines for batch and/or streaming data.\n\n· 3-5 years of Databricks experience. Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered.\n\n· Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks. Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design.\n\n· Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks\n\n· Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies.\n\n· Candidates with one or more of the above skillsets are encouraged to apply.\n\nDesired:\n\n· 5-10 years of DHS, DoD, or IC experience working in complex data environments, including the architecture and optimization of data schemas, terabyte-scale ETL, etc.\n\n· Exposure to implementing or migrating to Cloud environments like Amazon Web Services (AWS) or Microsoft Azure.\n\n· Previous experience as an Enterprise-level Data Architect, Data Engineer, Data Scientist, or Data Analyst.\n\n· Ability to apply advanced principles, theories, and concepts, and contribute to the development of innovative principles and ideas.\n\n-\n\n_________________________________________________________________________\n\nWhat You Can Expect:\n\nA culture of integrity.\n\nAt CACI, we place character and innovation at the center of everything we do. As a valued team member, you’ll be part of a high-performing group dedicated to our customer’s missions and driven by a higher purpose – to ensure the safety of our nation.\n\nAn environment of trust.\n\nCACI values the unique contributions that every employee brings to our company and our customers - every day. You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality.\n\nA focus on continuous growth.\n\nTogether, we will advance our nation's most critical missions, build on our lengthy track record of business success, and find opportunities to break new ground — in your career and in our legacy.\n\nYour potential is limitless. So is ours.\n\n_________________________________________________________________________\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits.\n\nThe proposed salary range for this position is:\n$113,200 - $237,800\n\nCACI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, age, national origin, disability, status as a protected veteran, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "Ashburn, VA",
    "job_city": "Ashburn",
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 39.043756699999996,
    "job_longitude": -77.4874416,
    "job_benefits": [
      "paid_time_off",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDI_gmnxno-NVwm8wAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ability to be adaptable and to work constructively with a technically diverse and geographically separated team is crucial",
        "You should have worked with or have a strong interest in agile software development practices and delivering deployable software in short sprints",
        "Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria includes but is not limited to:",
        "3 year check for felony convictions",
        "1 year check for illegal drug use",
        "1 year check for misconduct such as theft or fraud",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "7+ years of professional experience working on complex data challenges in the areas of data architecture and engineering",
        "Proven 7+ years of experience automating ELT data pipelines using Informatica",
        "Experience with cloud platforms (e.g., AWS, Azure, GCP) and services related to data storage and processing (e.g., S3, ADLS)",
        "Experience building and optimizing data pipelines for batch and/or streaming data",
        "3-5 years of Databricks experience",
        "Alternative/equivalent technologies: Significant experience with Snowflake, Google BigQuery, or Microsoft Azure Synapse Analytics will also be considered",
        "Experience with Databricks, including extensive hands-on experience with PySpark, Python, SQL, Kafka, and Databricks notebooks",
        "Strong experience with data modeling techniques (e.g., dimensional modeling, data vault) and database design",
        "Database skillset for AWS RDS concepts, and understanding of database principles used by tools such Oracle, PostgreSQL and Databricks",
        "Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies",
        "Candidates with one or more of the above skillsets are encouraged to apply"
      ],
      "Benefits": [
        "CACI values the unique contributions that every employee brings to our company and our customers - every day",
        "You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality",
        "A focus on continuous growth",
        "Pay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications",
        "Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives",
        "We offer competitive compensation, benefits and learning and development opportunities",
        "Our broad and competitive mix of benefits options is designed to support and protect employees and their families",
        "At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits",
        "$113,200 - $237,800"
      ],
      "Responsibilities": [
        "Percentage of Travel Required: Up to 10%",
        "Design, develop, and maintain robust and scalable data warehouse architectures and ETL/ELT data pipelines using Databricks or a similar cloud-based platform",
        "Optimize and troubleshoot data pipelines and warehouse performance to ensure efficient and reliable data processing",
        "Work with database developers and administrators across multiple product teams",
        "Serve as a data and technology expert across a broad and diverse set of mission critical applications",
        "Modernize the data warehouse environment by migrating the platform to Databricks",
        "Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes",
        "Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components",
        "Create or augment business and operational intelligence tools using languages such as SQL, Spark, and Python to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets",
        "Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "careers-caci-com-global-en-job-caciglobal322083externalenglobal",
    "_source": "new_jobs"
  },
  {
    "job_id": "Dcu1fLjPsIMWWyv9AAAAAA==",
    "job_title": "Associate Data Engineer",
    "employer_name": "THE DOW CHEMICAL COMPANY",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRHE6Qv_UOqARLxrnwfpr-ucKbs6GEppbjL9zYs&s=0",
    "employer_website": null,
    "job_publisher": "THE DOW CHEMICAL COMPANY",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "THE DOW CHEMICAL COMPANY",
        "apply_link": "https://jobs.dow.com/hiring/associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "At Dow, we believe in putting people first and we’re passionate about delivering integrity, respect and safety to our customers, our employees and the planet.\n\nOur people are at the heart of our solutions. They reflect the communities we live in and the world where we do business. Their diversity is our strength. We’re a community of relentless problem solvers that offers the daily opportunity to contribute with your perspective, transform industries and shape the future. Our purpose is simple - to deliver a sustainable future for the world through science and collaboration.If you’re looking for a challenge and meaningful role, you’re in the right place.\n\nAbout you and the role\n\nDow has an exciting and challenging opportunity for an Associate Data Engineer located in Midland, MI or Houston, TX. which will work on the Enterprise Data & Analytics - Data Analytics Platform team.\n\nAs an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow. TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects. They will work closely with amulti-disciplinary team to:\n• Createdata pipelines forprojectsin the Azure environment.\n• Write notebooks in Databricks\n• Develop Infrastructure as Code (IaC) code\n• Build logic apps\n• Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation.\n\nAssociate Data Engineer Responsibilities / Duties:\n• Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)\n• Develop and deploy data pipelines using Azure data services\n• Deployment of Azure services using Infrastructure as Code and Azure DevOps.\n• This entry level position is for an Independent Contributor and is not expected to be a people leader\n\nKnowledge, skills and abilities include:\n• Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives\n• Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’\n• Concepts of Data architecting - concepts\n• General understanding of digital industry trends\n\nOther Critical Skills:\n• Ability to thrive in challenging situations and solve complex problems\n• Ability to manage own work effort across multiple projects with little supervision\n• Analytical and problem-solving skills\n• Customer centricity\n• Good communication\n\nYour Skills\n• Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs). This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms.\n• Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems.\n• Business Processes: Understanding how business functions operate and how technology enables or improves them. This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs.\n• Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments.\n• Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards.\n\nRequired qualifications:\n• Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree.\n• A minimum requirement for this U.S. based position is the ability to work legally in the United States. No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process.\n\nYour preferred qualifications include:\n• A degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines\n• Experiencewith Azuredata services andPython (other programming language)\n• Experience developing intheAzureenvironment\n• Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling\n• Multi-application and cross-platform design experience\n\nNote: Relocation assistance is not available with this position.\n\nBenefits – What Dow offers you\n\nWe invest in you.\n\nDow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career. You bring your background, talent, and perspective to work every day. Dow rewards that commitment by investing in your total wellbeing.\n\nHere are just a few highlights of what you would be offered as a Dow employee:\n• Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives.\n• Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it.\n• Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals.\n• Employee stock purchase programs (availability varies depending on location).\n• Student Debt Retirement Savings Match Program (U.S. only).\n• Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match.\n• Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs. Travel insurance is also available in certain countries/locations.\n• Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building.\n• Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs.\n• Competitive yearly vacation allowance.\n• Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents).\n• Paid time off to care for family members who are sick or injured.\n• Paid time off to support volunteering and Employee Resource Group’s (ERG) participation.\n• Wellbeing Portal for all Dow employees, our one-stop shop to promote wellbeing, empowering employees to take ownership of their entire wellbeing journey.\n• On-site fitness facilities to help stay healthy and active (availability varies depending on location).\n• Employee discounts for online shopping, cinema tickets, gym memberships and more.\n• Additionally, some of our locations might offer:\n• Transportation allowance (availability varies depending on location)\n• Meal subsidiaries/vouchers (availability varies depending on location)\n• Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)\n\nJoin our team, we can make a difference together.\n\nAbout Dow\nDow (NYSE: DOW) is one of the world’s leading materials science companies, serving customers in high-growth markets such as packaging, infrastructure, mobility and consumer applications.Our global breadth, asset integration and scale, focused innovation, leading business positions and commitment to sustainability enable us to achieve profitable growth and help deliver a sustainable future. We operate manufacturing sites in 30countries and employ approximately36,000 people. Dow delivered sales of approximately$43 billionin 2024. References to Dow or the Company mean Dow Inc. and its subsidiaries. Learn more about us and our ambition to be the most innovative, customer-centric, inclusive and sustainable materials science company in the world by visitingwww.dow.comopens in a new tab.\n\nAs part of our dedication to inclusion, Dow is committed to equal opportunities in employment. We encourage every employee to bring their whole self to work each day to not only deliver more value, but also have a more fulfilling career. Further information regarding Dow's equal opportunities is available on www.dow.comopens in a new tab.\nDow is an Equal Employment Opportunity employer and is committed to providing opportunities without regard for race, color, religion, sex, including pregnancy, sexual orientation, or gender identity, national origin, age, disability and genetic information, including family medical history. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may call us at 1-833-My Dow HR (833-693-6947) and select option 8.",
    "job_is_remote": false,
    "job_posted_at": "4 days ago",
    "job_posted_at_timestamp": 1770854400,
    "job_posted_at_datetime_utc": "2026-02-12T00:00:00.000Z",
    "job_location": "La Marque, TX",
    "job_city": "La Marque",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 29.3685674,
    "job_longitude": -94.9713134,
    "job_benefits": [
      "health_insurance",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DDcu1fLjPsIMWWyv9AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This entry level position is for an Independent Contributor and is not expected to be a people leader",
        "Innate and insatiable curiosity about emerging technologies, with the ability to quickly learn and exploit cutting edge offerings to achieve business objectives",
        "Strong bias for action - you see yourself as an ‘initiator’ and ‘problem solver’",
        "Concepts of Data architecting - concepts",
        "General understanding of digital industry trends",
        "Ability to thrive in challenging situations and solve complex problems",
        "Ability to manage own work effort across multiple projects with little supervision",
        "Analytical and problem-solving skills",
        "Customer centricity",
        "Good communication",
        "Integration Services: The ability to design, configure, and support data and application integrations across systems (e.g., SAP, Azure, APIs)",
        "Minimum of aBachelor'sdegree OR relevant military experience at a U.S. E5 rank or Canadian Petty Officer 2nd Class or higher OR a minimum of 3 years of relevant experience in lieu of a Bachelor's degree",
        "A minimum requirement for this U.S. based position is the ability to work legally in the United States",
        "No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process",
        "Python (other programming language)",
        "Experience developing intheAzureenvironment",
        "Experiencewith structured/semi-/non-structured databases, data ingestion, and data modeling",
        "Multi-application and cross-platform design experience"
      ],
      "Benefits": [
        "We invest in you",
        "Dow invests in total rewards programs to help you manage all aspects of you: your pay, your health, your life, your future, and your career",
        "You bring your background, talent, and perspective to work every day",
        "Dow rewards that commitment by investing in your total wellbeing",
        "Equitable and market-competitive base pay and bonus opportunity across our global markets, along with locally relevant incentives",
        "Benefits and programs to support your physical, mental, financial, and social well-being, to help you get the care you need...when you need it",
        "Competitive retirement program that may include company-provided benefits, savings opportunities, financial planning, and educational resources to help you achieve your long term financial-goals",
        "Employee stock purchase programs (availability varies depending on location)",
        "Student Debt Retirement Savings Match Program (U.S. only)",
        "Dow will take the value of monthly student debt payments and apply them as if they are contributions to the Employees’ Savings Plan (401(k)), helping employees reach the Company match",
        "Robust medical and life insurance packages that offer a variety of coverage options to meet your individual needs",
        "Travel insurance is also available in certain countries/locations",
        "Opportunities to learn and grow through training and mentoring, work experiences, community involvement and team building",
        "Workplace culture empowering role-based flexibility to maximize personal productivity and balance personal needs",
        "Competitive yearly vacation allowance",
        "Paid time off for new parents (birthing and non-birthing, including adoptive and foster parents)",
        "Paid time off to care for family members who are sick or injured",
        "Paid time off to support volunteering and Employee Resource Group’s (ERG) participation",
        "On-site fitness facilities to help stay healthy and active (availability varies depending on location)",
        "Employee discounts for online shopping, cinema tickets, gym memberships and more",
        "Additionally, some of our locations might offer:",
        "Transportation allowance (availability varies depending on location)",
        "Meal subsidiaries/vouchers (availability varies depending on location)",
        "Carbon-neutral transportation incentives e.g. bike to work (availability varies depending on location)"
      ],
      "Responsibilities": [
        "which will work on the Enterprise Data & Analytics - Data Analytics Platform team",
        "As an Associate Data Engineer, you will work with a cross-functional team whoseobjectiveis todeliversolutions that drive business valuefor Dow",
        "TheAssociate Data Engineer willfocus on ingestion / persistence / curation of data as defined byspecificprojects",
        "They will work closely with amulti-disciplinary team to:",
        "Createdata pipelines forprojectsin the Azure environment",
        "Write notebooks in Databricks",
        "Develop Infrastructure as Code (IaC) code",
        "Build logic apps",
        "Identify opportunities for innovating with digital capabilities aiming to accelerate Dow’s digital transformation",
        "Own the development and deployment of solutions enabling data ingestion, storage, and consumption across Dow's enterprise data platforms (SAP and Azure based)",
        "Develop and deploy data pipelines using Azure data services",
        "Deployment of Azure services using Infrastructure as Code and Azure DevOps",
        "This includes understanding ETL processes, protocols (REST/SOAP), authentication methods, and integration platforms",
        "Proactive Problem Solving: Identifying issues before they escalate, analyzing root causes, and applying established procedures to resolve standard technical and process-related problems",
        "Business Processes: Understanding how business functions operate and how technology enables or improves them",
        "This includes partnering with analysts, developers, and business stakeholders to ensure solutions align with functional needs",
        "Technical Support / Troubleshooting: The ability to diagnose, investigate, and resolve issues in applications, systems, integrations, or infrastructure—especially in cloud, SAP, Windows/Linux, or DevSecOps environments",
        "Detail-Oriented Documentation: Creating accurate, complete technical documentation—including integration specifications, solution designs, process flows, and system configurations—ensuring clarity and adherence to architecture and compliance standards"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-dow-com-hiring-associate-data-engineer-la-marque-tx-698d65d09f87ef0a454c1e6c",
    "_source": "new_jobs"
  },
  {
    "job_id": "4jWnCYR8XCPyHreLAAAAAA==",
    "job_title": "Lead Data Engineer; Java, Python, Spark, AWS",
    "employer_name": "Capital One",
    "employer_logo": null,
    "employer_website": "https://www.capitalone.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/7OBXLYK4atQenktrzyPK5A?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/baltimore/maryland/software_development/4710755767/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Overview\n\nThe Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment. In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools. If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you.\n\nKey Responsibilities\n• Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools.\n• Work with developers experienced in machine learning, distributed microservices, and full stack systems.\n• Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake.\n• Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers.\n• Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans.\n• Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions.\n\nRequired Qualifications\n• Bachelor’s Degree\n• Minimum 4 years of experience in application development (internship experience does not apply)\n• At least 2 years of experience in big data technologies\n• At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)\n\nPreferred Qualifications\n• 7+ years of application development experience including Python, SQL, Scala, or Java\n• 4+ years of experience with a public cloud (AWS, Microsoft Azure, or Google Cloud)\n• 4+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n• 4+ years of experience with real-time data and streaming applications\n• 4+ years of experience with NoSQL implementations (e.g., Mongo or Cassandra)\n• 4+ years of data warehousing experience (Redshift or Snowflake)\n• 4+ years of Unix/Linux experience including basic commands and shell scripting\n• 2+ years of experience with Agile engineering practices\n\nBenefits & Perks\n• Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer\n• Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer\n• Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer\n• Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours.",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "Baltimore, MD",
    "job_city": "Baltimore",
    "job_state": "Maryland",
    "job_country": "US",
    "job_latitude": 39.2905023,
    "job_longitude": -76.6104072,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D4jWnCYR8XCPyHreLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Bachelor’s Degree",
        "Minimum 4 years of experience in application development (internship experience does not apply)",
        "At least 2 years of experience in big data technologies",
        "At least 1 year of experience with cloud computing (AWS, Microsoft Azure, or Google Cloud)"
      ],
      "Benefits": [
        "Benefits & Perks",
        "Compensation: McLean, VA: $193,400 - $220,700 for Lead Data Engineer",
        "Compensation: Plano, TX: $175,800 - $200,700 for Lead Data Engineer",
        "Compensation: Richmond, VA: $175,800 - $200,700 for Lead Data Engineer",
        "Note: Salaries for part-time roles will be prorated based on the agreed-upon number of hours"
      ],
      "Responsibilities": [
        "The Lead Data Engineer at Capital One is responsible for designing, developing, testing, implementing, and supporting data solutions using Java, Python, Spark, and AWS in an Agile environment",
        "In this role, you will collaborate with cross-functional teams, mentor engineers, and drive business transformation by leveraging cloud-based data warehousing and big data tools",
        "If you love pioneering innovative technology solutions and solving complex business problems in a collaborative and fast-paced setting, this role is for you",
        "Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools",
        "Work with developers experienced in machine learning, distributed microservices, and full stack systems",
        "Utilize programming languages like Java, Scala, and Python along with both SQL and NoSQL databases, and leverage cloud-based data warehousing services such as Redshift and Snowflake",
        "Share your passion for technology by staying updated on tech trends, experimenting with new technologies, and mentoring fellow engineers",
        "Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans",
        "Perform unit tests and conduct code reviews to ensure quality, performance, and maintainability of solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-7obxlyk4atqenktrzypk5a",
    "_source": "new_jobs"
  },
  {
    "job_id": "06v4fScOsJJeq7PuAAAAAA==",
    "job_title": "Fall 2026 Co-op – Data Engineering - Full-time",
    "employer_name": "Keurig Dr Pepper",
    "employer_logo": null,
    "employer_website": "https://www.keurigdrpepper.com",
    "job_publisher": "Snagajob",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.snagajob.com/jobs/1159701017?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Snagajob",
        "apply_link": "https://www.snagajob.com/jobs/1159701017?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "**Job Overview:**\n• *Fall 2026 Co-op – Data Engineering**\n\nAs a **Fall 2026 Co-op – Data Engineering** in **Burlington, MA** at Keurig Dr Pepper (KDP), you will be a part of the beverage revolution. You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry. You will have real responsibilities and will be provided opportunities to grow professionally. Come learn what it takes to succeed at an industry-leading company and help contribute to our ongoing success. We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements. Candidate should have sound knowledge in SQL scripting.\n• *Shift/Schedule:**\n\n+ The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026\n\n+ Location: Burlington, MA\n\n+ Full-time; 40 hours per week\n\n+ Monday-Friday\n\n+ 8:00am until 5:00pm\n\n+ Hybrid; In office Tues- Thurs\n• *As a Data Engineering Co-op you will:**\n\n+ Create and develop optimal data pipeline using ETL tools\n\n+ Develop complex sql scripts to process and retrieve the data from data warehouse\n\n+ Assemble large, complex data sets that meet functional / non-functional business requirements\n\n+ Optimize the data delivery and date pipelines to process big data\n\n+ Work with stakeholders including Engineering & Product Management to assist with data-related needs\n• *Elements of the KDP 2026 Co-op Program include:**\n\n+ Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment\n\n+ Receive mentor support for your professional development\n\n+ Participate in meet & greets and lunch & learns with KDP executives and other organization leaders\n\n+ Receive professional development training such as networking, professional skills development and presenting\n\n+ Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business\n• *Total Rewards:**\n\n+ $31.00/ hour\n\n+ Paid bi-weekly\n\n+ $5,000.00 Sign-on Bonus, paid within first 30 days of employment\n• *Requirements:**\n\n+ Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields\n\n+ Available to work 40 hours per week (M-F, 8-5) in Burlington, MA\n\n+ Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines\n\n+ Strong skills in Microsoft Excel and PowerPoint\n\n+ Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database\n\n+ Build and optimize big data pipelines, architectures and data sets\n\n+ Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement\n\n+ A successful history of transforming, processing and extracting data from large data warehouse\n\n+ Strong communication skills including excellent listening, written, and verbal abilities\n\n+ Ability to work cross-functionally, be independently driven, and adapt to changes\n\nFollowing skills would be considered as a plus:\n\n+ Ability to create reports using Microsoft Power BI\n\n+ Knowledge in developing machine learning data models\n• *Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future. The Company cannot offer employment to F-1 (student) visa holders who require sponsorship in the future or cannot work now on a full time basis.\n\nWe incorporate HireVue, an automated interview tool, into our campus recruitment process. Please visit this link (https://www.hirevue.com/candidates/interview-tips) to learn more about HireVue and how to prepare. *We recommend checking with your campus career center for additional preparation resources such as InterviewStream, Big Interview and more\n• *Company Overview:**\n\nKeurig Dr Pepper (NASDAQ: KDP) is a leading beverage company in North America, with a portfolio of more than 125 owned, licensed and partners brands and powerful distribution capabilities to provide a beverage for every need, anytime, anywhere. We operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. We work with big, exciting beverage brands and the #1 single-serve coffee brewing system in North America at KDP, and we have fun doing it!\n\nTogether, we have built a leading beverage company in North America offering hot and cold beverages together at scale. Whatever your area of expertise, at KDP you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. Will you join us?\n\nWe strive to be an employer of choice, providing a culture and opportunities that empower our team of ~29,000 employees to grow and develop. We offer robust benefits to support your health and wellness as well as your personal and financial well-being. We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work.\n\nKeurig Dr Pepper is an equal opportunity employer and recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *A.I. Disclosure:**\n\nKDP uses artificial intelligence to assist with initial resume screening and candidate matching. This technology helps us efficiently identify candidates whose qualifications align with our open roles. If you prefer not to have your application processed using artificial intelligence, you may opt out by emailing your resume and qualifications directly to kdpjobs@kdrp.com .\n\nKeurig Dr Pepper is an equal opportunity employer and affirmatively seeks diversity in its workforce. Keurig Dr Pepper recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *Job Overview:**\n• *Fall 2026 Co-op – Data Engineering**\n\nAs a **Fall 2026 Co-op – Data Engineering** in **Burlington, MA** at Keurig Dr Pepper (KDP), you will be a part of the beverage revolution. You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry. You will have real responsibilities and will be provided opportunities to grow professionally. Come learn what it takes to succeed at an industry-leading company and help contribute to our ongoing success. We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements. Candidate should have sound knowledge in SQL scripting.\n• *Shift/Schedule:**\n\n+ The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026\n\n+ Location: Burlington, MA\n\n+ Full-time; 40 hours per week\n\n+ Monday-Friday\n\n+ 8:00am until 5:00pm\n\n+ Hybrid; In office Tues- Thurs\n• *As a Data Engineering Co-op you will:**\n\n+ Create and develop optimal data pipeline using ETL tools\n\n+ Develop complex sql scripts to process and retrieve the data from data warehouse\n\n+ Assemble large, complex data sets that meet functional / non-functional business requirements\n\n+ Optimize the data delivery and date pipelines to process big data\n\n+ Work with stakeholders including Engineering & Product Management to assist with data-related needs\n• *Elements of the KDP 2026 Co-op Program include:**\n\n+ Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment\n\n+ Receive mentor support for your professional development\n\n+ Participate in meet & greets and lunch & learns with KDP executives and other organization leaders\n\n+ Receive professional development training such as networking, professional skills development and presenting\n\n+ Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business\n• *Total Rewards:**\n\n+ $31.00/ hour\n\n+ Paid bi-weekly\n\n+ $5,000.00 Sign-on Bonus, paid within first 30 days of employment\n• *Requirements:**\n\n+ Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields\n\n+ Available to work 40 hours per week (M-F, 8-5) in Burlington, MA\n\n+ Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines\n\n+ Strong skills in Microsoft Excel and PowerPoint\n\n+ Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database\n\n+ Build and optimize big data pipelines, architectures and data sets\n\n+ Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement\n\n+ A successful history of transforming, processing and extracting data from large data warehouse\n\n+ Strong communication skills including excellent listening, written, and verbal abilities\n\n+ Ability to work cross-functionally, be independently driven, and adapt to changes\n\nFollowing skills would be considered as a plus:\n\n+ Ability to create reports using Microsoft Power BI\n\n+ Knowledge in developing machine learning data models\n• *Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future. The Company cannot offer employment to F-1 (student) visa holders who require sponsorship in the future or cannot work now on a full time basis.\n\nWe incorporate HireVue, an automated interview tool, into our campus recruitment process. Please visit this link (https://www.hirevue.com/candidates/interview-tips) to learn more about HireVue and how to prepare. *We recommend checking with your campus career center for additional preparation resources such as InterviewStream, Big Interview and more\n• *Company Overview:**\n\nKeurig Dr Pepper (NASDAQ: KDP) is a leading beverage company in North America, with a portfolio of more than 125 owned, licensed and partners brands and powerful distribution capabilities to provide a beverage for every need, anytime, anywhere. We operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. We work with big, exciting beverage brands and the #1 single-serve coffee brewing system in North America at KDP, and we have fun doing it!\n\nTogether, we have built a leading beverage company in North America offering hot and cold beverages together at scale. Whatever your area of expertise, at KDP you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. Will you join us?\n\nWe strive to be an employer of choice, providing a culture and opportunities that empower our team of ~29,000 employees to grow and develop. We offer robust benefits to support your health and wellness as well as your personal and financial well-being. We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work.\n\nKeurig Dr Pepper is an equal opportunity employer and recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.\n• *A.I. Disclosure:**\n\nKDP uses artificial intelligence to assist with initial resume screening and candidate matching. This technology helps us efficiently identify candidates whose qualifications align with our open roles. If you prefer not to have your application processed using artificial intelligence, you may opt out by emailing your resume and qualifications directly to kdpjobs@kdrp.com .\n\nKeurig Dr Pepper is an equal opportunity employer and affirmatively seeks diversity in its workforce. Keurig Dr Pepper recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law.",
    "job_is_remote": false,
    "job_posted_at": "19 hours ago",
    "job_posted_at_timestamp": 1771138800,
    "job_posted_at_datetime_utc": "2026-02-15T07:00:00.000Z",
    "job_location": "Burlington, MA",
    "job_city": "Burlington",
    "job_state": "Massachusetts",
    "job_country": "US",
    "job_latitude": 42.504716099999996,
    "job_longitude": -71.19562049999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D06v4fScOsJJeq7PuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements",
        "Candidate should have sound knowledge in SQL scripting",
        "*As a Data Engineering Co-op you will:*",
        "Create and develop optimal data pipeline using ETL tools",
        "Develop complex sql scripts to process and retrieve the data from data warehouse",
        "Assemble large, complex data sets that meet functional / non-functional business requirements",
        "Optimize the data delivery and date pipelines to process big data",
        "*Elements of the KDP 2026 Co-op Program include:**",
        "Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment",
        "Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields",
        "Available to work 40 hours per week (M-F, 8-5) in Burlington, MA",
        "Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines",
        "Strong skills in Microsoft Excel and PowerPoint",
        "Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database",
        "Build and optimize big data pipelines, architectures and data sets",
        "A successful history of transforming, processing and extracting data from large data warehouse",
        "Strong communication skills including excellent listening, written, and verbal abilities",
        "Ability to work cross-functionally, be independently driven, and adapt to changes",
        "Ability to create reports using Microsoft Power BI",
        "Knowledge in developing machine learning data models",
        "*Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future",
        "*Fall 2026 Co-op – Data Engineering**",
        "We are looking for a Data Engineer who can develop ETL data pipelines to collect data from multiple sources, process the data and leverage the same to meet various business requirements",
        "Candidate should have sound knowledge in SQL scripting",
        "The KDP 2026 Fall Co-op Program will run from July 13 – December 11, 2026",
        "Full-time; 40 hours per week",
        "Hybrid; In office Tues- Thurs",
        "*As a Data Engineering Co-op you will:*",
        "Create and develop optimal data pipeline using ETL tools",
        "Develop complex sql scripts to process and retrieve the data from data warehouse",
        "Assemble large, complex data sets that meet functional / non-functional business requirements",
        "Optimize the data delivery and date pipelines to process big data",
        "*Elements of the KDP 2026 Co-op Program include:**",
        "Engaging and partnering on innovative projects to gain experience in a fast paced, cross functional team environment",
        "Receive professional development training such as networking, professional skills development and presenting",
        "Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business",
        "Must be an undergraduate currently enrolled in a full-time academic program from an accredited college or university, majoring in Data Engineering, Data Science, or related fields",
        "Available to work 40 hours per week (M-F, 8-5) in Burlington, MA",
        "Excellent organizational skills and attention to detail with a demonstrated ability to manage multiple projects, prioritize requests, and meet deadlines",
        "Strong skills in Microsoft Excel and PowerPoint",
        "Advanced SQL scripting expertise and knowledge working with relational databases, no sql databases, query authoring (SQL) as well as working familiarity with a variety of database",
        "Build and optimize big data pipelines, architectures and data sets",
        "Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement",
        "A successful history of transforming, processing and extracting data from large data warehouse",
        "Strong communication skills including excellent listening, written, and verbal abilities",
        "Ability to work cross-functionally, be independently driven, and adapt to changes",
        "Ability to create reports using Microsoft Power BI",
        "Knowledge in developing machine learning data models",
        "*Please Note** : You must be work authorized in the United States on a full time basis without the need for employer sponsorship now or in the future"
      ],
      "Benefits": [
        "$31.00/ hour",
        "Paid bi-weekly",
        "$5,000.00 Sign-on Bonus, paid within first 30 days of employment",
        "We offer robust benefits to support your health and wellness as well as your personal and financial well-being",
        "We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work",
        "*Total Rewards:**",
        "$31.00/ hour",
        "Paid bi-weekly",
        "$5,000.00 Sign-on Bonus, paid within first 30 days of employment",
        "We offer robust benefits to support your health and wellness as well as your personal and financial well-being",
        "We also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work"
      ],
      "Responsibilities": [
        "You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry",
        "You will have real responsibilities and will be provided opportunities to grow professionally",
        "Full-time; 40 hours per week",
        "8:00am until 5:00pm",
        "Work with stakeholders including Engineering & Product Management to assist with data-related needs",
        "Receive mentor support for your professional development",
        "Participate in meet & greets and lunch & learns with KDP executives and other organization leaders",
        "Receive professional development training such as networking, professional skills development and presenting",
        "Complete a project from start to finish and present it and your takeaways to your team, department, and senior leaders to highlight your project work and impact on the business",
        "Perform root cause analysis on data , answer specific business questions and identify opportunities for improvement",
        "You will be working in a supportive, highly technical manufacturing environment that produces some of KDP’s most recognized beverages in the industry",
        "You will have real responsibilities and will be provided opportunities to grow professionally",
        "8:00am until 5:00pm",
        "Work with stakeholders including Engineering & Product Management to assist with data-related needs",
        "Receive mentor support for your professional development",
        "Participate in meet & greets and lunch & learns with KDP executives and other organization leaders"
      ]
    },
    "job_onet_soc": "15111100",
    "job_onet_job_zone": "5",
    "id": "www-snagajob-com-jobs-1159701017",
    "_source": "new_jobs"
  },
  {
    "job_id": "aLEr5N2HS9ba0TU3AAAAAA==",
    "job_title": "Data Engineer - Remote at Staffing the Universe United States",
    "employer_name": "Staffing the Universe",
    "employer_logo": null,
    "employer_website": null,
    "job_publisher": "Ibfportal.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Ibfportal.com",
        "apply_link": "https://ibfportal.com/office/job/data-engineer-remote-at-staffing-the-universe-united-states-Ym9LbU9GTU5hNlh5eDE0Nys1UTRrK2VVZmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Data Engineer - Remote job at Staffing the Universe. United States. Data Engineer\n\nData Engineer Hartford, CT or Remote Contract No third-party C2C\n\nJob Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team. On this team, you will be helping architect and deliver a wide variety of code artifacts. You will be working to build a scalable and secure ETL solutions for ope...",
    "job_is_remote": false,
    "job_posted_at": "10 days ago",
    "job_posted_at_timestamp": 1770336000,
    "job_posted_at_datetime_utc": "2026-02-06T00:00:00.000Z",
    "job_location": "New Haven, CT",
    "job_city": "New Haven",
    "job_state": "Connecticut",
    "job_country": "US",
    "job_latitude": 41.308274,
    "job_longitude": -72.9278835,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DaLEr5N2HS9ba0TU3AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Data Engineer Hartford, CT or Remote Contract No third-party C2C"
      ],
      "Responsibilities": [
        "Job Responsibilities: As a Data Engineer, you will have the opportunity to expand your skills in a variety of areas while working on a data-focused team",
        "On this team, you will be helping architect and deliver a wide variety of code artifacts",
        "You will be working to build a scalable and secure ETL solutions for ope.."
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "ibfportal-com-office-job-data-engineer-remote-at-staffing-the-universe-united-states-ym9lbu9gtu5hnlh5ede0nys1utrrk2vvzmc9pq",
    "_source": "new_jobs"
  },
  {
    "job_id": "vmP5DjcyLh4_B94fAAAAAA==",
    "job_title": "Senior Data Engineer (Remote)",
    "employer_name": "Parsons Corporation",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTWoeHM7rt-U37K1_ab3N5lIK6QuMuDbepnStZr&s=0",
    "employer_website": "https://www.parsons.com",
    "job_publisher": "Parsons Careers - Parsons Corporation",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Parsons Careers - Parsons Corporation",
        "apply_link": "https://jobs.parsons.com/jobs/senior-data-engineer-remote-virtual-r-174702-jobs--information-technology--?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "In a world of possibilities, pursue one with endless opportunities. Imagine Next!\n\nAt Parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. Guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. Unleash your talent and redefine what’s possible.\n\nJob Description:\n\nParsons is looking for an amazingly talented Senior Data Engineer to join our team! In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization.\n\nWhat You'll Be Doing:\n• Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture.\n• Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake.\n• Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing.\n• Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing.\n• Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation.\n• Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms.\n• Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data.\n• Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement.\n\nWhat Required Skills You'll Bring:\n• Strong hands-on experience with T-SQL and Python.\n• Experience with comprehensive data conversion projects is preferred (ERP systems including Oracle Cloud ERP and/or SAP S4/HANA)\n• Experience with Relational Database systems\n• Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)\n• Familiar with multi-dimensional and tabular models\n• 5+ years of experience in data engineering, data architecture, or data platform development.\n• Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar).\n• Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines.\n• Deep understanding of lakehouse architecture and medallion design patterns.\n• Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines.\n• Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats.\n• Strong problem-solving skills and ability to work independently in a fast-paced environment.\n• US person\n\nWhat Desired Skills You'll Bring:\n• Experience with data governance, security, and compliance (e.g., SOX, HIPAA).\n• Snowflake, Azure Data Engineer, dbt, and/or Databricks certifications\n• Exposure to real-time data processing and streaming technologies (e.g., Kafka, Spark Streaming).\n• Familiarity with data observability tools and automated testing frameworks for pipelines.\n• Bachelor's or Master’s degree in Computer Science, Information Systems, or a related field\n\nSecurity Clearance Requirement:\nNone\n\nThis position is part of our Corporate team.\n\nFor over 80 years, Parsons Corporation, has shaped the future of the defense, intelligence, and critical infrastructure markets. Our employees work in a close-knit team environment to find new, innovative ways to deliver smart solutions that are used and valued by customers around the world. By combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, transportation, smart cities, and more, we're providing tomorrow's solutions today.\n\nSalary Range: $100,900.00 - $176,600.00\n\nWe value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!\n\nThis position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled.\n\nParsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status.\n\nWe truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! Imagine next and join the Parsons quest—APPLY TODAY!\n\nParsons is aware of fraudulent recruitment practices. To learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Virginia",
    "job_city": null,
    "job_state": "Virginia",
    "job_country": "US",
    "job_latitude": 37.4315734,
    "job_longitude": -78.6568942,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DvmP5DjcyLh4_B94fAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 101000,
    "job_max_salary": 177000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Strong hands-on experience with T-SQL and Python",
        "Experience with Relational Database systems",
        "Experience with both on-premises / cloud ETL toolsets (preferably SSIS, ADF, Synapse, AWS)",
        "Familiar with multi-dimensional and tabular models",
        "5+ years of experience in data engineering, data architecture, or data platform development",
        "Proficiency in PySpark and SQL notebooks (e.g., Microsoft Fabric, Databricks, Synapse, or similar)",
        "Experience with Azure Data Factory and/or Informatica for building scalable ingestion pipelines",
        "Deep understanding of lakehouse architecture and medallion design patterns",
        "Experience with dbt, GitHub source control, branching strategies, and CI/CD pipelines",
        "Familiarity with data ingestion from APIs, SQL Server, and flat files into Parquet/Delta formats",
        "Strong problem-solving skills and ability to work independently in a fast-paced environment",
        "US person"
      ],
      "Benefits": [
        "Salary Range: $100,900.00 - $176,600.00",
        "We value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, Employee Stock Ownership Plan (ESOP), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle!"
      ],
      "Responsibilities": [
        "In this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization",
        "Design and implement scalable, efficient data ingestion pipelines using ADF, Informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture",
        "Develop robust ETL/ELT workflows to ingest data from diverse sources (e.g., SQL Server, flat files, APIs) into Parquet/Delta formats and model them into semantic layers in Snowflake",
        "Build and maintain incremental and CDC-based pipelines to support near-real-time and daily batch processing",
        "Apply best practices for Snowflake implementation, including performance tuning, cost optimization, and secure data sharing",
        "Leverage dbt for data transformation and modeling, and implement GitHub-based source control, branching strategies, and CI/CD pipelines for deployment automation",
        "Ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms",
        "Collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data",
        "Mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-parsons-com-jobs-senior-data-engineer-remote-virtual-r-174702-jobs-information-technology",
    "_source": "new_jobs"
  },
  {
    "job_id": "1Eb8GME5F6h5wzAiAAAAAA==",
    "job_title": "Data Engineer",
    "employer_name": "Rural King",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSI99A-RNW3mNS-GYEPIGd0U4Qdfm6662lvJ7uw&s=0",
    "employer_website": "https://www.ruralking.com",
    "job_publisher": "Career.io",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Career.io",
        "apply_link": "https://career.io/job/data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-rural-king-4371246007?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Rural-King-Supply/Job/Data-Engineer/-in-Mattoon,IL?jid=4c03fb6a091c6a1e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Teal",
        "apply_link": "https://www.tealhq.com/job/data-engineer_7ea1a64fa7b5bb1f8b19872d37b34c2da2d7e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Monster",
        "apply_link": "https://www.monster.com/job-openings/data-engineer-mattoon-il--e8d0d5a5-70ea-4eed-87dd-58b13c57a77d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Job Listings - ICIMS",
        "apply_link": "https://careers-ruralking.icims.com/jobs/35283/data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Nexxt",
        "apply_link": "https://www.nexxt.com/jobs/data-engineer-mattoon-il-3161318692-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "SimplyHired",
        "apply_link": "https://www.simplyhired.com/job/_m6FAmu2Nyxj74VafN91ouQs45puOR74OyTLalSIYaTrYL6icAgj4w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "About us\n\nRural King is America's Farm and Home Store, providing essentials to the communities we serve. With a wide array of necessities ranging from food and feed to farm and home products, Rural King serves over 130 locations across 13 states and is constantly expanding. Our annual sales exceed $2.5 Billion, and our heart beats in Mattoon, IL, home to our corporate office, distribution center, and flagship store.\n\nOne thing our customers appreciate is our unique shopping experience, complete with complimentary popcorn and coffee. It's just one way we show our appreciation for their support.\n\nAt Rural King, we value our associates and strive to create a positive, rewarding workplace. We offer growth opportunities, competitive benefits, and a people-first environment where dedicated individuals come together to serve rural communities passionately. Join us, and you'll find not just a job but a chance to grow professionally, contribute meaningfully, and make a difference in the lives of those we serve.\n\nHow we reward you\n\n401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%\n\nHealthcare plans to support your needs\n\nVirtual doctor visits\n\nAccess to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program\n\n15% Associate Discount\n\nDave Ramsey's SmartDollar Program\n\nAssociate Assistance Program\n\nRK Cares Associate Hardship Program\n\n24/7 Chaplaincy Services\n\nCompany paid YMCA Family Membership\n\nWhat You'll do\n\nAs a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization.\n• Design, build, maintain, and manage the systems that move data efficiently within the organization.\n• Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake.\n• Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs\n• Monitor the performance of data pipelines that are efficient and secure\n• Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism.\n• Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments.\n• Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively.\n• Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement.\n• Perform other duties as assigned.\n\nSupervisory Responsibilities\n\nNone\n\nEssential Qualities for Success\n• At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education.\n• Information Technology Engineering experience preferred.\n• Experience as a Data Engineer or similar software engineering role.\n• Proficient with git.\n• Good knowledge of SQL and PHP or similar languages.\n• Good knowledge of big data technologies.\n• Good knowledge of data modeling.\n• Familiar with Looker, Power BI or other BI tools.\n• Working knowledge of databases, MySQL/MariaDB, and SQL.\n• Strong understanding of retail business practices.\n• Excellent negotiation and conflict resolution skills.\n• Demonstrated ability to adapt in a fast-paced environment.\n• Strong analytical and problem-solving skills.\n• Excellent organizational skills and attention to detail.\n• Demonstrated behaviors must reflect integrity, professionalism, and confidentiality.\n\nPhysical Requirements\n• Ability to maintain a seated or standing position for extended durations.\n• Capability to lift 15 pounds periodically.\n• Able to navigate and access all facilities.\n• Skill to effectively communicate verbally with others, both in-person and via electronic devices.\n• Close vision for computer-related tasks.\n\nReasonable accommodations may be made to enable individuals with disabilities to perform essential job functions.\n\nThe pay range for this position is $55,000 - $65,000 annualized and is bonus eligible. Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs. To learn more about our benefits, review here https://acrobat.adobe.com/id/urn:aaid:sc:VA6C2:14539c15-191a-4b77-9c13-f6ccfce10094.\n\nResponsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data. They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization. - Design, build, maintain, and manage the systems that move data efficiently within the organization. - Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake. - Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism. - Actively participate in learning initiatives offered such as training programs, workshops, and webinars. Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments. - Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively. - Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement. - Perform other duties as assigned. Supervisory Responsibilities None",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "Mattoon, IL",
    "job_city": "Mattoon",
    "job_state": "Illinois",
    "job_country": "US",
    "job_latitude": 39.4830897,
    "job_longitude": -88.37282549999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D1Eb8GME5F6h5wzAiAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55000,
    "job_max_salary": 65000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "At least 4 years of relevant experience and a bachelor's degree in computer science or engineering or equivalent combination of experience and education",
        "Experience as a Data Engineer or similar software engineering role",
        "Proficient with git",
        "Good knowledge of SQL and PHP or similar languages",
        "Good knowledge of big data technologies",
        "Good knowledge of data modeling",
        "Familiar with Looker, Power BI or other BI tools",
        "Working knowledge of databases, MySQL/MariaDB, and SQL",
        "Strong understanding of retail business practices",
        "Excellent negotiation and conflict resolution skills",
        "Demonstrated ability to adapt in a fast-paced environment",
        "Strong analytical and problem-solving skills",
        "Excellent organizational skills and attention to detail",
        "Demonstrated behaviors must reflect integrity, professionalism, and confidentiality",
        "Ability to maintain a seated or standing position for extended durations",
        "Capability to lift 15 pounds periodically",
        "Able to navigate and access all facilities",
        "Skill to effectively communicate verbally with others, both in-person and via electronic devices",
        "Close vision for computer-related tasks"
      ],
      "Benefits": [
        "401(k) plan that provides a 100% match on the first 3% of your contributions and 50% of the next 2%",
        "Healthcare plans to support your needs",
        "Access to Centers of Excellence with Barnes Jewish Hospital and Mayo Clinic's Complex Care Program",
        "15% Associate Discount",
        "Dave Ramsey's SmartDollar Program",
        "Associate Assistance Program",
        "RK Cares Associate Hardship Program",
        "24/7 Chaplaincy Services",
        "The pay range for this position is $55,000 - $65,000 annualized and is bonus eligible",
        "Exact compensation is determined by factors such as relevant geographic location, education, certifications, experience, job level, shift, and organizational needs"
      ],
      "Responsibilities": [
        "As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs",
        "Monitor the performance of data pipelines that are efficient and secure",
        "Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned",
        "Responsibilities What You'll do As a Data Engineer, you will be responsible for designing, building, and maintaining the data architecture, infrastructure, and systems necessary for collecting, storing, processing, and analyzing large volumes of data",
        "They work with large and complex data sets from various sources, including databases, APIs, and streaming data, to create scalable and reliable pipelines that facilitate data collection, storage, processing, and analysis They play a crucial role in the data pipeline, ensuring that data is available, accessible, and reliable for data scientists, analysts, and other stakeholders within an organization",
        "Design, build, maintain, and manage the systems that move data efficiently within the organization",
        "Design and implement ETL pipelines that can extract data from various sources, transform it, and load it into the data warehouse and/or data lake",
        "Work collaboratively with cross-functional teams and communicate effectively with technical and non-technical stakeholders to understand data needs - Monitor the performance of data pipelines that are efficient and secure - Use discretion and independent judgment in daily decisions while maintaining a high level of confidentiality and professionalism",
        "Actively participate in learning initiatives offered such as training programs, workshops, and webinars",
        "Leverage these opportunities to acquire new knowledge, refine existing skills, and stay current on the latest developments",
        "Provide friendly, proactive, and professional internal and external support to others, assisting with inquiries, concerns, and issues promptly and effectively",
        "Demonstrate behaviors that exemplify Rural King's Values: People First, Integrity, Attitude, Initiative, Teamwork, Accountability, and Continuous Improvement",
        "Perform other duties as assigned"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "career-io-job-data-engineer-mattoon-rural-king-123889fa4972056665b5b329b4f7747d",
    "_source": "new_jobs"
  },
  {
    "job_id": "fKq613YVZoGaO3ZhAAAAAA==",
    "job_title": "Temporary  Student Data Engineer",
    "employer_name": "University of Notre Dame",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRB_bk0V1eXoFSLkXbpRAL9w5OPDz55ACjhxHof&s=0",
    "employer_website": null,
    "job_publisher": "BMES Career Center",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "BMES Career Center",
        "apply_link": "https://jobboard.bmes.org/jobs/22037211/temporary-student-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/5e385fd85cc450ceae1fd6846215958e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/698af9934db8972cec006c83?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/temporary-student-data-engineer--notre-dame--e272bff6f756c6e91c32e5a9ffa44cbfd?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/indiana/business/4868026961/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-notre-dame-temporary-student-data-engineer-university-part_time?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Temporary Student Data Engineer\n\nNotre Dame, IN, United States\nContract\nProvost\nTemporary\n\nCompany Description\n\nJob Description\nWe are seeking a Student Data Engineer (SDE) to join a team of students creating a Roblox game as a data collection tool, gauging students' interest in STEM and health care careers. The Lucy Family Institute for Data & Society (LFIDS) leverages data science, AI & ML toward social good. LFIDS engages with the Notre Dame community & beyond through funded research projects & collaborations, educational workshops, and special events. SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana. In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed.\n\nKey Responsibilities:\n\nWithin assigned projects, this role requires completion of data processing and programming tasks related to:\n• Data collection, management, harvesting, processing, transformation, and visualization;\n• Prototype data processing solutions;\n• Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms.\n\nThe successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners. Other responsibilities may include data analysis and giving presentations to diverse audiences.\n\nAdditional Requirements\n\nIn addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested. For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience.\n\nAdditional Opportunities\n\nStudent data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects. Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately.\n\nThe Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests.\n\nCore Qualities & Expectations\n• Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving\n• Confidentiality: Maintaining confidentiality is required.\n• Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings).\n• Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly. Perfection is not expected-it's okay to take time to learn. What matters is trying your best in each unique circumstance. We're committed to supporting your growth and confidence in the role.\n• Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities. We aim to provide a structure that supports your success.\n• Attention to detail or the ability to follow a set of instructions that we'll co-create and adjust based on your preferred learning and working style.\n• Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer.\n\nQualifications\nWe are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:\n• Data science methods and tools,\n• Software design\n• User experience principles\n\nExperience in an LFIDS area of expertise, like:\n• R and/or Python\n• Events and communications support\n• Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites).\n\nAdditional Information\nCompensation: $17.00/hour\n\nApplications for this position will close on February 13, 2026.\n\nThe University of Notre Dame seeks to attract, develop, and retain the highest quality faculty, staff and administration. The University is an Equal Opportunity Employer, and does not discriminate on the basis of race, color, national or ethnic origin, sex, disability, veteran status, genetic information, or age in employment. Moreover, Notre Dame prohibits discrimination against veterans or disabled qualified individuals, and complies with 41 CFR 60-741.5(a) and 41 CFR 60-300.5(a). We strongly encourage applications from candidates attracted to a university with a Catholic identity.\n\nTo apply, visit https://jobs.smartrecruiters.com/UniversityOfNotreDame/3743990011597785-temporary-student-data-engineer\n\nCopyright 2025 Jobelephant.com Inc. All rights reserved.\n\nPosted by the FREE value-added recruitment advertising agency jeid-5691f5d28abbde4e8b71884761a96763",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Notre Dame, IN",
    "job_city": "Notre Dame",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.7001908,
    "job_longitude": -86.2379328,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DfKq613YVZoGaO3ZhAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "Student data engineers with the requisite skill sets (or who express an interest in gaining requisite skills, e.g., R, Python, Shiny, GIS, GitHub, podcasting, and video editing) will have opportunities to assist a variety of LFIDS faculty/staff with data analysis, software development & communications, as well as investigations of emerging technologies, resources, and creative projects",
        "Service Orientation: Values inclusivity, courtesy, patience, resourcefulness, and problem-solving",
        "Confidentiality: Maintaining confidentiality is required",
        "Reliability & dependability are key, especially as SSEs may be responsible for maintaining Institute spaces during times when full-time team members are unavailable (e.g., due to meetings)",
        "Adaptability: We welcome a mindset that is open to learning new systems and adapting quickly",
        "Organizational proficiency: Includes the ability to manage time effectively based on clearly communicated priorities",
        "We aim to provide a structure that supports your success",
        "Independence & Teamwork: Most tasks are completed independently, though there are occasional team-based efforts where we divide and conquer",
        "We are currently seeking Saint Mary's students with a strong (preferably demonstrated) interest in the following:",
        "Data science methods and tools,",
        "Software design",
        "User experience principles",
        "Experience in an LFIDS area of expertise, like:",
        "R and/or Python",
        "Events and communications support",
        "Familiarity with data science, software applications, and/or multimedia content creation and/or editing (e.g., video, PowerPoint slides, podcasts, websites)"
      ],
      "Benefits": [
        "Compensation: $17.00/hour"
      ],
      "Responsibilities": [
        "SDEs serve an important role as technical contributors to data-centric projects engaging with Lucy faculty and staff, the broader Notre Dame research community, and regional partners within Indiana",
        "In turn, in addition to the #GOALS project with the Roblox game development, other projects may be assigned as needed",
        "Within assigned projects, this role requires completion of data processing and programming tasks related to:",
        "Data collection, management, harvesting, processing, transformation, and visualization;",
        "Prototype data processing solutions;",
        "Experience with a variety of technology environments, ranging from databases to stats to cloud computing platforms",
        "The successful candidate will be a team player who collaborates effectively with fellow developers, faculty, university researchers, students, and business partners",
        "Other responsibilities may include data analysis and giving presentations to diverse audiences",
        "In addition to regular shift duties based on available capacity, Student Data Engineers may be assigned to complete tasks associated with LFIDS projects as requested",
        "For example, our Scholarly Impact, Data Michiana, and SocioEconomic Indicators dashboard leads may assign students to upload data or develop application features, or SDEs may be asked to try a new software interface and document its basic functions or user experience",
        "Student data engineers who contribute to work that is later published or presented will be acknowledged appropriately",
        "The Institute is committed to ensuring that our student data engineers find their time on our team to be advantageous to their professional development, and we are happy to consider SDEs' proposals for projects that overlap with LFIDS priorities and their research/professional development interests"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobboard-bmes-org-jobs-22037211-temporary-student-data-engineer",
    "_source": "new_jobs"
  },
  {
    "job_id": "XQf3XW1gRIsW1UiuAAAAAA==",
    "job_title": "AWS Redshift Data Engineer Data Architect",
    "employer_name": "Numentica LLC",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTootVXWXzkNN4CMEZIpwL0zphlLjGSX6fP41El&s=0",
    "employer_website": "https://numentica.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=a7729c40db42&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This is a remote position.\n\nRole : AWS Redshift Data Engineer / Data Architect - Remote\n\nOverview\n\nWe are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture.\n\nKey Responsibilities\n\nOptimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS).\n\nLead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime.\n\nAnalyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads.\n\nReview and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis.\n\nDesign data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency.\n\nCollaborate with stakeholders on architecture recommendations and best practices.\n\nRequired Skills\n\nStrong hands-on experience with Amazon Redshift (Provisioned Serverless RMS).\n\nDeep SQL performance tuning and query optimization expertise.\n\nSolid AWS data engineering background (RDS DynamoDB Kinesis).\n\nExperience with modern lakehouse concepts especially Apache Iceberg.\n\nData architecture and analytical platform design experience.\n\nNice to Have\n\nExposure to AI-assisted query optimization or analytics platforms.\n\nExperience building or supporting BI and analytics use cases at scale.\n\nRequired Skills :\n\nAWS Redshift Data Engineer / Data Architect\n\nKey Skills\n\nFund Management,Drafting,End User Support,Infrastructure,Airlines,Catia\n\nEmployment Type : Full Time\n\nExperience : years\n\nVacancy : 1",
    "job_is_remote": false,
    "job_posted_at": "2 days ago",
    "job_posted_at_timestamp": 1771027200,
    "job_posted_at_datetime_utc": "2026-02-14T00:00:00.000Z",
    "job_location": "California",
    "job_city": null,
    "job_state": "California",
    "job_country": "US",
    "job_latitude": 36.778261,
    "job_longitude": -119.4179324,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DXQf3XW1gRIsW1UiuAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are looking for a senior AWS Redshift expert to support analytics platform enhancements including Redshift optimization migration to Redshift Serverless and modern data ingestion architecture",
        "Strong hands-on experience with Amazon Redshift (Provisioned Serverless RMS)",
        "Deep SQL performance tuning and query optimization expertise",
        "Solid AWS data engineering background (RDS DynamoDB Kinesis)",
        "Experience with modern lakehouse concepts especially Apache Iceberg",
        "Data architecture and analytical platform design experience",
        "Exposure to AI-assisted query optimization or analytics platforms",
        "Experience building or supporting BI and analytics use cases at scale",
        "AWS Redshift Data Engineer / Data Architect"
      ],
      "Responsibilities": [
        "Optimize and modernize Amazon Redshift environments including consolidation into Redshift Managed Storage (RMS)",
        "Lead or support migration from Provisioned Redshift to Redshift Serverless with zero or minimal downtime",
        "Analyze and tune Redshift SQL for performance across batch and ad-hoc analytical workloads",
        "Review and enhance data ingestion pipelines from AWS sources such as RDS DynamoDB and Kinesis",
        "Design data delivery into Apache Iceberg tables and Redshift with minimal duplication and high efficiency",
        "Collaborate with stakeholders on architecture recommendations and best practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "8ojyM0IL2Dgweb6mAAAAAA==",
    "job_title": "Lead Data Engineer",
    "employer_name": "Guardianlife",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTEHyPUSx8in9lbGwtt3n-urbaK-a93llGwRZYg&s=0",
    "employer_website": "https://www.guardianlife.com",
    "job_publisher": "ZipRecruiter",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "ZipRecruiter",
        "apply_link": "https://www.ziprecruiter.com/c/Guardian-Life/Job/Lead-Data-Engineer/-in-Holmdel,NJ?jid=5ff92b3cfbdbb9fa&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      },
      {
        "publisher": "Jobright",
        "apply_link": "https://jobright.ai/jobs/info/6984c41c0f6f7e7a2cdf36a2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/guardian-life/holmdel-nj/lead-data-engineer/448a3771cda355807800f3a245f659f2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5618263952?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/lead-data-engineer-holmdel-new-jersey-us-guardian-life-r000108510?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "JobLeads",
        "apply_link": "https://www.jobleads.com/us/job/lead-data-engineer--holmdel-township--e33d7f694afe11a60b979debb2c4bd89a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "BeBee",
        "apply_link": "https://us.bebee.com/job/40aefed101891767dbcd5f364e470e55?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Learn4Good",
        "apply_link": "https://www.learn4good.com/jobs/holmdel-township/new-jersey/software_development/4858586430/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Guardian is seeking a highly skilled and motivated Lead Data Engineer to join the FPRS&CSWM Data Engineering team. In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases. Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient. The ideal candidate will have a passion for data engineering, thrive in a collaborative environment and are excited about leveraging cutting-edge technologies to drive business success.\n\nYour contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers. You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs. We value curiosity, creativity, and continuous learning. If you're passionate about solving meaningful problems and creating value through data-driven innovation, we look forward to welcoming you to our team.\n\nYou will\n• Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables.\n• Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth.\n• Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products\n• Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems.\n• Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.\n• Construct meaningful data assets sourced from structured, semi structured, and unstructured data.\n• Develop real-time data solutions by creating new API endpoints or streaming frameworks.\n• Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle.\n• Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams.\n• Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication.\n• Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions.\n• Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives.\n• Stay up-to-date with the latest trends in modern data engineering, machine learning & AI.\n\nYou have\n• Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field.\n• 5+ years of experience working with Python, SQL, PySpark, and bash scripts. Proficient in software development lifecycle and software engineering practices.\n• 4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases.\n• 3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark.\n• 2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality.\n• Solid understanding of data modeling and warehousing techniques. Experience working in a data warehouse is a plus.\n• Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities.\n• Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries).\n• Proficient in understanding and incorporating software engineering principles in design & development process.\n• Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent).\n• Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business.\n\nLocation\n• Three days a week at a Guardian office in Bethlehem, PA, New York, NY. Pittsfield, MA or Holmdel, NJ.\n\nSalary Range:\n\n$99,150.00 - $162,885.00\n\nThe salary range reflected above is a good faith estimate of base pay for the primary location of the position. The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate. In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation.\n\nOur Promise\n\nAt Guardian, you'll have the support and flexibility to achieve your professional and personal goals. Through skill-building, leadership development and philanthropic opportunities, we provide opportunities to build communities and grow your career, surrounded by diverse colleagues with high ethical standards.\n\nInspire Well-Being\n\nAs part of Guardian's Purpose - to inspire well-being - we are committed to offering contemporary, supportive, flexible, and inclusive benefits and resources to our colleagues. Explore our company benefits at www.guardianlife.com/careers/corporate/benefits.Benefits apply to full-time eligible employees. Interns are not eligible for most Company benefits.\n\nEqual Employment Opportunity\n\nGuardian is an equal opportunity employer. All qualified applicants will be considered for employment without regard to age, race, color, creed, religion, sex, affectional or sexual orientation, national origin, ancestry, marital status, disability, military or veteran status, or any other classification protected by applicable law.\n\nAccommodations\n\nGuardian is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities.Guardian also provides reasonable accommodations to qualified job applicants (and employees) to accommodate the individual's known limitations related to pregnancy, childbirth, or related medical conditions, unless doing so would create an undue hardship. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact MyHR@glic.com. Please note: this resource is for accommodation requests only. For all other inquires related to your application and careers at Guardian, refer to the Guardian Careers site.\n\nVisa Sponsorship\n\nGuardian is not currently or in the foreseeable future sponsoring employment visas. In order to be a successful applicant. you must be legally authorized to work in the United States, without the need for employer sponsorship.\n\nCurrent Guardian Colleagues: Please apply through the internal Jobs Hub in Workday.",
    "job_is_remote": false,
    "job_posted_at": "11 days ago",
    "job_posted_at_timestamp": 1770249600,
    "job_posted_at_datetime_utc": "2026-02-05T00:00:00.000Z",
    "job_location": "Holmdel, NJ",
    "job_city": "Holmdel",
    "job_state": "New Jersey",
    "job_country": "US",
    "job_latitude": 40.3848944,
    "job_longitude": -74.18900599999999,
    "job_benefits": [
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3D8ojyM0IL2Dgweb6mAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 99150,
    "job_max_salary": 162885,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "We value curiosity, creativity, and continuous learning",
        "Bachelor's or Master's degree with 8+ years of experience in Computer Science, Engineering, or a related field",
        "5+ years of experience working with Python, SQL, PySpark, and bash scripts",
        "Proficient in software development lifecycle and software engineering practices",
        "4+ years of experience developing and maintaining robust data pipelines for both structured and unstructured data for advanced analytical and reporting use cases",
        "3+ years of experience working with Cloud Data Warehousing (Redshift, Snowflake, Databricks SQL or equivalent) platforms and distributed frameworks like Spark",
        "2+ years of experience leading a team of engineers and a track record of delivering robust and scalable data solutions with highest quality",
        "Solid understanding of data modeling and warehousing techniques",
        "Proficiency in understanding REST APIs, experience using different types of APIs to extract data or perform functionalities",
        "Hands-on experience building and maintaining tools and libraries used by multiple teams across the organization (e.g., Data Engineering utility libraries, DQ Libraries)",
        "Proficient in understanding and incorporating software engineering principles in design & development process",
        "Hands-on experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Github, Bitbucket), orchestration (Airflow, Prefect or equivalent)",
        "Excellent communication skills and ability to work and collaborate with cross-functional teams across technology and business",
        "you must be legally authorized to work in the United States, without the need for employer sponsorship"
      ],
      "Benefits": [
        "$99,150.00 - $162,885.00",
        "The salary range reflected above is a good faith estimate of base pay for the primary location of the position",
        "The salary for this position ultimately will be determined based on the education, experience, knowledge, and abilities of the successful candidate",
        "In addition to salary, this role may also be eligible for annual, sales, or other incentive compensation",
        "Interns are not eligible for most Company benefits"
      ],
      "Responsibilities": [
        "In this role, you will play a crucial part in designing, building, and maintaining the data pipelines that power our Operational ,reporting, ML & AI use cases",
        "Your expertise will help us transform raw data into actionable insights via data products, ensuring that our data solutions are robust, scalable and efficient",
        "Your contributions will go beyond hands-on engineering, as you help bring life to innovative ideas and mentor other engineers",
        "You'll thrive in a fast-paced, collaborative environment, balancing technical execution with a deep understanding of business needs",
        "Lead technical design and implementation of data engineering solutions, ensuring best practices and high-quality deliverables",
        "Mentor and guide junior engineers, conducting code reviews and technical sessions to foster team growth",
        "Perform detailed analysis of raw data sources by applying business context and collaborate with cross-functional teams to transform raw data into data products",
        "Create scalable and trusted data pipelines which generate curated data assets in centralized data lake/data warehouse ecosystems",
        "Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues",
        "Construct meaningful data assets sourced from structured, semi structured, and unstructured data",
        "Develop real-time data solutions by creating new API endpoints or streaming frameworks",
        "Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data lifecycle",
        "Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and other IT teams",
        "Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication",
        "Provide technical leadership and mentorship on continuous improvement in building reusable and scalable solutions",
        "Contribute to enhancing strategy for advanced data engineering practices and lead execution of key initiatives",
        "Stay up-to-date with the latest trends in modern data engineering, machine learning & AI"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-ziprecruiter-com-c-guardian-life-job-lead-data-engineer-in-holmdel-nj",
    "_source": "new_jobs"
  },
  {
    "job_id": "mLWbCE2BsJ1026TLAAAAAA==",
    "job_title": "Slalom Flex (Project Based)- Federal GCP Data Engineer",
    "employer_name": "Slalom",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTz83P7TkZM86wEdE65TUA9A-yqu267uumzStA3&s=0",
    "employer_website": "https://www.slalom.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "GCP Data Engineer (U.S. Citizenship Required)\n\nAbout Us\n\nSlalom is a purpose-led, global business and technology consulting company. From strategy to implementation, our approach is fiercely human. In six+ countries and 43+ markets, we deeply understand our customers—and their customers—to deliver practical, end-to-end solutions that drive meaningful impact. Backed by close partnerships with over 400 leading technology providers, our 10,000+ strong team helps people and organizations dream bigger, move faster, and build better tomorrows for all. We’re honored to be consistently recognized as a great place to work, including being one of Fortune’s 100 Best Companies to Work For seven years running. Learn more at slalom.com.\n\nAbout The Role\n\nWe are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA. This role is a hands‑on engineering position working within a secure Google Cloud Platform environment to design, build, and optimize scalable data pipelines and analytics capabilities that enable high‑quality insights and operational excellence for our federal client.\n\nThis position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance.\n\nWhat You Will Do\n\nData Engineering & Cloud Development\n• Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage.\n• Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs.\n• Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation.\n\nClient Leadership & Delivery\n• Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements.\n• Translate technical designs and delivery timelines for both technical and non‑technical audiences.\n• Support modernization of legacy data environments into scalable GCP‑based architectures.\n• Ensure all solutions align with federal data governance, security, and performance standards.\n\nSolution Optimization & Innovation\n• Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling.\n• Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines.\n• Develop and maintain data lineage, metadata documentation, and enterprise data models.\n• Identify linkages across disparate datasets to build unified, interoperable data architectures.\n• Perform cleanup of existing datasets and transformation logic where needed.\n\nCollaboration & Team Leadership\n• Work closely with data architects, BI developers, cloud engineers, and data scientists.\n• Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning.\n• Track work in Jira and maintain documentation in Confluence.\n• Support testing, deployment, and quality assurance of data products.\n• Mentor junior data engineering team members and contribute to best-practice frameworks.\n\nMust-Have Qualifications\n• U.S. citizenship\n• Ability to obtain and maintain a federal Public Trust clearance\n• 3+ years of experience in cloud-based data engineering\n• Strong hands-on expertise with Google BigQuery\n• Proficiency in Python for pipeline development, automation, and cloud integration\n• Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar\n• Strong SQL skills, including data modeling and data quality testing\n• Experience with Git-based version control and CI/CD concepts\n• Familiarity with data governance, metadata management, and compliance considerations\n• Strong communication and stakeholder engagement skills\n\nNice-to-Have Skills\n• Experience supporting federal or regulated environments\n• Familiarity with Looker and downstream BI enablement\n• Understanding of ML workloads or data structures optimized for modeling\n• Experience with Agile/Scrum or SAFe\n• Knowledge of data quality frameworks and testing strategies\n• Exposure to GCP data governance tools such as Dataplex\n• Experience with serverless architectures (Cloud Functions, Cloud Run)\n• Familiarity with JavaScript for Dataform SQLX extensibility\n• Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)\n• Experience with dbt, Dataform, Databricks, or other analytics engineering tooling\n• Relevant GCP certifications\n\nCompensation And Benefits\n\nSlalom prides itself on helping team members thrive in their work and life. As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability. We also offer yearly $350 reimbursement account for any well-being-related expenses.\n\nSlalom is committed to fair and equitable compensation practices. For this position, the base salary pay range is $55/hr to $75/hr. Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors. The salary pay range is subject to change and may be modified at any time.\n\nEEO and Accommodations\n\nSlalom is an equal opportunity employer and is committed to inclusion, diversity, and equity in the workplace. All qualified applicants will receive consideration\n\nfor employment without regard to race, color, religion, sex, national origin, disability status, protected veterans’ status, or any other characteristic protected by federal, state, or local laws. Slalom will also consider qualified applications with criminal histories, consistent with legal requirements.\n\nSlalom welcomes and encourages applications from individuals with disabilities. Reasonable accommodations are available for candidates during all aspects of the\n\nselection process. Please advise the talent acquisition team if you require accommodations during the interview process.",
    "job_is_remote": false,
    "job_posted_at": "13 days ago",
    "job_posted_at_timestamp": 1770076800,
    "job_posted_at_datetime_utc": "2026-02-03T00:00:00.000Z",
    "job_location": "Oak Grove, NC",
    "job_city": "Oak Grove",
    "job_state": "North Carolina",
    "job_country": "US",
    "job_latitude": 35.9815344,
    "job_longitude": -78.8205619,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DmLWbCE2BsJ1026TLAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 55,
    "job_max_salary": 75,
    "job_salary_period": "HOUR",
    "job_highlights": {
      "Qualifications": [
        "We are seeking a GCP Data Engineer with strong BigQuery experience to support a major federal engagement focused on disaster recovery, data modernization, and mission‑critical analytics for FEMA",
        "This position requires U.S. citizenship and the ability to obtain and maintain a Public Trust clearance",
        "U.S. citizenship",
        "Ability to obtain and maintain a federal Public Trust clearance",
        "3+ years of experience in cloud-based data engineering",
        "Strong hands-on expertise with Google BigQuery",
        "Proficiency in Python for pipeline development, automation, and cloud integration",
        "Experience building data pipelines in GCP, including BigQuery, Dataform, Airflow/Cloud Composer, Cloud Functions, or similar",
        "Strong SQL skills, including data modeling and data quality testing",
        "Experience with Git-based version control and CI/CD concepts",
        "Familiarity with data governance, metadata management, and compliance considerations",
        "Strong communication and stakeholder engagement skills",
        "Experience supporting federal or regulated environments",
        "Familiarity with Looker and downstream BI enablement",
        "Understanding of ML workloads or data structures optimized for modeling",
        "Experience with Agile/Scrum or SAFe",
        "Knowledge of data quality frameworks and testing strategies",
        "Exposure to GCP data governance tools such as Dataplex",
        "Experience with serverless architectures (Cloud Functions, Cloud Run)",
        "Familiarity with JavaScript for Dataform SQLX extensibility",
        "Hands-on experience with orchestration tools (Airflow, Prefect, Dagster, Luigi)",
        "Experience with dbt, Dataform, Databricks, or other analytics engineering tooling",
        "Relevant GCP certifications",
        "Slalom will also consider qualified applications with criminal histories, consistent with legal requirements"
      ],
      "Benefits": [
        "Slalom prides itself on helping team members thrive in their work and life",
        "As a result, Slalom is proud to invest in benefits that include meaningful time off and paid holidays, 401(k) with a match, a range of choices for highly subsidized health, dental, & vision coverage, adoption and fertility assistance, and short/long-term disability",
        "We also offer yearly $350 reimbursement account for any well-being-related expenses",
        "Slalom is committed to fair and equitable compensation practices",
        "For this position, the base salary pay range is $55/hr to $75/hr",
        "Actual compensation will depend upon an individual’s skills, experience, qualifications, location, and other relevant factors",
        "The salary pay range is subject to change and may be modified at any time"
      ],
      "Responsibilities": [
        "Data Engineering & Cloud Development",
        "Design, build, and maintain cloud‑native ETL/ELT data pipelines using BigQuery, Dataform, Python, Cloud Composer (Airflow), Cloud Functions, and Cloud Storage",
        "Develop BigQuery‑centric data models, transformations, and analytics layers supporting downstream Looker dashboards and federal reporting needs",
        "Implement modern analytics engineering practices, including version‑controlled SQLX (Dataform), modular transformations, data quality checks, and documentation",
        "Client Leadership & Delivery",
        "Collaborate with federal stakeholders to understand data ingestion, transformation, governance, and reporting requirements",
        "Translate technical designs and delivery timelines for both technical and non‑technical audiences",
        "Support modernization of legacy data environments into scalable GCP‑based architectures",
        "Ensure all solutions align with federal data governance, security, and performance standards",
        "Solution Optimization & Innovation",
        "Optimize BigQuery workloads using partitioning, clustering, incremental processing, and cost‑efficient modeling",
        "Maintain robust CI/CD practices using GitLab or GitHub for version control, merge requests, and promotion pipelines",
        "Develop and maintain data lineage, metadata documentation, and enterprise data models",
        "Identify linkages across disparate datasets to build unified, interoperable data architectures",
        "Perform cleanup of existing datasets and transformation logic where needed",
        "Collaboration & Team Leadership",
        "Work closely with data architects, BI developers, cloud engineers, and data scientists",
        "Participate in SAFe Agile ceremonies including daily standups, retrospectives, and PI planning",
        "Track work in Jira and maintain documentation in Confluence",
        "Support testing, deployment, and quality assurance of data products",
        "Mentor junior data engineering team members and contribute to best-practice frameworks",
        "Slalom welcomes and encourages applications from individuals with disabilities"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-slalom-flex-project-based-federal-gcp-data-engineer-at-slalom-4357516712",
    "_source": "new_jobs"
  },
  {
    "job_id": "ybwkukVOvvN7CZ4IAAAAAA==",
    "job_title": "Data Engineer Prin",
    "employer_name": "American Electric Power",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR0RrFTOdrhzngcyQL-2zVSyVH1kAYBiD2Gj2L_&s=0",
    "employer_website": null,
    "job_publisher": "Tallo",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Tallo",
        "apply_link": "https://tallo.com/jobs/technology/data-engineer/oh/franklin/data-engineer-prin-5d6c21303973/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "This job listing in Franklin - OH has been recently added. Tallo will add a summary here for this job shortly.",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Franklin, OH",
    "job_city": "Franklin",
    "job_state": "Ohio",
    "job_country": "US",
    "job_latitude": 39.5589474,
    "job_longitude": -84.30410739999999,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DybwkukVOvvN7CZ4IAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": "YEAR",
    "job_highlights": {},
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "tallo-com-jobs-technology-data-engineer-oh-franklin-data-engineer-prin-5d6c21303973",
    "_source": "new_jobs"
  },
  {
    "job_id": "Zl_lLJqOx3OHfJdpAAAAAA==",
    "job_title": "Tech Lead/Data Engineer III",
    "employer_name": "Medica",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_FelXxkN3248jJhI07DlC_ZOvbFg5RbXHeQzt&s=0",
    "employer_website": "https://www.medica.com",
    "job_publisher": "LinkedIn",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "LinkedIn",
        "apply_link": "https://www.linkedin.com/jobs/view/tech-lead-data-engineer-iii-at-medica-4368808744?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Lensa",
        "apply_link": "https://lensa.com/job-v1/medica/hopkins-mn/lead-data-engineer/788bbbb1a1cac318a6dc43a1b175d33c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Adzuna",
        "apply_link": "https://www.adzuna.com/details/5613772574?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-mn-hopkins-tech-lead-data-engineer-iii-medica-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Medica is seeking an experienced Tech Lead/Data Engineer III. This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives. This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance.\n\nKey Responsibilities:\n\nWe're a team that owns our work with accountability, makes data-driven decisions, embraces continuous learning, and celebrates collaboration — because success is a team sport. It's our mission to be there in the moments that matter most for our members and employees. Join us in creating a community of connected care, where coordinated, quality service is the norm and every member feels valued.\n\nDesign, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources. Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data. Write complex SQL queries to support analytics needs. Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark.\n\nKey Accountabilities:\n• Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics.\n• Ensuring data quality, integrity, and security across all stages of the data lifecycle.\n• Collaborating with stakeholders to define data requirements and translate them into technical specifications.\n• Monitoring and optimizing performance of data systems and ETL processes.\n• Supporting the deployment and maintenance of data infrastructure in cloud environments.\n• Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting.\n• Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows.\n\nIn addition to engineering responsibilities, the Data Generalist component of this role includes:\n• Performing exploratory data analysis and generating actionable insights.\n• Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel.\n• Collaborating with cross-functional teams to align data efforts with business goals.\n• Automating data workflows using scripting languages such as Python or R.\n• Supporting business intelligence initiatives and translating data into strategic recommendations.\n• Documenting data processes and contributing to data governance standards.\n• Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets.\n• Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation.\n\nQualifications:\n• Bachelor's degree or equivalent experience in related field\n• 5 years of related work experience beyond the degree\n• Proficient use of SQL\n\nSkills And Abilities\n• Microsoft Certified: (or equivalent cloud) Azure Data Engineer\n• Microsoft Certified: Azure Fundamentals\n• Snowflake SnowPro Core Certification\n• Snowflake Advanced Architect Certification\n• Healthcare industry experience is desired\n\nThis position is an Office role, which requires an employee to work onsite at our Minnetonka, MN office, on average, 3 days per week.\n\nThe full salary grade for this position is $100,300 - $172,000. While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465. Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary. Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees.\n\nThe compensation and benefits information is provided as of the date of this posting. Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law.\n\nEligibility to work in the US: Medica does not offer work visa sponsorship for this role. All candidates must be legally authorized to work in the United States at the time of application. Employment is contingent on verification of identity and eligibility to work in the United States.\n\nWe are an Equal Opportunity employer, where all qualified candidates receive consideration for employment indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Hopkins, MN",
    "job_city": "Hopkins",
    "job_state": "Minnesota",
    "job_country": "US",
    "job_latitude": 44.9244005,
    "job_longitude": -93.41143989999999,
    "job_benefits": [
      "health_insurance",
      "dental_coverage",
      "paid_time_off"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DZl_lLJqOx3OHfJdpAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 100000,
    "job_max_salary": 172000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "This role is ideal for an innovative and strategic thinker with a depth of experience in enterprise data and broader cloud-based data engineering initiatives",
        "This individual is someone who thrives in a mission-driven, healthcare-focused organization and is passionate about ensuring efficiency, scalability, financial integrity and compliance",
        "Bachelor's degree or equivalent experience in related field",
        "5 years of related work experience beyond the degree",
        "Proficient use of SQL",
        "Microsoft Certified: (or equivalent cloud) Azure Data Engineer",
        "Microsoft Certified: Azure Fundamentals",
        "Snowflake SnowPro Core Certification",
        "Snowflake Advanced Architect Certification",
        "All candidates must be legally authorized to work in the United States at the time of application"
      ],
      "Benefits": [
        "The full salary grade for this position is $100,300 - $172,000",
        "While the full salary grade is provided, the typical hiring salary range for this role is expected to be between $100,300 - $150,465",
        "Annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. In addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary",
        "Medica offers a generous total rewards package that includes competitive medical, dental, vision, PTO, Holidays, paid volunteer time off, 401K contributions, caregiver services and many other benefits to support our employees",
        "The compensation and benefits information is provided as of the date of this posting",
        "Medica’s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law"
      ],
      "Responsibilities": [
        "Design, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of sources",
        "Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data",
        "Write complex SQL queries to support analytics needs",
        "Evaluate and recommend tools and technologies for data infrastructure and processing, including modern cloud platforms such as Snowflake, Azure, AWS, and data integration tools like Apache Airflow, dbt, Informatica Intelligent Data Management Cloud (IDMC), and distributed processing frameworks such as Apache Spark",
        "Designing and implementing scalable data pipelines and storage solutions to support enterprise analytics",
        "Ensuring data quality, integrity, and security across all stages of the data lifecycle",
        "Collaborating with stakeholders to define data requirements and translate them into technical specifications",
        "Monitoring and optimizing performance of data systems and ETL processes",
        "Supporting the deployment and maintenance of data infrastructure in cloud environments",
        "Developing and maintaining complex SQL scripts and stored procedures for data transformation and reporting",
        "Leveraging Informatica IDMC for cloud-native data integration, data quality, and governance workflows",
        "In addition to engineering responsibilities, the Data Generalist component of this role includes:",
        "Performing exploratory data analysis and generating actionable insights",
        "Creating dashboards and visualizations using tools like Power BI, Tableau, or Excel",
        "Collaborating with cross-functional teams to align data efforts with business goals",
        "Automating data workflows using scripting languages such as Python or R",
        "Supporting business intelligence initiatives and translating data into strategic recommendations",
        "Documenting data processes and contributing to data governance standards",
        "Applying AI and machine learning techniques to enhance predictive analytics, automate decision-making, and uncover deeper insights from complex datasets",
        "Utilizing preferred AI tools and platforms such as TensorFlow, Azure Machine Learning, scikit-learn, and PyTorch to build and deploy models that support enterprise analytics and innovation"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-linkedin-com-jobs-view-tech-lead-data-engineer-iii-at-medica-4368808744",
    "_source": "new_jobs"
  },
  {
    "job_id": "q4w1sbGeUlXZqnSwAAAAAA==",
    "job_title": "(USA) Senior, Data Engineer",
    "employer_name": "Walmart",
    "employer_logo": null,
    "employer_website": "https://corporate.walmart.com",
    "job_publisher": "Talentify",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talentify",
        "apply_link": "https://www.talentify.io/job/usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Position Summary...\n\nWhat you'll do... Role summary:\n\nAs a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights. You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets. Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance. This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment.\nAbout the team:\n\nWe’re a small but high‑impact Data Engineering team building the data foundations that power measurement and feature innovation for our advertising products. Operating at the intersection of engineering, analytics, and product strategy, we deliver scalable pipelines and accurate performance insights that drive data‑informed decisions. Our agility and end‑to‑end ownership set us apart—we move quickly, collaborate closely across functions, and build systems that directly influence how advertisers measure success and how new ad features evolve. If you want to build reliable, scalable data systems with visible impact, this is the team for you.\nWhat you'll do:\n• Design and build scalable data pipelines and models using Databricks, PySpark, and SQL.\n• Integrate and transform data from multiple sources with strong quality, observability, and governance.\n• Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions.\n• Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management.\n• Optimize pipelines for reliability, performance, and cost across cloud environments.\n• Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions.\n\nWhat you'll bring:\n• Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling.\n• Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow.\n• Solid understanding of cloud platforms, especially Google Cloud Platform.\n• Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform.\n• Strong grounding in data governance, data quality, and compliance best practices.\n• Ability to design resilient data architectures across warehouses, lakes, and streaming systems.\n• Proven skill in translating complex business needs into effective data solutions.\n• Familiarity with machine learning concepts and how they integrate with data engineering workflows.\n• A passion for finding ways to integrate AI and LLMs into daily engineering\nactivities and products, while not compromising simplicity and code\nmaintainability.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices. Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\nFor information about benefits and eligibility, see One.Walmart.\nDallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00\nDenver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses. Additional compensation for certain positions may also include :\n- Stock\n\nㅤ\n\nㅤ\n\nㅤ\n\nㅤ\n\n‎\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years’ experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related\nfield.\n2 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n14901 Quorum Dr, Dallas, TX 75254-7521, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Rangeley, ME",
    "job_city": "Rangeley",
    "job_state": "Maine",
    "job_country": "US",
    "job_latitude": 44.965682099999995,
    "job_longitude": -70.6427102,
    "job_benefits": [
      "paid_time_off",
      "dental_coverage",
      "health_insurance"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dq4w1sbGeUlXZqnSwAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "This role requires strong technical expertise, a passion for solving complex data challenges, and the ability to drive innovation within a fast‑moving enterprise environment",
        "Extensive experience in data engineering, including scalable pipeline development, data integration, and modeling",
        "Strong proficiency in SQL, Python, PySpark, Databricks, and workflow orchestration with Airflow",
        "Solid understanding of cloud platforms, especially Google Cloud Platform",
        "Hands‑on experience with CI/CD using GitHub Actions and infrastructure automation with Terraform",
        "Strong grounding in data governance, data quality, and compliance best practices",
        "Ability to design resilient data architectures across warehouses, lakes, and streaming systems",
        "Proven skill in translating complex business needs into effective data solutions",
        "Familiarity with machine learning concepts and how they integrate with data engineering workflows",
        "A passion for finding ways to integrate AI and LLMs into daily engineering",
        "activities and products, while not compromising simplicity and code",
        "Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field",
        "Option 2: 5 years’ experience in",
        "software engineering or related field",
        "Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related",
        "2 years' experience in data engineering, database engineering, business intelligence, or business analytics",
        "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
        "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture"
      ],
      "Benefits": [
        "At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet",
        "Health benefits include medical, vision and dental coverage",
        "Financial benefits include 401(k), stock purchase and company-paid life insurance",
        "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
        "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
        "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
        "The amount you receive depends on your job classification and length of employment",
        "It will meet or exceed the requirements of paid sick leave laws, where applicable",
        "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
        "Tuition, books, and fees are completely paid for by Walmart",
        "Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
        "Benefits are subject to change and may be subject to a specific plan or program terms",
        "Dallas, Texas US-11571: The annual salary range for this position is $90,000.00 - $180,000.00",
        "Denver, Colorado US-11581: The annual salary range for this position is $99,000.00 - $198,000.00 Additional compensation includes annual or quarterly performance bonuses"
      ],
      "Responsibilities": [
        "As a Senior Data Engineer at Walmart, you will build and optimize scalable data pipelines and models that power critical business insights",
        "You’ll leverage cloud technologies, automation, and modern data engineering practices to transform complex datasets into reliable, high‑quality assets",
        "Partnering closely with cross‑functional teams, you’ll translate business needs into robust technical solutions that improve data accessibility, performance, and governance",
        "Design and build scalable data pipelines and models using Databricks, PySpark, and SQL",
        "Integrate and transform data from multiple sources with strong quality, observability, and governance",
        "Orchestrate and automate workflows using Airflow and CI/CD pipelines in GitHub Actions",
        "Develop and maintain infrastructure using Terraform and support DevOps tasks such as deployments, monitoring, and environment management",
        "Optimize pipelines for reliability, performance, and cost across cloud environments",
        "Collaborate with Product, Data Science, and Engineering teams to deliver reliable, scalable, and efficient data solutions"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talentify-io-job-usa-senior-data-engineer-rangeley-maine-us-walmart-r-2399845",
    "_source": "new_jobs"
  },
  {
    "job_id": "zidiJyI2uFY85BMZAAAAAA==",
    "job_title": "Data Engineer with Databricks Focus",
    "employer_name": "VirtualVocations",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQxZcedD_3dF93ViStyNIWTQoxuaUqovO_AbE5a&s=0",
    "employer_website": "https://www.virtualvocations.com",
    "job_publisher": "Talent.com",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.talent.com/view?id=7b777f5d7a11&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Talent.com",
        "apply_link": "https://www.talent.com/view?id=7b777f5d7a11&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "A company is looking for a Data Engineer with a focus on Databricks.\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines using Databricks technologies\n\nLead the migration and rehydration of approximately 500 PowerBI reports, optimizing data sources\n\nImplement and maintain CI / CD pipelines for data assets using modern DevOps practices\n\nRequired Qualifications\n\n4+ years of hands-on data engineering experience\n\nStrong proficiency in Python and SQL\n\nDeep experience with Databricks and its associated technologies\n\nProven track record of implementing CI / CD for data workloads\n\nRelevant certifications in Databricks or Azure Data Engineering preferred",
    "job_is_remote": false,
    "job_posted_at": "3 days ago",
    "job_posted_at_timestamp": 1770940800,
    "job_posted_at_datetime_utc": "2026-02-13T00:00:00.000Z",
    "job_location": "Fort Wayne, IN",
    "job_city": "Fort Wayne",
    "job_state": "Indiana",
    "job_country": "US",
    "job_latitude": 41.0794759,
    "job_longitude": -85.1362929,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DzidiJyI2uFY85BMZAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "4+ years of hands-on data engineering experience",
        "Strong proficiency in Python and SQL",
        "Deep experience with Databricks and its associated technologies",
        "Proven track record of implementing CI / CD for data workloads"
      ],
      "Responsibilities": [
        "Design, build, and maintain scalable data pipelines using Databricks technologies",
        "Lead the migration and rehydration of approximately 500 PowerBI reports, optimizing data sources",
        "Implement and maintain CI / CD pipelines for data assets using modern DevOps practices"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-talent-com-view",
    "_source": "new_jobs"
  },
  {
    "job_id": "xu-VGehYTuGfUSZxAAAAAA==",
    "job_title": "Data Engineer - Python",
    "employer_name": "Apptad Inc",
    "employer_logo": null,
    "employer_website": "https://apptad.com",
    "job_publisher": "Digitalhire",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": true,
    "apply_options": [
      {
        "publisher": "Digitalhire",
        "apply_link": "https://jobs.digitalhire.com/job-listing/opening/2VqttgF6jgftHenVNT8bJZ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": true
      }
    ],
    "job_description": "Position Overview\n\nWe are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions. This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment.\n\nKey Responsibilities\n• Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory.\n• Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions.\n• Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing.\n• Implement data migration and transformation workflows leveraging Azure cloud services.\n• Ensure data quality, performance tuning, and optimization of data pipelines.\n• Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects.\n• Apply best practices for data governance, privacy, and security within Azure environments.\n• Lead the design and implementation of both batch and streaming data pipelines.\n• Provide technical leadership and mentorship to team members.\n• Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions.\n• Troubleshoot and resolve issues to ensure high availability and reliability.\n• Participate in architectural discussions and develop coding standards and security guidelines.\n• Drive automation of data workflows and improve operational efficiency.\n• Support production environments with timely incident response and root cause analysis.\n\nRequired Qualifications\n• 5 to 7 years of relevant experience in data engineering or related fields.\n• Expertise in Python programming.\n• Proficiency in Azure Data Factory and its ecosystem.\n\nPreferred Qualifications\n• Familiarity with Azure Data Lake and other Azure cloud services.",
    "job_is_remote": false,
    "job_posted_at": "7 days ago",
    "job_posted_at_timestamp": 1770595200,
    "job_posted_at_datetime_utc": "2026-02-09T00:00:00.000Z",
    "job_location": "Cockrell Hill, TX",
    "job_city": "Cockrell Hill",
    "job_state": "Texas",
    "job_country": "US",
    "job_latitude": 32.7362421,
    "job_longitude": -96.8869481,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dxu-VGehYTuGfUSZxAAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "We are seeking a Specialist with 5 to 7 years of experience in designing and implementing data engineering solutions",
        "5 to 7 years of relevant experience in data engineering or related fields",
        "Expertise in Python programming",
        "Proficiency in Azure Data Factory and its ecosystem"
      ],
      "Responsibilities": [
        "This role focuses on leveraging Python and Azure Data Factory to build scalable ETL processes and data pipelines within an Azure environment",
        "Design, develop, and maintain scalable data pipelines and ETL processes using Python and Azure Data Factory",
        "Collaborate with data application teams and product owners to understand requirements and deliver analytics solutions",
        "Utilize Azure Data Factory and Azure Data Lake for data ingestion, storage, and processing",
        "Implement data migration and transformation workflows leveraging Azure cloud services",
        "Ensure data quality, performance tuning, and optimization of data pipelines",
        "Support continuous integration and continuous deployment (CICD) pipelines for data engineering projects",
        "Apply best practices for data governance, privacy, and security within Azure environments",
        "Lead the design and implementation of both batch and streaming data pipelines",
        "Provide technical leadership and mentorship to team members",
        "Collaborate with DevOps teams to manage deployment, monitoring, and support of solutions",
        "Troubleshoot and resolve issues to ensure high availability and reliability",
        "Participate in architectural discussions and develop coding standards and security guidelines",
        "Drive automation of data workflows and improve operational efficiency",
        "Support production environments with timely incident response and root cause analysis"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "jobs-digitalhire-com-job-listing-opening-2vqttgf6jgfthenvnt8bjz",
    "_source": "new_jobs"
  },
  {
    "job_id": "B94kz2kLrz_jTu22AAAAAA==",
    "job_title": "Manager, HR Data Engineer",
    "employer_name": "Metropolitan Transportation Authority",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJ2luycnmBz43jpzLUO-YuE-x2KemrXhe2VUsU&s=0",
    "employer_website": null,
    "job_publisher": "Glassdoor",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.glassdoor.com/job-listing/manager-hr-data-engineer-metropolitan-transportation-authority-JV_KO0,24_KE25,62.htm?jl=1009935451706&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Glassdoor",
        "apply_link": "https://www.glassdoor.com/job-listing/manager-hr-data-engineer-metropolitan-transportation-authority-JV_KO0,24_KE25,62.htm?jl=1009935451706&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Talents By Vaia",
        "apply_link": "https://talents.vaia.com/companies/unavailable/manager-data-engineer-41508266/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Description\n\nJOB TITLE:\n\nManager, HR Data Engineer\n\nDEPT/DIV:\n\nPeople Department\n\nWORK LOCATION:\n\n2 Broadway\n\nFULL/PART-TIME\n\nFULL\n\nSALARY RANGE:\n\n$110,000 - $123,000\n\nDEADLINE:\n\nUntil filled\n\nThis position is eligible for teleworking, which is currently one day per week. New hires are eligible to apply 30 days after their effective hire date.\n\nOpening:\n\nThe Metropolitan Transportation Authority is North America's largest transportation network, serving a population of 15.3 million people across a 5,000-square-mile travel area surrounding New York City, Long Island, southeastern New York State, and Connecticut. The MTA network comprises the nation’s largest bus fleet and more subway and commuter rail cars than all other U.S. transit systems combined. MTA strives to provide a safe and reliable commute, excellent customer service, and rewarding opportunities.\n\nPosition Objective:\n\nThis position will report to the Director of HR Data Science and support the day-to-day data pipeline initiatives to design, build, and maintain ease for data structures to facilitate reporting and monitor key performance indicators. The incumbent will collaborate across Human Capital Management disciplines to identify internal/external data sources to design table structure, define ETL strategy, automate quality assurance checks, and implement scalable ETL solutions.\n\nResponsibilities:\n• Ensure that all assignments are completed with the highest quality and within agreed-upon Service Level agreement guidelines and Key Performance Indicator (KPI) targets.\n• Create and conduct a project/architecture design review.\n• Proficient knowledge of Azure Data Factory, Databricks, & Azure Delta Lake.\n• Experience programming languages (e.g., Python, R).\n• Working experience in data extraction using API.\n• Develop HR data pipelines and reports with advanced SQL programming language and maintain Data Warehousing/Data Lakes/Data Hubs and Analytical reporting Environment.\n• Work with IT teams to collect required data from internal and external systems and troubleshoot HR system issues.\n• Design and build modern data management solutions and create POC when necessary to test new approaches.\n• Create runbooks and actionable alerts as part of the development process.\n• Perform SQL and ETL tuning as necessary.\n• Perform ad hoc analysis as necessary.\n• Identify and implement continuous improvement initiatives as assigned.\n• Other duties as assigned.\n\nQualifications:\n\nKnowledge/Skills/Abilities:\n• Strong understanding of data modeling principles, including Dimensional modeling, data normalization principles, etc.\n• Proficient understanding of SQL Engines and able to develop advanced queries and analytics\n• Familiarity with data exploration/data visualization tools like MS Power BI, PeopleSoft HCM, JobVite,\n• JDXpert, Jetdocs, or Oracle Analytics CloudAbility to think strategically, analyze, and interpret Human Capital Management and Financial data.\n• Strong communication skills – written and verbal presentations.\n• Excellent conceptual and analytical reasoning competencies.\n• Comfortable working in a fast-paced and highly collaborative environment.\n• Process-oriented with excellent documentation skills, including strong Excel & PowerPoint skills, Visio flows, and mock-up creation.\n\nRequired Education and Experience:\n• Bachelor's degree in Computer Science, Information Management, Statistics, or Finance or related field. An equivalent combination of education and experience may be considered in lieu of a degree.\n• A minimum of four (4) years of relevant professional experience leading, implementing, and reporting on business key performance indicators in a data warehousing/lake/hub environment.\n• Minimum of four (4) years of experience using SQL for analytics, working with traditional relational databases and/or distributed systems such as PeopleSoft Enterprise Performance Management (EPM), Hadoop / Hive, BigQuery, Redshift, or Oracle Databases.\n\nPreferred:\n• Master's Degree in Computer Science, Information Management, or Statistics\n• Experience programming languages (e.g., Python, R) preferred.\n• Minimum of four (4) years of management experience\n• Minimum of one (1) year of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\n• Understanding of Finance Data and Human Resource Data practices and procedures.\n• Proficient Knowledge of Data Repositories/warehouses/Lakes/Hubs.\n\nOther Information\n\nMay need to work outside of normal work hours (i.e., evenings and weekends)\n\nTravel may be required to other MTA locations or other external sites.\n\nAccording to the New York State Public Officers Law & the MTA Code of Ethics, all employees who hold a policymaking position must file an Annual Statement of Financial Disclosure (FDS) with the NYS Commission on Ethics and Lobbying in Government (the “Commission”).\n\nEmployees driving company vehicles must complete defensive driver training once every three years for current MNR drivers, or within 180 days of hire or transfer for an employee entering an authorized driving position.\n\nEqual Employment Opportunity\n\nMTA and its subsidiary and affiliated agencies are Equal Opportunity Employers, including those concerning veteran status and individuals with disabilities.\n\nThe MTA encourages qualified applicants from diverse backgrounds, experiences, and abilities, including military service members, to apply.",
    "job_is_remote": false,
    "job_posted_at": "5 days ago",
    "job_posted_at_timestamp": 1770768000,
    "job_posted_at_datetime_utc": "2026-02-11T00:00:00.000Z",
    "job_location": "United States",
    "job_city": null,
    "job_state": null,
    "job_country": "US",
    "job_latitude": 38.794595199999996,
    "job_longitude": -106.5348379,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DB94kz2kLrz_jTu22AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_min_salary": 110000,
    "job_max_salary": 123000,
    "job_salary_period": "YEAR",
    "job_highlights": {
      "Qualifications": [
        "Experience programming languages (e.g., Python, R)",
        "Working experience in data extraction using API",
        "Strong understanding of data modeling principles, including Dimensional modeling, data normalization principles, etc",
        "Proficient understanding of SQL Engines and able to develop advanced queries and analytics",
        "Familiarity with data exploration/data visualization tools like MS Power BI, PeopleSoft HCM, JobVite,",
        "JDXpert, Jetdocs, or Oracle Analytics CloudAbility to think strategically, analyze, and interpret Human Capital Management and Financial data",
        "Strong communication skills – written and verbal presentations",
        "Excellent conceptual and analytical reasoning competencies",
        "Comfortable working in a fast-paced and highly collaborative environment",
        "Process-oriented with excellent documentation skills, including strong Excel & PowerPoint skills, Visio flows, and mock-up creation",
        "Bachelor's degree in Computer Science, Information Management, Statistics, or Finance or related field",
        "An equivalent combination of education and experience may be considered in lieu of a degree",
        "A minimum of four (4) years of relevant professional experience leading, implementing, and reporting on business key performance indicators in a data warehousing/lake/hub environment",
        "Minimum of four (4) years of experience using SQL for analytics, working with traditional relational databases and/or distributed systems such as PeopleSoft Enterprise Performance Management (EPM), Hadoop / Hive, BigQuery, Redshift, or Oracle Databases",
        "May need to work outside of normal work hours (i.e., evenings and weekends)",
        "Employees driving company vehicles must complete defensive driver training once every three years for current MNR drivers, or within 180 days of hire or transfer for an employee entering an authorized driving position"
      ],
      "Benefits": [
        "$110,000 - $123,000"
      ],
      "Responsibilities": [
        "This position will report to the Director of HR Data Science and support the day-to-day data pipeline initiatives to design, build, and maintain ease for data structures to facilitate reporting and monitor key performance indicators",
        "The incumbent will collaborate across Human Capital Management disciplines to identify internal/external data sources to design table structure, define ETL strategy, automate quality assurance checks, and implement scalable ETL solutions",
        "Ensure that all assignments are completed with the highest quality and within agreed-upon Service Level agreement guidelines and Key Performance Indicator (KPI) targets",
        "Create and conduct a project/architecture design review",
        "Proficient knowledge of Azure Data Factory, Databricks, & Azure Delta Lake",
        "Develop HR data pipelines and reports with advanced SQL programming language and maintain Data Warehousing/Data Lakes/Data Hubs and Analytical reporting Environment",
        "Work with IT teams to collect required data from internal and external systems and troubleshoot HR system issues",
        "Design and build modern data management solutions and create POC when necessary to test new approaches",
        "Create runbooks and actionable alerts as part of the development process",
        "Perform SQL and ETL tuning as necessary",
        "Perform ad hoc analysis as necessary",
        "Identify and implement continuous improvement initiatives as assigned",
        "Other duties as assigned",
        "Travel may be required to other MTA locations or other external sites"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-glassdoor-com-job-listing-manager-hr-data-engineer-metropolitan-transportation-authority-jv_ko0-24_ke25-62-htm",
    "_source": "new_jobs"
  },
  {
    "job_id": "HbnLs4Eglyfjre53AAAAAA==",
    "job_title": "Lead Data Engineer - Financial Data, Compliance & Business Intelligence",
    "employer_name": "Crowe",
    "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR9Jy0qQcDq1Z-tUfxslQj89D4g7hpqFneD78Ll&s=0",
    "employer_website": "https://www.crowe.com",
    "job_publisher": "Local Job Network",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://jobs.localjobnetwork.com/job/detail/85100412/Lead-Data-Engineer-Financial-Data-Compliance-Business-Intelligence?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Local Job Network",
        "apply_link": "https://jobs.localjobnetwork.com/job/detail/85100412/Lead-Data-Engineer-Financial-Data-Compliance-Business-Intelligence?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Ladders",
        "apply_link": "https://www.theladders.com/job/lead-data-engineer-financial-data-compliance-business-intelligence-crowe-miami-fl_83795134?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      },
      {
        "publisher": "Jobrapido",
        "apply_link": "https://us.jobrapido.com/jobpreview/2378212067837476864?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "Your Journey at Crowe Starts Here:\n\nAt Crowe, you can build a meaningful and rewarding career. With real flexibility to balance work with life moments, you're trusted to deliver results and make an impact. We embrace you for who you are, care for your well-being, and nurture your career. Everyone has equitable access to opportunities for career growth and leadership. Over our 80-year history, delivering excellent service through innovation has been a core part of our DNA across our audit, tax, and consulting groups. That's why we continuously invest in innovative ideas, such as AI-enabled insights and technology-powered solutions, to enhance our services. Join us at Crowe and embark on a career where you can help shape the future of our industry.\n\nJob Description:\n\nWe are looking for an experienced ETL Engineer to join our Data Engineering team. The ideal candidate will have a strong understanding of ETL processes and technologies, as well as experience with a variety of data sources. The ETL developer will be responsible for designing, developing, and maintaining ETL pipelines that extract, transform, and load data into our data warehouse. As a member of the Data Analytics team, you will provide meaningful firm-wide contributions that uphold the team's dedication to \"One Crowe\". We strive for an unparalleled client experience and look to you to promote our success.\n\nAdditionally, you will work as part of a team of problem solvers with extensive consulting and industry experience, helping our clients solve their complex business issues from strategy to execution.\n\nSpecific responsibilities include but are not limited to:\n• Identify & map source data in various commercial and custom systems\n• Design, develop, test and maintain ETL pipelines\n• Extract, transform, and load data from a variety of data sources\n• Work with data engineers and other stakeholders to define data requirements\n• Troubleshoot and debug ETL pipelines\n• Stay up-to-date on ETL technologies and best practices\n• Formulate development estimates (task, effort, dependencies, etc.)\n• Develop test cases and demonstrate results for assigned deliverables.\n• Maintain and present daily/weekly status\n• Articulate project deliverable details and activities to client and project team members\n• Adapt to various industries and quickly learn the client's context, language, and jargon to clearly document and translate business requirements to development plans\n• Maintain a professional presence and be ready to interact with the client at all times\n\nQualifications\n• Bachelor's degree in computer science, information technology, or a related field\n• 3+ years of experience in ETL development\n• Hands on experience with Bank Secrecy Act (BSA), Financial Audits, or Anti-Money Laundering (AML) regulations and compliance requirements.\n• Previous consulting experience or experience working with external clients\n• Strong understanding of ETL processes and technologies\n• Experience with a variety of data sources, including relational databases, flat files, and web APIs\n• Experience with Python, Java, or another programming language\n• Experience with data warehouse technologies, such as Azure, AWS, and Snowflake\n• Excellent problem-solving and debugging skills\n• Strong communication and teamwork skills\n• Solid understanding of software development life cycle models as well as expert knowledge of both Agile and traditional project management principles and practices and the ability to blend them together in the right proportions to fit a project and business environment\n• Technical background, with understanding or hands-on experience in enterprise solutions for different industries\n• High-level knowledge of BI architecture or data warehousing\n• Excellent written and verbal communication skills\n• Effectively managing multiple deliverables, clients and projects in a fast-paced environment\n• Strong problem solving and analytical skills\n\nCertifications (Preferred)\n• Certifications in BI (ETL tools, visualization tools, cloud architectures) are expected\n• Certifications such as CAMS (Certified Anti-Money Laundering Specialist), CFE (Certified Fraud Examiner).\n\nKey Stakeholders This Role Interacts With:\n\nInternal\n• Senior BI Analyst\n• Data Product Manager\n• BI Architect / Senior Architect\n• Data Engineers\n• Report Developer (Power BI, Tableau)\n\nExternal\n• C-Suite executives, business leaders, and department heads\n• Data Owners, Data Stewards\n• Enterprise Information Management and Data Governance Team\n• IT Staff - Architects, DBAs, Developers, etc\n\nWe expect the candidate to uphold Crowe's values of Care, Trust, Courage, and Stewardship. These values define who we are. We expect all of our people to act ethically and with integrity at all times.\n\nIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire. Crowe is not sponsoring for work authorization at this time.\n\nThe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Crowe, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $104,500.00 - $208,300.00 per year.\n\nOur Benefits:\nYour exceptional people experience starts here. At Crowe, we know that great peopleare what makes a great firm. We care about our people and offer employees a comprehensive total rewards package. Learn more about what working at Crowe can mean for you!\n\nHow You Can Grow:\nWe will nurture your talent in an inclusive culture that values diversity. You will have the chance to meet on a consistent basis with your Career Coach that will guide you in your career goals and aspirations. Learn more about where talent can prosper!\n\nMore about Crowe:\nCrowe (www.crowe.com) is one of the largest public accounting, consulting and technology firms in the United States. Crowe uses its deep industry expertise to provide audit services to public and private entities while also helping clients reach their goals with tax, advisory, risk and performance services. Crowe is recognized by many organizations as one of the country's best places to work. Crowe serves clients worldwide as an independent member of Crowe Global, one of the largest global accounting networks in the world. The network consists of more than 200 independent accounting and advisory services firms in more than 130 countries around the world.\n\nCrowe LLP provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, sexual orientation, gender identity or expression, genetics, national origin, disability or protected veteran status, or any other characteristic protected by federal, state or local laws.\n\nCrowe LLP does not accept unsolicited candidates, referrals or resumes from any staffing agency, recruiting service, sourcing entity or any other third-party paid service at any time. Any referrals, resumes or candidates submitted to Crowe, or any employee or owner of Crowe without a pre-existing agreement signed by both parties covering the submission will be considered the property of Crowe, and free of charge.\n\nCrowe will consider for employment all qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local laws, including the City of Los Angeles' Fair Chance Initiative for Hiring Ordinance, Los Angeles County Fair Chance Ordinance, San Francisco Fair Chance Ordinance, and the California Fair Chance Act.\n\nPlease visit our webpage to see notices of the various state and local Ban-the-Box laws and Fair Chance Ordinances, where applicable.\n\nIf you are interested in applying for employment with Crowe and are in need of an accommodation or require special assistance to navigate our website or to complete your application, please visit our Applicant Assistance and Accommodations page for more information: https://careers.crowe.com/crowe-applicant-assistance-and-accommodation\n\nEOE M/F/D/V",
    "job_is_remote": false,
    "job_posted_at": "12 days ago",
    "job_posted_at_timestamp": 1770163200,
    "job_posted_at_datetime_utc": "2026-02-04T00:00:00.000Z",
    "job_location": "Washington, DC",
    "job_city": "Washington",
    "job_state": "District of Columbia",
    "job_country": "US",
    "job_latitude": 38.9072873,
    "job_longitude": -77.0369274,
    "job_benefits": null,
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DHbnLs4Eglyfjre53AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "The ideal candidate will have a strong understanding of ETL processes and technologies, as well as experience with a variety of data sources",
        "Bachelor's degree in computer science, information technology, or a related field",
        "3+ years of experience in ETL development",
        "Hands on experience with Bank Secrecy Act (BSA), Financial Audits, or Anti-Money Laundering (AML) regulations and compliance requirements",
        "Previous consulting experience or experience working with external clients",
        "Strong understanding of ETL processes and technologies",
        "Experience with a variety of data sources, including relational databases, flat files, and web APIs",
        "Experience with Python, Java, or another programming language",
        "Experience with data warehouse technologies, such as Azure, AWS, and Snowflake",
        "Excellent problem-solving and debugging skills",
        "Strong communication and teamwork skills",
        "Solid understanding of software development life cycle models as well as expert knowledge of both Agile and traditional project management principles and practices and the ability to blend them together in the right proportions to fit a project and business environment",
        "Technical background, with understanding or hands-on experience in enterprise solutions for different industries",
        "High-level knowledge of BI architecture or data warehousing",
        "Excellent written and verbal communication skills",
        "Effectively managing multiple deliverables, clients and projects in a fast-paced environment",
        "Strong problem solving and analytical skills",
        "BI Architect / Senior Architect",
        "We expect the candidate to uphold Crowe's values of Care, Trust, Courage, and Stewardship"
      ],
      "Benefits": [
        "The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs",
        "A reasonable estimate of the current range is $104,500.00 - $208,300.00 per year"
      ],
      "Responsibilities": [
        "The ETL developer will be responsible for designing, developing, and maintaining ETL pipelines that extract, transform, and load data into our data warehouse",
        "As a member of the Data Analytics team, you will provide meaningful firm-wide contributions that uphold the team's dedication to \"One Crowe\"",
        "We strive for an unparalleled client experience and look to you to promote our success",
        "Additionally, you will work as part of a team of problem solvers with extensive consulting and industry experience, helping our clients solve their complex business issues from strategy to execution",
        "Identify & map source data in various commercial and custom systems",
        "Design, develop, test and maintain ETL pipelines",
        "Extract, transform, and load data from a variety of data sources",
        "Work with data engineers and other stakeholders to define data requirements",
        "Troubleshoot and debug ETL pipelines",
        "Stay up-to-date on ETL technologies and best practices",
        "Formulate development estimates (task, effort, dependencies, etc.)",
        "Develop test cases and demonstrate results for assigned deliverables",
        "Maintain and present daily/weekly status",
        "Articulate project deliverable details and activities to client and project team members",
        "Adapt to various industries and quickly learn the client's context, language, and jargon to clearly document and translate business requirements to development plans",
        "Maintain a professional presence and be ready to interact with the client at all times",
        "Data Product Manager",
        "Report Developer (Power BI, Tableau)",
        "Data Owners, Data Stewards",
        "Enterprise Information Management and Data Governance Team",
        "IT Staff - Architects, DBAs, Developers, etc"
      ]
    },
    "job_onet_soc": "13104100",
    "job_onet_job_zone": "3",
    "id": "jobs-localjobnetwork-com-job-detail-85100412-lead-data-engineer-financial-data-compliance-business-intelligence",
    "_source": "new_jobs"
  },
  {
    "job_id": "RC-Ath7fkXGMQs_2AAAAAA==",
    "job_title": "Data Engineer III 38",
    "employer_name": "Blue Nile",
    "employer_logo": null,
    "employer_website": "https://www.bluenile.com",
    "job_publisher": "Jobilize",
    "job_employment_type": "Full-time",
    "job_employment_types": [
      "FULLTIME"
    ],
    "job_apply_link": "https://www.jobilize.com/job/us-all-cities-data-engineer-iii-38-blue-nile-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
    "job_apply_is_direct": false,
    "apply_options": [
      {
        "publisher": "Jobilize",
        "apply_link": "https://www.jobilize.com/job/us-all-cities-data-engineer-iii-38-blue-nile-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic",
        "is_direct": false
      }
    ],
    "job_description": "The Data Engineer III is responsible for building and maintaining robust data pipelines and models for the R2Net organization, overseeing a broad scope of ETL infrastructure and databases.\nBy deploying expertise across many diverse systems, APIs, and platforms, this role plays a key part in enabling accurate and timely access to data across the organization, with an emphasis on simplifying and centralizing a complex ecosystem - and thereby providing trustworthy, analytic-ready resources for the various business units.\n\nThis role, by interfacing with key stakeholders in Engineering, Analytics, Operations, Finance, Marketing, and Customer Service, will work to build a data environment that is accurate, complete, timely, and dependable, and will serve as a trusted partner to key associates across the org.\nIn achieving these goals, this role will extend beyond pipeline management - it will involve deep collaboration with business stakeholders, proactive engagement in data strategy, and a focus on driving measurable impact through data.\nThis engineer is expected to bridge technical execution with business outcomes, owning long-term initiatives that drive top-line and bottom-line growth for R2Net as a whole.\n\nKey Responsibilities:\n• Design, implement, and maintain complex data pipelines, ensuring scalability and reliability using Airflow, dbt, Rivery, Python, and SQL, enabling robust ingestion and transformation of structured and semi-structured data.\n• Serve as a strategic partner to business teams, working closely with stakeholders to translate high-level goals into data solutions that support forecasting, performance tracking, and optimization.\n• Develop and maintain clean, well-documented data models in Snowflake and BigQuery that support analytics, reporting, and operational workflows and contribute to architecture decisions.\n• Integrate data from a variety of internal and external sources, including Google Analytics and third-party APIs, to support full-funnel visibility across departments.\n• Enable self-service analytics by ensuring data assets are discoverable and usable via tools such as Tableau, including thoughtful semantic layer design and performance tuning.\n• Contribute to the development of robust monitoring and observability practices for data quality and pipeline health.\n• Collaborate on architecture and design decisions, including cloud infrastructure and containerization using AWS.\nPulumi and Docker.\n• Maintain strong documentation and promote engineering standards that ensure transparency, maintainability, and reusability of data systems.\n\nRequirements\n• 7+ years of professional experience in data engineering, analytics engineering, or related roles.\n• Advanced proficiency in SQL and Python, with expertise in efficient query writing, data structures, and software engineering principles.\n• Hands-on experience with Snowflake and/or Big Query, including data modeling and performance optimization.\n• Proficiency with orchestration tools (e.g., Airflow) and data integration tools like dbt.\n• Experience working with cloud platforms, especially AWS, for data storage, compute, and infrastructure management, including services such as AWS Batch, ECR, Lambda functions, and related tools.\n• Familiarity with data analytics and visualization tools, particularly Tableau, and ability to support data consumers in building actionable dashboards.\n• Experience with marketing and product data sources, including Google Analytics and similar platforms.\n• Strong knowledge of ETL/ELT design and data warehousing solutions.\n• Familiarity with CI/CD pipelines and DevOps practices for data engineering.\n• Strong skills in Microsoft Suite for documentation and collaboration.\n• Robust experience with API design and integration.\n• Familiarity with SCRUM development methodologies and tools like Jira\nBenefits\n\nAt R2Net - James Allen & Blue Nile, many of our roles offer a high-quality, comprehensive benefits package including healthcare, paid time off, retirement planning and opportunities for career advancement.\nSome offerings are dependent upon the role, employment type, work schedule or location:\n• Paid Time Off\n• Medical, Dental, Vision and Prescription Insurance\n• 401(k) Retirement Plan with Company Match\n• Flexible Spending Account Health Savings Account\n• Tuition Reimbursement\n• Employee Discount\n• Parental Leave\n• Life Insurance\n\nAt this time, R2Net will not sponsor a new applicant for employment authorization for this position.\n\nAdditional Information:\nR2NET INC.\nis an equal opportunity employer committed to diversity and inclusion in the workplace.\nAll qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status or any other basis prohibited under applicable federal, state or local law.\nR2NET INC.\nwill consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law.\n\nPay Transparency Disclosure\n\nIf based in Blue Nile's Seattle or New York hub offices, this role has the annual base salary range stated below.\n\nAnnual base pay:\n$147,000 - $172,000.\nFinal pay rate shall be determined and is based on experience and qualifications\n\nJob level and actual compensation will be decided based on factors including, but not limited to, individual qualifications objectively assessed during the interview process (including skills and prior relevant experience, potential impact, and scope of role), market demands, and specific work location.\nThe listed range is a guideline, and the range for this role may be modified.\nFor roles that are available to be filled remotely, the pay range is localized according to employee work location by a factor of between 80% and 100% of range.\nPlease discuss your specific work location with your recruiter for more information.\n• LI-WK1",
    "job_is_remote": false,
    "job_posted_at": "6 days ago",
    "job_posted_at_timestamp": 1770681600,
    "job_posted_at_datetime_utc": "2026-02-10T00:00:00.000Z",
    "job_location": "Oregon",
    "job_city": null,
    "job_state": "Oregon",
    "job_country": "US",
    "job_latitude": 43.8041334,
    "job_longitude": -120.5542012,
    "job_benefits": [
      "paid_time_off",
      "health_insurance",
      "dental_coverage"
    ],
    "job_google_link": "https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DRC-Ath7fkXGMQs_2AAAAAA%3D%3D&vssid=jobs-detail-viewer",
    "job_salary": null,
    "job_min_salary": null,
    "job_max_salary": null,
    "job_salary_period": null,
    "job_highlights": {
      "Qualifications": [
        "7+ years of professional experience in data engineering, analytics engineering, or related roles",
        "Advanced proficiency in SQL and Python, with expertise in efficient query writing, data structures, and software engineering principles",
        "Hands-on experience with Snowflake and/or Big Query, including data modeling and performance optimization",
        "Proficiency with orchestration tools (e.g., Airflow) and data integration tools like dbt",
        "Experience working with cloud platforms, especially AWS, for data storage, compute, and infrastructure management, including services such as AWS Batch, ECR, Lambda functions, and related tools",
        "Familiarity with data analytics and visualization tools, particularly Tableau, and ability to support data consumers in building actionable dashboards",
        "Experience with marketing and product data sources, including Google Analytics and similar platforms",
        "Strong knowledge of ETL/ELT design and data warehousing solutions",
        "Familiarity with CI/CD pipelines and DevOps practices for data engineering",
        "Strong skills in Microsoft Suite for documentation and collaboration",
        "Robust experience with API design and integration",
        "Familiarity with SCRUM development methodologies and tools like Jira",
        "will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law"
      ],
      "Benefits": [
        "At R2Net - James Allen & Blue Nile, many of our roles offer a high-quality, comprehensive benefits package including healthcare, paid time off, retirement planning and opportunities for career advancement",
        "Some offerings are dependent upon the role, employment type, work schedule or location:",
        "Paid Time Off",
        "Medical, Dental, Vision and Prescription Insurance",
        "401(k) Retirement Plan with Company Match",
        "Flexible Spending Account Health Savings Account",
        "Tuition Reimbursement",
        "Employee Discount",
        "Parental Leave",
        "Life Insurance",
        "If based in Blue Nile's Seattle or New York hub offices, this role has the annual base salary range stated below",
        "$147,000 - $172,000",
        "Final pay rate shall be determined and is based on experience and qualifications",
        "Job level and actual compensation will be decided based on factors including, but not limited to, individual qualifications objectively assessed during the interview process (including skills and prior relevant experience, potential impact, and scope of role), market demands, and specific work location"
      ],
      "Responsibilities": [
        "The Data Engineer III is responsible for building and maintaining robust data pipelines and models for the R2Net organization, overseeing a broad scope of ETL infrastructure and databases",
        "By deploying expertise across many diverse systems, APIs, and platforms, this role plays a key part in enabling accurate and timely access to data across the organization, with an emphasis on simplifying and centralizing a complex ecosystem - and thereby providing trustworthy, analytic-ready resources for the various business units",
        "This role, by interfacing with key stakeholders in Engineering, Analytics, Operations, Finance, Marketing, and Customer Service, will work to build a data environment that is accurate, complete, timely, and dependable, and will serve as a trusted partner to key associates across the org",
        "In achieving these goals, this role will extend beyond pipeline management - it will involve deep collaboration with business stakeholders, proactive engagement in data strategy, and a focus on driving measurable impact through data",
        "This engineer is expected to bridge technical execution with business outcomes, owning long-term initiatives that drive top-line and bottom-line growth for R2Net as a whole",
        "Design, implement, and maintain complex data pipelines, ensuring scalability and reliability using Airflow, dbt, Rivery, Python, and SQL, enabling robust ingestion and transformation of structured and semi-structured data",
        "Serve as a strategic partner to business teams, working closely with stakeholders to translate high-level goals into data solutions that support forecasting, performance tracking, and optimization",
        "Develop and maintain clean, well-documented data models in Snowflake and BigQuery that support analytics, reporting, and operational workflows and contribute to architecture decisions",
        "Integrate data from a variety of internal and external sources, including Google Analytics and third-party APIs, to support full-funnel visibility across departments",
        "Enable self-service analytics by ensuring data assets are discoverable and usable via tools such as Tableau, including thoughtful semantic layer design and performance tuning",
        "Contribute to the development of robust monitoring and observability practices for data quality and pipeline health",
        "Collaborate on architecture and design decisions, including cloud infrastructure and containerization using AWS",
        "Pulumi and Docker",
        "Maintain strong documentation and promote engineering standards that ensure transparency, maintainability, and reusability of data systems"
      ]
    },
    "job_onet_soc": "15113200",
    "job_onet_job_zone": "4",
    "id": "www-jobilize-com-job-us-all-cities-data-engineer-iii-38-blue-nile-hiring-now-job-immediately",
    "_source": "new_jobs"
  }
]